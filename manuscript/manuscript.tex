\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}

\title{Categorical Predictor Embeddings in Actuarial Modeling}

\author{
    Kevin Kuo
   \\
    RStudio and Kasa AI \\
   \\
  \texttt{\href{mailto:kevin@kasa.ai}{\nolinkurl{kevin@kasa.ai}}} \\
   \And
    Ronald Richman
   \\
    QED Actuaries and Consultants \\
   \\
  \texttt{\href{mailto:ronaldrichman@gmail.com}{\nolinkurl{ronaldrichman@gmail.com}}} \\
  }

\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{cslreferences}%
  {\setlength{\parindent}{0pt}%
  \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces}%
  {\par}

\begin{document}
\maketitle

\def\tightlist{}


\begin{abstract}
Enter the text of your abstract here.
\end{abstract}

\keywords{
    blah
   \and
    blee
   \and
    bloo
   \and
    these are optional and can be removed
  }

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Categorical data are modelled by actuaries in many different contexts,
from pricing of insurance products to reserving for the liabilities
generated by these products. Sometimes, these data are modelled in an
explicit manner, for example, when building models that apply across
multiple categories, a form of dummy coding is usually used. For
example, when building models to price the frequency of motor insurance
claims, claim experience relating to different types of motor vehicle
will often be modelled by including a (single) factor within models that
modifies the relative frequency predicted for each type of vehicle. In
other cases, modelling is performed for each category separately, thus
the categorical data is used within the modelling in an implicit manner,
for example, common practice is to estimate reserves for different lines
of business separately, meaning to say, with no parameters being shared
across each of the reserving models.

Recently, several studies of insurance problems have applied an
alternative approach, originally from the natural language processing
literature (Bengio et al. 2003) and now applied to diverse types of
machine learning problem (Guo and Berkhahn 2016), known as
\textbf{categorical embeddings}. Instead of trying to capture the
differences between categories using a single factor, the categorical
embedding approach rather maps each category to a low-dimensional
numeric vector, which is then used within the model as a new predictor
variable.

This approach to modelling categorical data has several advantages over
more traditional treatments of categorical data. Using categorical
embeddings instead of traditional techniques has been shown to increase
predictive accuracy of models, for example, see (Richman 2018) in the
context of pricing. Models incorporating categorical embeddings can be
pre-calibrated to traditional actuarial models, increasing the speed
with which these models can be calibrated and leading to models with
better explainability (Wüthrich and Merz 2019). Finally, the similarity
between the vectors learned for different categories can be inspected,
sometimes leading to insights into the workings of models, see, for
example, (Kuo 2019).

On the other hand, several open questions about the use of embeddings
within actuarial work remain, which we aim to address in this study.
First, hyperparameter settings for embeddings, such as the dimensions of
the embedding layer and the use of regularization techniques such as
dropout or normalization, that achieve optimal predictive performance
has not yet been studied in detail in the actuarial literature. In this
work, we aim to study how embeddings using different settings perform in
the context of a large-scale predictive modelling problem, and give
guidance on the process that can be followed to determine this in other
problems. Although neural network have been shown to achieve excellent
predictive accuracy on actuarial tasks, many actuaries still prefer to
use GLM models for pricing tasks, thus, the issue of whether
transferring embeddings to GLM models can achieve better performance is
considered in this paper. Traditional actuarial techniques such as
credibility theory have been used to work with some types of categorical
data, but no study has been performed whether this can be applied within
embeddings; here, we investigate whether performance is enhanced by
applying credibility theory on embeddings relating to categorical
variables with many labels. Whereas embedding layers are usually
considered in the context of categorical data,the option exists to
quantize numerical data and model it using embeddings, however, the
results of doing so have not been investigated. Finally, in the past
several years, a new type of neural network architecture based on
attention (Vaswani et al. 2017) has been successfully used on embeddings
in the field of natural language processing and we incorporate attention
based models into our predictive modelling example.

In this work, we utilize the recently released National Flood Insurance
Program (NFIP) dataset (Federal Emergency Management Agency 2019) which
provides exposure information for policies written under the NFIP since
XXXXDateXXXX, as well as the claims data relating to these exposures. We
refer the reader to Appendix \ref{NFIP_eda} for an exploratory analysis
of this dataset.

The rest of this manuscript is organized as follows. Section
\ref{lit_review} reviews recent applications of embeddings in the
actuarial literature. Section \ref{defns} provides the notation used in
the paper and defines GLMs, neural networks and related modelling
concepts. In Section \ref{modelling}, we provide initial models for the
NFIP dataset and consider the influence of hyperparameter choices on the
results of the neural network model. Section \ref{tl} considers how
successfully the embeddings used in the neural network model can be
transferred to a GLM model of the same data. Extensions to the modelling
approach are considered in Section \ref{extend} and we focus on
attention based models in Section \ref{attention}. The interpretability
of embedding layers is addressed in Section \ref{interpretation}.
Finally, Section \ref{Conclusions} provides a discussion of the results
of this paper and considers avenues for future research.

(TODO: link to github repo)

\hypertarget{lit_review}{%
\section{Literature Review}\label{lit_review}}

Categorical data are usually modelled within GLMs and other predictive
models using indicator variables which capture the effect of each level
of the category, see, for example, Section 2 in (Goldburd et al. 2020),
using one of two main encoding schemes: dummy-coding and one-hot
encoding. Dummy-coding, used in the popular \texttt{R} statistical
software, assigns one level of the category as a baseline, for which an
indicator variable is not calibrated, and the rest of the levels are
assigned indicator variables, thus, producing estimates within the model
of how the effects of each level differ from the baselines. One-hot
encoding, often used in machine learning, is similar to dummy-coding,
but assigns indicator variables to each level, in other words,
calibrates an extra indicator variable compared with dummy-coding.

A different approach to modelling categorical data is credibility theory
(see (Bühlmann and Gisler 2005) for an overview), which, in the context
of rating, can be applied to derive premiums that reflect the experience
of a particular policyholder, by estimating premiums as a weighted
average between the premium produced using the collective experience
(i.e.~of all policyholders) and the premium produced using the
experience of the particular policyholder. The weight used in this
average is called a credibility factor and is calculated with reference
to te variability of the policyholder experience relative to the
variability of the group experience. In this context, the implicit
categorical variable is the policyholder under consideration.

Generalized Linear Mixed Models (GLMMs) are an extension of GLMs that
are designed for modelling categorical data using a principle very
similar to that of credibility theory (Klinker 2010). Instead of
calibrating indicator variables for each level of the category, GLMMs
estimate effects for each of these levels as a combination of the
overall group mean and the experience in each level of the category.

Embedding layers represent a different approach to the problem of
modelling categorical data that was recently introduced in an actuarial
context. Note that in the next section, we reflect on similarities
between the conventional approaches discussed above and embedding
layers. (Richman 2018) reviewed the concept of embedding layers and
connected the sharing of information across categories to the familiar
concept of credibility theory. In that work, two applications of
embedding layers were demonstrated. The first of these was in a Property
and Casualty (P\&C) pricing context, it was shown that the out-of-sample
accuracy of a neural network trained to predict claims frequencies on
motor third party liability was enhanced by modelling the categorical
variables within this dataset using embedding layers. Second, a neural
network with embedding layers was used to model all of the mortality
rates in the Human Mortality Database, where the differences in
population mortality across countries and the differences in mortality
at different ages were modelled with embedding layers, again producing
more accurate out of sample performance than the other models tested.

Contemporaneous with that work is the DeepTriangle model of (Kuo 2019),
which applied recurrent neural networks to the problem of Incurred but
not Reported (IBNR) loss reserving, to model jointly the paid and
incurred losses in the Schedule P dataset. Embedding layers were used to
capture the effect of differences in reserving delays and loss ratios
for each company in the Schedule P dataset. Evaluating the results of
the DeepTriangle method showed that the out of sample performance of the
model (tested against the lower triangles in the Schedule P dataset)
exceeded that of traditional IBNR reserving techniques.

Many other applications of embeddings have subsequently appeared in the
actuarial literature. Within mortality forecasting, (Richman and
Wüthrich 2019) and (Perla et al. 2020) both apply embeddings layers to
model and forecast mortality rates on a large scale. (Wüthrich and Merz
2019) discussed how embeddings can be calibrated using GLM techniques
and then incorporated into a combined actuarial neural network, with
subsequent contributions in P\&C pricing by (Schelldorfer and Wüthrich
2019) and in IBNR reserving by (Gabrielli 2019) and (Gabrielli, Richman,
and Wüthrich 2019). Other applications in IBNR reserving are in (Kuo
2020) and (Delong, Lindholm, and Wuthrich 2020) who use embedding layers
to model individual claims development.

\hypertarget{defns}{%
\section{Definitions}\label{defns}}

In this study, we are concerned with regression modelling, which is the
task of predicting an unknown outcome \(y\) on the basis of information
about that outcome contained in predictor variables, or features, stored
in a matrix \(X\). For simplicity, we only consider the case of
univariate outcomes, i.e., \(y \in \mathbb{R}^1\). The outcomes and the
rows of the predictor variable matrix are indexed by
\(i \in \{1 \dots I\}\), where \(i\) represents a particular observation
of \((y_i, \mathbf{x}_i)\), where bold indicates that we are now dealing
with a vector. The columns of the predictor variables are indexed by
\(j \in \{1 \dots J\}\), where \(j\) represents a particular predictor
variable, of which \(J\) have been observed, thus, we use the notation
\(X_j\) to represent the \(j\)th predictor variable and
\(X \in \mathbb{R}^J\). Formally, we look to build regression models
that map from the predictor variables \(\mathbf{x}_i\) to the outcome
\(y\) using a function \(f\) of the form:

\[
\mathbf{f} : \mathbb{R}^J \mapsto \mathbb{R}^1, \quad \quad \mathbf{x_i} \mapsto  \mathbf f{(\mathbf{x_i})} = y. 
\]

We will use mainly use GLMs and neural networks to approximate the
function \(f(.)\).

The predictor variables that we consider here are comprised of two
types: continuous variables, taking on numerical values and represented
by the matrix \(X_{num}\) with \(J^{num}\) columns, and categorical
variables, which take on discrete values indicating one of several
possible categories, represented by the matrix \(X_{cat}\) with
\(J^{cat}\) columns, such that \(J^{num} + J^{cat} = J\).

\hypertarget{categorical-data-modeling}{%
\subsection{Categorical data modeling}\label{categorical-data-modeling}}

A categorical variable \(X_j, j \in J_{cat}\) takes as its value only
one of a finite number of labels. Let the set of labels be
\(\mathcal{P}^j = \{p^j_1, p^j_2,\dots, p^j_{n_{\mathcal{P}^j}} \}\),
where \(n_\mathcal{P}^j = |\mathcal{P}^j|\) is the cardinality or number
of levels, in \(\mathcal{P}^j\). One-hot encoding maps each value
\(x_{i,j}\) of \(X_j\) to \(n_\mathcal{P}^j\) indicator variables, which
take a value of \(1\) if the label of \(x_{i,j}\) corresponds to the
level of the indicator variable, and \(0\) otherwise. An example of
one-hot encoding is shown in Table \ref{tab:onehot}.

\begin{longtable}[]{@{}lrrrr@{}}
\caption{Example one-hot encoding of the state variable}\tabularnewline
\toprule
state & state\_KS & state\_OK & state\_UT & state\_WA\tabularnewline
\midrule
\endfirsthead
\toprule
state & state\_KS & state\_OK & state\_UT & state\_WA\tabularnewline
\midrule
\endhead
CO & 0 & 0 & 0 & 0\tabularnewline
KS & 1 & 0 & 0 & 0\tabularnewline
OK & 0 & 1 & 0 & 0\tabularnewline
UT & 0 & 0 & 1 & 0\tabularnewline
WA & 0 & 0 & 0 & 1\tabularnewline
\bottomrule
\end{longtable}

One-hot encoding is often used in the machine learning community while
the statistical community often favors dummy coding, which, instead of
assigning \(n_p^j\) indicator variables, assigns one of the levels of
the categories as a baseline, and maps all of the other \(n_p^j -1\)
variables to indicator variables.

The approach that we focus on in this study is to use embeddings, which
map each level of the categorical data to a low dimensional vector of
parameters that is learned together with the rest of the GLM or neural
network that is used for the modeling problem. Formally, an embedding is

\[
z_\mathcal{P^j} : \mathcal{P^j} \to \mathbb{R}^{q_\mathcal{P^j} }, \quad \quad p^j \mapsto  z_\mathcal{P^j}(p),
\]

where \(q_\mathcal{P^j}\) is the dimension of the embedding for the
\(j\)th categorical variable and \(z_\mathcal{P^j}(.)\) is an function
that maps from the particular element of the labels \(p\) to the
embedding space. This function is left implicit, meaning to say, we
allow the embeddings to be derived during the process of fitting the
model and do not attempt to specify exactly how the embeddings can be
derived from the input data.

\hypertarget{glms-and-neural-networks}{%
\subsection{GLMs and Neural Networks}\label{glms-and-neural-networks}}

\hypertarget{modelling}{%
\section{A Minimalist Neural Network}\label{modelling}}

In this section, we describe, fit, and evaluate both a minimalist neural
network architecture utilizing embeddings and a traditional GLM. The
working example for our experiments is as follows: Given a set of claims
characteristics, we predict the losses paid on the property coverage of
the policy. In the rest of this section, we describe the dataset we use,
describe formally the models being considered, and discuss results.

\hypertarget{nfip-data}{%
\subsection{NFIP data}\label{nfip-data}}

The data we use comes from the National Flood Insurance Program (NFIP)
and is made available by the OpenFEMA initiative of the Federal
Emergency Management Agency (FEMA) (TODO: cite). Two datasets are made
available by OpenFEMA: A policies dataset with exposure information, and
a claims dataset with claims transactions, including paid amounts.
Because there is no way to associate records of the two datasets, we are
limited to fitting severity models on the claims dataset. While the
complete dataset contains over two million transactions, for the
purposes of our experiments we limit ourselves to data from 2000 to
August 2019, which amounts to approximately 1.4 million claims. The
dataset can be downloaded from Cellar (TODO: link). The dataset includes
a rich variety of variables, from occupancy type to coarse coordinates.
For our models, we work with a few selected variables that represent
continuous and discrete variables of low and high cardinalities, which
we list in Table \ref{tab:variables}.

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:variables} Variables used in
modeling.}\tabularnewline
\toprule
\begin{minipage}[b]{0.47\columnwidth}\raggedright
Variable\strut
\end{minipage} & \begin{minipage}[b]{0.47\columnwidth}\raggedright
Type\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.47\columnwidth}\raggedright
Variable\strut
\end{minipage} & \begin{minipage}[b]{0.47\columnwidth}\raggedright
Type\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.47\columnwidth}\raggedright
Building insurance coverage\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
Numeric\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
Basement enclosure type\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
Categorical\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
Number of floors in the insured building\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
Categorical (binned in original dataset)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
Flood zone\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
Categorical (high cardinality)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
Primary residence\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
Categorical (binary indicator)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{severity-glm}{%
\subsection{Severity GLM}\label{severity-glm}}

asdf

\hypertarget{neural-net}{%
\subsection{Neural net}\label{neural-net}}

asdf

\hypertarget{performance-evaluation-framework-and-results}{%
\subsection{Performance evaluation framework and
results}\label{performance-evaluation-framework-and-results}}

asdf

\hypertarget{interpretation}{%
\section{Interpretations}\label{interpretation}}

Extracting and interpreting trained embeddings, including visualization
techniques Can we explain the learned embeddings using a GLM? distance
to water etc Rainfall Density Distance to coast

\hypertarget{tl}{%
\section{Transfer Learning}\label{tl}}

How can embedding layers be used in GLM models? (Potential uses of
embedding layers as a feature engineering technique for GLM) Transfer
learned embeddings to GLM - backprop on NN and use as preprocessing
Train GLM on embedding layers - backprop on 1 layer NN

\hypertarget{extend}{%
\section{Extending the model}\label{extend}}

Investigate approaches for using embedding layers for numeric variables,
or numeric variables that were captured as categorical variables (Can
the modelling of numerical variables benefit from the application of
embedding layers?) How should this best be done? CatBoost - cut into 256
groups smoothness Map numeric to a dense layer and use that as embedding
- basis expansion Add a distribution to the embeddings Can insights from
credibility theory lead to enhanced embeddings? Take credibility mixture
over embeddings Cluster embeddings PCA of embeddings

\hypertarget{attention}{%
\section{Attention based modelling}\label{attention}}

Can we get some lift by adding attention layers? TabNet model
Transformer model

\hypertarget{Conclusions}{%
\section{Conclusions}\label{Conclusions}}

Conclusions

\hypertarget{refs}{}
\begin{cslreferences}
\leavevmode\hypertarget{ref-Bengio2003}{}%
Bengio, Yoshua, Réjean Ducharme, Pascal Vincent, and Christian Jauvin.
2003. ``A Neural Probabilistic Language Model.'' \emph{Journal of
Machine Learning Research} 3 (6): 1137--55.
\url{https://doi.org/10.1162/153244303322533223}.

\leavevmode\hypertarget{ref-Buhlmann2006}{}%
Bühlmann, Hans, and Alois Gisler. 2005. \emph{A Course in Credibility
Theory and its Applications}. Springer Science \& Business Media.
\url{https://doi.org/10.1080/03461230600889660}.

\leavevmode\hypertarget{ref-Delong2020a}{}%
Delong, Lukasz, Mathias Lindholm, and Mario V. Wuthrich. 2020.
``Collective Reserving using Individual Claims Data.'' \emph{SSRN
Electronic Journal}, May. \url{https://doi.org/10.2139/ssrn.3582398}.

\leavevmode\hypertarget{ref-FederalEmergencyManagementAgency2019}{}%
Federal Emergency Management Agency. 2019. ``FIMA NFIP Redacted Claims
Data Set.''
\url{https://www.fema.gov/media-library/assets/documents/180376\%20https://www.fema.gov/media-library/assets/documents/180374}.

\leavevmode\hypertarget{ref-Gabrielli2019c}{}%
Gabrielli, Andrea. 2019. ``A Neural Network Boosted Double
over-Dispersed Poisson Claims Reserving Model.'' \emph{SSRN Electronic
Journal}, April. \url{https://doi.org/10.2139/ssrn.3365517}.

\leavevmode\hypertarget{ref-Gabrielli2019}{}%
Gabrielli, Andrea, Ronald Richman, and Mario V. Wüthrich. 2019. ``Neural
network embedding of the over-dispersed Poisson reserving model.''
\emph{Scandinavian Actuarial Journal}.
\url{https://doi.org/10.1080/03461238.2019.1633394}.

\leavevmode\hypertarget{ref-Goldburd}{}%
Goldburd, Mark, Anand Khare, Dan Tevet, and Dmitriy Guller. 2020.
\emph{Generalized Linear Models for Insurance Rating}. Second Edi.
Casualty Actuarial Society. \url{www.casact.org}.

\leavevmode\hypertarget{ref-Guo2016}{}%
Guo, Cheng, and Felix Berkhahn. 2016. ``Entity Embeddings of Categorical
Variables.'' \emph{arXiv} arXiv:1604.
\url{http://arxiv.org/abs/1604.06737}.

\leavevmode\hypertarget{ref-Klinker2010}{}%
Klinker, Fred. 2010. ``Generalized Linear Mixed Models for Ratemaking: A
Means of Introducing Credibility into a Generalized Linear Model
Setting.'' \emph{Casualty Actuarial Society E-Forum, Winter 2011 Volume
2} 2 (1): 1--25.
\url{http://scholar.google.com/scholar?hl=en\%7B/\&\%7DbtnG=Search\%7B/\&\%7Dq=intitle:Generalized+Linear+Mixed+Models+for+Ratemaking+:+A+Means+of+Introducing+Credibility+into+a+Generalized+Linear+Model+Setting\%7B/\#\%7D0}.

\leavevmode\hypertarget{ref-Kuo2019}{}%
Kuo, Kevin. 2019. ``Deeptriangle: A deep learning approach to loss
reserving.'' \emph{Risks} 7 (3).
\url{https://doi.org/10.3390/risks7030097}.

\leavevmode\hypertarget{ref-Kuo2020}{}%
---------. 2020. ``Individual Claims Forecasting with Bayesian Mixture
Density Networks,'' March. \url{http://arxiv.org/abs/2003.02453}.

\leavevmode\hypertarget{ref-Richman2020}{}%
Perla, Francesca, Ronald Richman, Salvatore Scognamiglio, and Mario V.
Wüthrich. 2020. ``Time-Series Forecasting of Mortality Rates using Deep
Learning.'' \emph{SSRN Electronic Journal}.

\leavevmode\hypertarget{ref-Richman2018}{}%
Richman, R. 2018. ``AI in Actuarial Science.'' \emph{SSRN Electronic
Journal}, October. \url{https://doi.org/10.2139/ssrn.3218082}.

\leavevmode\hypertarget{ref-Richman2019d}{}%
Richman, R, and Mario V. Wüthrich. 2019. ``A neural network extension of
the Lee-Carter model to multiple populations.'' \emph{Annals of
Actuarial Science}. \url{https://doi.org/10.1017/S1748499519000071}.

\leavevmode\hypertarget{ref-schelldorfer2019nesting}{}%
Schelldorfer, Jürg, and Mario V. Wüthrich. 2019. ``Nesting Classical
Actuarial Models into Neural Networks.'' \emph{SSRN Electronic Journal}.
\url{https://doi.org/10.2139/ssrn.3320525}.

\leavevmode\hypertarget{ref-Vaswani2017}{}%
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.
``Attention is all you need.'' In \emph{Advances in Neural Information
Processing Systems}, 2017-Decem:5999--6009.
\url{http://arxiv.org/abs/1706.03762v5}.

\leavevmode\hypertarget{ref-wuthrich2019yes}{}%
Wüthrich, Mario V, and Michael Merz. 2019. ``Yes, we CANN!'' \emph{ASTIN
Bulletin: The Journal of the IAA} 49 (1): 1--3.
\end{cslreferences}

\bibliographystyle{unsrt}
\bibliography{references.bib}


\end{document}
