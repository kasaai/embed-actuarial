@article{Bahdanau2015a,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
eprint = {1409.0473},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--15},
title = {{Neural machine translation by jointly learning to align and translate}},
year = {2015}
}

@inproceedings{Vaswani2017a,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {1706.03762v5},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.03762v5},
issn = {10495258},
mendeley-groups = {To read},
pages = {5999--6009},
title = {{Attention is all you need}},
volume = {2017-Decem},
year = {2017}
}

@techreport{FSB2014,
address = {Pretoria},
author = {FSB},
institution = {Financial Services Board},
publisher = {Financial Services Board},
title = {{Position Paper 64 - Life SCR Longevity Risk}},
year = {2014}
}
@article{Camarda2008,
author = {Camarda, Carlos},
pages = {154},
title = {{Smoothing methods for the analysis of  mortality development}},
url = {https://docs.google.com/viewerng/viewer?url=e-archivo.uc3m.es/bitstream/handle/10016/5133/ThesisCAMARDA.pdf},
year = {2008}
}

@article{Klinker2011,
abstract = {GLMs that include explanatory classification variables with sparsely populated levels assign large standard errors to these levels but do not otherwise shrink estimates toward the mean in response to low credibility. Accordingly, actuaries have attempted to superimpose credibility on a GLM setting, but the resulting methods do not appear to have caught on. The Generalized Linear Mixed Model (GLMM) is yet another way of introducing credibility-like shrinkage toward the mean in a GLM setting. Recently available statistical software, such as SAS PROC GLIMMIX, renders these models more readily accessible to actuaries. This paper offers background on GLMMs and presents a case study displaying shrinkage towards the mean very similar to Buhlmann-Straub credibility. Keywords:},
author = {Klinker, Fred},
number = {1},
pages = {1--25},
title = {{Generalized Linear Mixed Models for Ratemaking: A Means of Introducing Credibility into a Generalized Linear Model Setting}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Generalized+Linear+Mixed+Models+for+Ratemaking+:+A+Means+of+Introducing+Credibility+into+a+Generalized+Linear+Model+Setting{\#}0},
volume = {2},
year = {2010}
}


@article{kelliher_wilmot_vij_klumpes_2013,
abstract = {Risk terminology varies from organisation to organisation, and actuaries working in different organisations may use different terms to refer to the same risk, or use the same nomenclature for completely different risks. This paper sets out a classification system developed by the Risk Classification Working Party for the Profession that can be used as a common reference point for discussing risk. Actuaries will not be required to use this system, but it is hoped that common terminology will reduce the possibility of confusion in discussing risks.},
author = {Kelliher, P. O. J. and Wilmot, D. and Vij, J. and Klumpes, P. J. M.},
doi = {10.1017/s1357321712000293},
issn = {1357-3217},
journal = {British Actuarial Journal},
number = {1},
pages = {91--121},
publisher = {Cambridge University Press},
title = {{A common risk classification system for the Actuarial Profession}},
volume = {18},
year = {2013}
}
@article{Whelan2009a,
author = {Whelan, S F},
isbn = {1748-5002},
journal = {Annals of Actuarial Science},
number = {01},
pages = {67--104},
title = {{Mortality in Ireland at advanced ages, 1950-2006: Part 2: Graduated rates}},
volume = {4},
year = {2009}
}
@techreport{CSI2012a,
abstract = {This document sets out the major findings and results of the annuitant mortality investigation conducted by the Continuous Statistical Investigation (CSI) committee. This investigation covered the period 1 January 2001 to 31 December 2004.},
address = {Cape Town},
author = {CSI},
institution = {Actuarial Society of South Africa},
language = {en},
publisher = {Continuous Statistical Investigation Committee, Actuarial Society of South Africa},
title = {{Annuitant Mortality 2001-2004}},
year = {2012}
}
@article{Tuljapurkar1998,
author = {Tuljapurkar, Shripad and Boe, Carl},
isbn = {1092-0277},
journal = {North American Actuarial Journal},
number = {4},
pages = {13--47},
title = {{Mortality change and forecasting: how much and how little do we know?}},
volume = {2},
year = {1998}
}
@article{Omran1971,
author = {Omran, Abdel R},
isbn = {0026-3745},
journal = {The Milbank Memorial Fund Quarterly},
pages = {509--538},
title = {{The epidemiologic transition: a theory of the epidemiology of population change}},
year = {1971}
}
@book{Chollet2018,
abstract = {Objective. To determine whether breastfeeding reduced the risk of childhood obesity in the infants of a multi-ethnic cohort of women with pregestational diabetes. Methods. In this retrospective cohort study, women with pregestational diabetes were mailed a questionnaire about breastfeeding and current height and weight of mothers and infants. Predictors of obesity (weight for age {\textgreater}85 percentile) were assessed among offspring of index pregnancies, using univariate and multivariable logistic regression. Results. Of 125 women, 81 (65{\%}) had type 1 diabetes and 44 (35{\%}) had type 2 diabetes. The mean age of offspring was 4.5 years. On univariate analysis, significant predictors of obesity in offspring were type 2 diabetes (odds ratio, OR 2.4, 95{\%} confidence interval, CI 0.99-5.72); maternal body mass index (BMI) {\textgreater} 25 (OR 4.4, 95{\%} CI 1.4-19.4); and any breastfeeding (OR 0.22, 95{\%} CI 0.07-0.72). After multivariable adjustment, breastfeeding (OR 0.20, 95{\%} CI 0.06-0.69) and having an overweight/obese mother (OR 3.49, 95{\%} CI 1.03-16.2) remained independently associated with childhood obesity. Conclusion. Breastfeeding significantly decreased the likelihood of obesity in offspring of mothers with pregestational diabetes, independent of maternal BMI and diabetes type. Women with diabetes should be encouraged to breastfeed, given the increased risk of obesity in their children.},
author = {Ghatak, Abhijit},
booktitle = {Deep Learning with R},
doi = {10.1007/978-981-13-5850-0},
isbn = {161729554X},
publisher = {Manning Publications Co.},
title = {{Deep Learning with R}},
year = {2019}
}
@article{Nelson1987,
abstract = {This paper introduces a parametrically parsimonious model for yield curves that has the ability to represent the shapes generally associated with yield curves: monotonic, humped, and S shaped. We find that the model explains 96{\%} of the variation in bill yields across maturities during the period 1981-83. The movement of the parameters through time reflects and confirms a change in Federal Reserve monetary policy in late 1982. The ability of the fitted curves to predict the price of the long-term Treasury bond with a correlation of .96 suggests that the model captures important attributes of the yield/maturity relation.},
author = {Nelson, Charles R and Siegel, Andrew F},
doi = {10.1086/296409},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nelson, Siegel - 1987 - Parsimonious Modeling of Yield Curves.pdf:pdf},
issn = {0021-9398},
journal = {The Journal of Business},
number = {4},
pages = {473},
title = {{Parsimonious Modeling of Yield Curves}},
volume = {60},
year = {1987}
}
@misc{StatsSA2013,
author = {{Stats SA}},
publisher = {SSA Pretoria},
title = {{Mid-year population estimates}},
year = {2013}
}
@article{Chen2017,
abstract = {EU Gender Directive ruled out discrimination against gender in charging premium for insurance products. This prohibition prevents the use of the standard actuarial fairness principle to price life insurance products. According to current actuarial practice, unisex premiums are calculated with a simple weighting rule of the gender-specific life tables. This procedure is likely to violate portfolio fairness principles. Up to our knowledge, in the actuarial literature there is no unisex mortality model that respects the unisex fairness principle. This paper is the first attempt to fill this gap. First, we recall the notion of unisex fairness principle and the corresponding unisex fair premium. Then, we provide a unisex stochastic mortality model for the mortality intensity that is underlying the pricing of a life portfolio of females and males belonging to the same cohort. Finally, we calibrate the unisex mortality model using the unisex fairness principle. We find that the weighting coefficient between the males' and females' own mortalities depends mainly on the quote of portfolio relative to each gender, on the age, and on the type of insurance products. The knowledge of a proper unisex mortality model could help life insurance companies to better understanding the nature of the risk of a mixed portfolio.},
author = {Chen, An and Vigna, Elena},
doi = {10.1016/j.insmatheco.2017.01.007},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Vigna - 2017 - A unisex stochastic mortality model to comply with EU Gender Directive.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Actuarial fairness,Doubly stochastic process,Gender Directive,Life table,Stochastic mortality intensity,Unisex tariff},
pages = {124--136},
title = {{A unisex stochastic mortality model to comply with EU Gender Directive}},
volume = {73},
year = {2017}
}
@article{Hill2005,
author = {Hill, Kenneth and Choi, Yoonjoung and Tim{\ae}us, Ian},
journal = {Demographic Research},
number = {12},
pages = {281--300},
title = {{Unconventional approaches to mortality estimation}},
volume = {13},
year = {2005}
}
@book{Hill2003,
author = {Hill, Kenneth},
publisher = {UN},
title = {{Adult mortality in the developing world: what we know and how we know it}},
year = {2003}
}
@article{Bah2008,
abstract = {In a period of about five years, from 1997 to 2002, South Africa remarkably improved the coverage and production of its vital statistics. This period witnessed the entrance of South Africa into the select league of countries that publish statistics on multiple causes of death and that make use automatic coding of causes of death. These achievements were accomplished through multiple forces working in unison. Some of the important factors contributing to the achievement were lessons learned from study tours to Australia, Sweden and the U.S.A. The paper describes these lessons and how they were adapted to suit the South African reality. Comparison is made between the status of demographic statistics by the end of apartheid and in the post-apartheid era. Stakeholder relationships that shaped the transformation of demographic statistics in the new South Africa are also discussed.},
author = {Bah, Sulaiman},
doi = {10.12927/whp.2013.21017},
issn = {17183340},
journal = {World health {\&} population},
number = {1},
pages = {50--59},
title = {{Multiple forces working in unison: the case of rapid improvement of vital statistics in South Africa post-1996.}},
volume = {11},
year = {2009}
}
@misc{Richman2015,
address = {Johannesburg},
author = {Richman, R and Dorrington, R},
booktitle = {Seventh African Population Conference},
title = {{Investigating old age exaggeration in South African population and survey data using near extinct generations methods}},
year = {2015}
}
@article{Hickman1999,
abstract = {The North American Actuarial Journal is honoring the Society of Actuaries on its golden anniversary in 1999 by publishing a series of articles on the contributions of actuaries to the development of ideas. In this issue, the second of the series, we explore the development of the credibility idea and the relationships of this development to deep issues in the methodology of science. We begin with a short essay by James C. Hickman and continue with comments by three actuaries who were deeply involved with the development of credibility: Robert A. Bailey, Hans Bu{\"{u}}hlmann, and Charles C. Hewitt. {\textcopyright} 1999 Taylor {\&} Francis Group, LLC.},
author = {Hickman, James C. and Heacox, Linda},
doi = {10.1080/10920277.1999.10595793},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hickman, Heacox - 1999 - Credibility theory The cornerstone of actuarial science.pdf:pdf},
issn = {10920277},
journal = {North American Actuarial Journal},
month = {apr},
number = {2},
pages = {1--8},
publisher = { Taylor {\&} Francis Group },
title = {{Credibility theory: The cornerstone of actuarial science}},
volume = {3},
year = {1999}
}
@article{Kuhn2008,
abstract = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
author = {Kuhn, Max},
issn = {15487660},
journal = {Journal Of Statistical Software},
keywords = {model building,networkspaces,parallel processing,r,tuning parameters},
number = {5},
pages = {1--26},
title = {{caret Package}},
url = {http://www.jstatsoft.org/v28/i05/paper},
volume = {28},
year = {2008}
}
@article{Kim1986,
author = {Kim, Young J},
isbn = {0070-3370},
journal = {Demography},
number = {3},
pages = {451--461},
title = {{Examination of the generalized age distribution}},
volume = {23},
year = {1986}
}
@article{Dong2016,
abstract = {Characterizing driving styles of human drivers using vehicle sensor data, e.g., GPS, is an interesting research problem and an important real-world requirement from automotive industries. A good representation of driving features can be highly valuable for autonomous driving, auto insurance, and many other application scenarios. However, traditional methods mainly rely on handcrafted features, which limit machine learning algorithms to achieve a better performance. In this paper, we propose a novel deep learning solution to this problem, which could be the first attempt of extending deep learning to driving behavior analysis based on GPS data. The proposed approach can effectively extract high level and interpretable features describing complex driving patterns. It also requires significantly less human experience and work. The power of the learned driving style representations are validated through the driver identification problem using a large real dataset.},
archivePrefix = {arXiv},
arxivId = {1607.03611},
author = {Dong, Weishan and Li, Jian and Yao, Renjie and Li, Changsheng and Yuan, Ting and Wang, Lanjun},
eprint = {1607.03611},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Dong-2016-Characterizing driving styles with d.pdf:pdf},
journal = {arXiv},
shorttitle = {Characterizing driving styles with deep learning},
title = {{Characterizing Driving Styles with Deep Learning}},
url = {http://arxiv.org/abs/1607.03611},
volume = {arXiv:1607},
year = {2016}
}
@misc{CEIOPS,
author = {{Committee of European Insurance and Occupational Pensions Supervisors}, CEIOPS},
number = {October},
pages = {1--45},
title = {{CEIOPS' Advice for Level 2 Implementing Measures on Solvency II: SCR Standard Formula. Article 109 -. Structure and Design of Market Risk Module}},
year = {2009}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1505.05424},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blundell et al. - 2015 - Weight uncertainty in neural networks.pdf:pdf},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
pages = {1613--1622},
title = {{Weight uncertainty in neural networks}},
volume = {2},
year = {2015}
}
@book{Abbott,
author = {Abbott, Stephen},
file = {:C$\backslash$:/Users/user-pc/Desktop/2015{\_}Book{\_}UnderstandingAnalysis.pdf:pdf},
isbn = {0387944273},
title = {{Undergraduate Texts in Mathematics Understanding Analysis}}
}
@article{Wuthrich2018e,
abstract = {Machine learning techniques make it feasible to calculate claims reserves on individual claims data. This paper illustrates how these techniques can be used by providing an explicit example in individual claims reserving.},
author = {W{\"{u}}thrich, Mario V.},
doi = {10.1080/03461238.2018.1428681},
issn = {16512030},
journal = {Scandinavian Actuarial Journal},
keywords = {Individual claims data,individual claims reserving,machine learning,micro-level stochastic reserving,regression tree},
month = {jul},
number = {6},
pages = {465--480},
publisher = {Taylor and Francis Ltd.},
title = {{Machine learning in individual claims reserving}},
volume = {2018},
year = {2018}
}
@article{Camarda2008a,
abstract = {In many applications data can be interpreted as indirect observations of a latent distribution. A typical example is the phenomenon known as digit preference, i.e. the tendency to round outcomes to pleasing digits. The composite link model (CLM) is a useful framework to uncover such latent distributions. Moreover, when applied to data showing digit preferences, this approach allows estimation of the proportions of counts that were transferred to neighbouring digits. As the estimating equations generally are singular or severely ill-conditioned, we impose smoothness assumptions on the latent distribution and penalize the likelihood function. To estimate the misreported proportions, we use a weighted least-squares regression with an added L1 penalty. The optimal smoothing parameters are found by minimizing the Akaike's information Criterion (AIC). The approach is verified by a simulation study and several applications are presented. {\textcopyright} 2008 SAGE Publications.},
author = {Camarda, Carlo G. and Eilers, Paul H C and Gampe, Jutta},
doi = {10.1177/1471082X0800800404},
isbn = {1471-082X},
issn = {1471082X},
journal = {Statistical Modelling},
keywords = {Composite link model digit preference,L1 penalty,Penalized likelihood,Smoothing},
number = {4},
pages = {385--401},
title = {{Modelling general patterns of digit preference}},
volume = {8},
year = {2008}
}
@article{Buettner2002,
abstract = {In 1998 the United Nations Population Division extended the age format of its estimates and projections of population dynamics for all countries and areas of the world from 80 years and above to 100 years and above. The paper is based on experiences made during the implementation of relevant mortality projection methodologies and their application in two rounds of global population projections. The paper first briefly addresses the need for the explicit inclusion of very old population segments into the regular UN estimates and projections. It is argued that since population aging is an important issue for both developed and developing countries, the need for more information regarding the elderly, and the oldest-old in particular, is significant. The paper then documents the methods that have been evaluated and implemented, namely, the relational mortality standard proposed by Himes, Preston, and Condran, the Coale-Kisker extrapolation method for extending empirical age patterns of mortality to very high ages, and the Carter-Lee projection method for projecting model patterns of mortality to very high levels of life expectancy at birth. The methods are critically reviewed, and possible improvements to the methods are discussed. The paper concludes with a discussion of different views regarding the future evolution of mortality at older ages, their regional variability, and the necessity to improve the coverage and quality of data collected in this area. {\textcopyright} 2002 Taylor {\&} Francis Group, LLC.},
author = {Buettner, Thomas},
doi = {10.1080/10920277.2002.10596053},
isbn = {1092-0277},
issn = {10920277},
journal = {North American Actuarial Journal},
number = {3},
pages = {14--29},
title = {{Approaches and Experiences in Projecting Mortality Patterns for the Oldest-Old}},
volume = {6},
year = {2002}
}
@article{booth2006demographic,
abstract = {Approaches and developments in demographic and population forecasting since 1980 are reviewed. Three approaches to forecasting demographic processes are extrapolation, expectation (individual-level birth expectations or population-level opinions of experts), and theory-based structural modelling involving exogenous variables. Models include 0-3 factors (age, period and cohort). Decomposition and disaggregation are also used in multistate models, including macrosimulation and microsimulation. Forecasting demographic change is difficult; accuracy depends on the particular situation or trends, but it is not clear when a method will perform best. Estimates of uncertainty (model-based ex ante error, expert-opinion-based ex ante error, and ex post error) differ; uncertainty estimation is highly uncertain. Probabilistic population forecasts are based on stochastic population renewal or random scenarios. The approaches to population forecasting, demographic process forecasting and error estimation are closely linked. Complementary methods that combine approaches are increasingly employed. The paper summarises developments, assesses progress and considers the future. {\textcopyright} 2006 International Institute of Forecasters.},
author = {Booth, Heather},
doi = {10.1016/j.ijforecast.2006.04.001},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Causal models,Demographic modelling,Disaggregation,Expectations,Extrapolation,Fertility,Migration,Mortality,Population forecasting},
number = {3},
pages = {547--581},
publisher = {Elsevier},
title = {{Demographic forecasting: 1980 to 2005 in review}},
volume = {22},
year = {2006}
}
@article{Martin1980,
author = {Martin, Linda G},
isbn = {0032-4728},
journal = {Population studies},
number = {2},
pages = {381--395},
title = {{A modification for use in destabilized populations of Brass's technique for estimating completeness of death registration}},
volume = {34},
year = {1980}
}
@inproceedings{Gotmare2019,
abstract = {The convergence rate and final performance of common deep learning models have significantly benefited from heuristics such as learning rate schedules, knowledge distillation, skip connections, and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit such analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz., mode connectivity and canonical correlation analysis (CCA), and hypothesize reasons for the success of the heuristics. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectivity and CCA. Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed to the deeper layers.},
archivePrefix = {arXiv},
arxivId = {1810.13243},
author = {Gotmare, Akhilesh and {Shirish Keskar}, Nitish and Xiong, Caiming and Socher, Richard},
booktitle = {7th International Conference on Learning Representations, ICLR 2019},
eprint = {1810.13243},
title = {{A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation}},
year = {2019}
}
@article{Lynch2001,
author = {Lynch, Scott M and Brown, J Scott},
isbn = {0070-3370},
journal = {Demography},
number = {1},
pages = {79--95},
title = {{Reconsidering mortality compression and deceleration: An alternative model of mortality rates}},
volume = {38},
year = {2001}
}
@article{Coale1991,
author = {Coale, Ansley J and Li, Shaomin},
isbn = {0070-3370},
journal = {Demography},
number = {2},
pages = {293--301},
title = {{The effect of age misreporting in China on the calculation of mortality rates at very high ages}},
volume = {28},
year = {1991}
}
@article{Deprez2017,
abstract = {Various stochastic models have been proposed to estimate mortality rates. In this paper we illustrate how machine learning techniques allow us to analyze the quality of such mortality models. In addition, we present how these techniques can be used for differentiating the different causes of death in mortality modeling.},
archivePrefix = {arXiv},
arxivId = {1705.03396},
author = {Deprez, Philippe and Shevchenko, Pavel V. and W{\"{u}}thrich, Mario V.},
doi = {10.1007/s13385-017-0152-4},
eprint = {1705.03396},
isbn = {2190-9733},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Boosting,Cause-of-death mortality,Machine learning,Mortality modeling,Regression},
number = {2},
pages = {337--352},
title = {{Machine learning techniques for mortality modeling}},
volume = {7},
year = {2017}
}
@article{Bebbington2012,
abstract = {Using a new distribution capable of exhibiting all the possible modes of accelerating and decelerating mortality, we conduct a systematic investigation of late-life mortality in humans. We check the insensitivity of the distribution to age cutoffs in the data relative to the logistic mortality model and propose a method to forecast evolution in the characteristic deceleration ages of the distribution. A number of data sets have been explored, with a particular emphasis on those originating from Scandinavia. Although those from Australia, Canada, and the USA are compatible with Gompertzian mortality, those from the other countries examined are not. We find in particular that the onset of mortality deceleration is being progressively delayed in Western societies but that there is evidence of mortality plateauing at earlier ages. {\textcopyright} 2014 Copyright Taylor {\&} Francis Group, LLC.},
author = {Bebbington, Mark and Green, Rebecca and Lai, Chin Diew and Zitikis, Ri{\v{c}}ardas},
doi = {10.1080/03461238.2012.676562},
isbn = {0346-1238
1651-2030},
issn = {03461238},
journal = {Scandinavian Actuarial Journal},
keywords = {force of mortality,life-tables,momentous ages,regression meta-analysis,senescent mortality},
number = {3},
pages = {189--207},
title = {{Beyond the Gompertz law: exploring the late-life mortality deceleration phenomenon}},
volume = {2014},
year = {2014}
}
@book{Goldburd,
author = {Goldburd, Mark and Khare, Anand and Tevet, Dan and Guller, Dmitriy},
edition = {Second Edi},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldburd et al. - Unknown - Casualty Actuarial Society CAS MONOGRAPH SERIES NUMBER 5 Second Edition GENERALIZED LINEAR MODELS FOR INSURA.pdf:pdf},
publisher = {Casualty Actuarial Society},
title = {{Generalized Linear Models for Insurance Rating}},
url = {www.casact.org},
year = {2020}
}
@article{LeCun1998,
author = {LeCun, Y and Bottou, L and Bengio, Y and Haffner, P},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
shorttitle = {Gradient-based learning applied to document recogn},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Hamilton2017,
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
archivePrefix = {arXiv},
arxivId = {1706.02216},
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
eprint = {1706.02216},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamilton, Ying, Leskovec - 2017 - Inductive Representation Learning on Large Graphs.pdf:pdf},
title = {{Inductive Representation Learning on Large Graphs}},
url = {http://arxiv.org/abs/1706.02216},
year = {2017}
}
@misc{Romo2020,
author = {Romo, Vladimir Canudas},
title = {{Australian Human Mortality Database | ANU School of Demography}},
url = {https://demography.cass.anu.edu.au/research/australian-human-mortality-database},
urldate = {2020-06-15},
year = {2020}
}
@article{PeterTerBerge2008,
author = {{Peter Ter Berge}},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peter Ter Berge - 2008 - Counterparty Portfolio modelling of default risk.pdf:pdf},
journal = {Life {\&} Pensions},
number = {April 2008},
pages = {29--33},
title = {{Counterparty Portfolio modelling of default risk}},
year = {2008}
}
@techreport{Dorrington2015a,
address = {Cape Town, South Africa},
author = {Dorrington, R E and Bradshaw, D and Laubscher, R and Nannan, N},
publisher = {South African Medical Research Council},
title = {{Rapid mortality surveillance report 2014}},
year = {2015}
}
@article{Rudin2019,
abstract = {Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to $\backslash$textit{\{}explain{\}} black box models, rather than creating models that are $\backslash$textit{\{}interpretable{\}} in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward -- it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.},
archivePrefix = {arXiv},
arxivId = {1811.10154},
author = {Rudin, Cynthia},
doi = {10.1038/s42256-019-0048-x},
eprint = {1811.10154},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rudin - 2019 - Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.pdf:pdf},
journal = {Nature Machine Intelligence},
number = {5},
pages = {206--215},
title = {{Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead}},
volume = {1},
year = {2019}
}
@article{quarg2004munich,
author = {Schmidt, Klaus D.},
doi = {10.1007/978-3-319-30056-6_27},
journal = {Bl{\"{a}}tter der DGVFM},
number = {4},
pages = {201--207},
publisher = {Springer},
title = {{Munich Chain Ladder Method}},
volume = {26},
year = {2016}
}
@article{Gerland2014,
author = {Gerland, Patrick},
isbn = {1744-1730},
journal = {Asian Population Studies},
number = {3},
pages = {274--303},
title = {{UN Population Division's Methodology in Preparing Base Population for Projections: case study for India}},
volume = {10},
year = {2014}
}
@article{Weidner2016,
abstract = {This paper presents pricing innovations to German car insurance. The purpose is to provide an effective approach to adapting actuarial pricing decision to incorporate telematic data, which differs substantially from established tariff criteria in complexity and volume. A vehicle mobility model and a real-world sample of driving profiles form the input into the analysis. We propose an allocation of the driving profiles based on velocity and acceleration parameters to specific driving styles for evaluating the driving behaviour to subsequently enable discounts or surcharges on the premiums to obtain usage-based insurance premiums. The result is highly relevant for actuaries, who calculate the tariffs, but also for managers, as they have to make a pricing decision.},
author = {Weidner, W and Transchel, F and Weidner, R},
doi = {10.1017/S1748499516000130},
edition = {09/13},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Weidner-2016-Telematic driving profile classif.pdf:pdf},
isbn = {1748-4995},
issn = {1748-4995},
journal = {Annals of Actuarial Science},
keywords = {Car insurance,Car insurance Pricing innovations Telematic Evalua,Driving styles,Evaluation of driving behaviour,Pricing innovations,Telematic},
number = {2},
pages = {213--236},
publisher = {Cambridge University Press},
shorttitle = {Telematic driving profile classification in car in},
title = {{Telematic driving profile classification in car insurance pricing}},
url = {https://www.cambridge.org/core/article/telematic-driving-profile-classification-in-car-insurance-pricing/E3EDF4CB87B1E4FC07B93E4E6251851D},
volume = {11},
year = {2016}
}
@article{Mcleod,
abstract = {This paper uses Wilkie's definitions of mutuality and solidarity to review the history of private healthcare in South Africa. The vision for a future unified national healthcare system is given and the phases of reform are outlined. The first phase of reforms has been completed and these are contextualised in terms of a return to solidarity principles. The elements of a planned social health insurance system are described and it is shown how income cross subsidies will further entrench the principles of solidarity. In the past, healthcare actuaries largely supported mutuality principles. The implications for the actuarial profession are suggested.},
author = {McLeod, HD},
doi = {10.4314/saaj.v5i1.24507},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcleod - Unknown - MUTUALITY AND SOLIDARITY IN HEALTHCARE IN SOUTH AFRICA.pdf:pdf},
isbn = {0)285721933},
issn = {1680-2179},
journal = {South African Actuarial Journal},
keywords = {Healthcare,community rating,medical schemes,minimum benefits,risk equalisation,social health insurance,social solidarity},
number = {1},
title = {{Mutuality and solidarity in healthcare in South Africa}},
volume = {5},
year = {2005}
}
@misc{UnitedNations2007,
address = {New York},
author = {{United Nations}, Population Division},
isbn = {9211512875},
publisher = {United Nations},
title = {{World population prospects: the 2006 revision}},
year = {2007}
}
@article{LeCun2015,
author = {LeCun, Y and Bengio, Y and Hinton, G},
issn = {1476-4687},
journal = {Nature},
number = {7553},
pages = {436},
shorttitle = {Deep Learning},
title = {{Deep Learning}},
volume = {521},
year = {2015}
}
@article{Wood2015,
author = {Wood, Simon},
journal = {R package version},
pages = {1.7--29},
title = {{Package ‘mgcv'}},
year = {2015}
}
@article{Breiman2001,
abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated bya given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical communityhas been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theoryand practice, has developed rapidlyin fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move awayfrom exclusive dependence on data models and adopt a more diverse set of tools. {\textcopyright} 2001 Institute of Mathematical Statistics.},
author = {Breiman, Leo},
doi = {10.1214/ss/1009213726},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Breiman-2001-Statistical modeling{\_} The two cul.pdf:pdf},
isbn = {0883-4237},
issn = {08834237},
journal = {Statistical Science},
number = {3},
pages = {199--215},
title = {{Statistical modeling: The two cultures}},
volume = {16},
year = {2001}
}
@book{Khanchel2019,
abstract = {In this paper, we discuss in more detail the context of Tunisian banks, with a focus on digital transformation strategies considered, the mission that preceded this study and the methodology and approach. Secondly, we will analyze the results of the study to reconstruct the features of the digital transformation of movement in Tunisian banks.The study of the digital transformation of Tunisian banks is carried on the board Matine Consulting firm. This study is launched in the continuity of the inaugural training program certifying on the topic of FinTechs and digital transformation of banks, set up by Matine in collaboration with the Academy of Banking and Finance (ABF).},
author = {Khanchel, Hanen},
booktitle = {Journal of Business Administration Research},
doi = {10.5430/jbar.v8n2p20},
file = {:C$\backslash$:/Users/user-pc/Desktop/10.1007@978-3-030-23719-6.pdf:pdf},
isbn = {9783030237189},
issn = {1927-9507},
number = {2},
pages = {20},
title = {{The Impact of Digital Transformation on Banking}},
volume = {8},
year = {2019}
}
@article{Ntozi1978,
author = {Ntozi, James Patrick Manyenye},
isbn = {0070-3370},
journal = {Demography},
number = {4},
pages = {509--521},
title = {{The Demeny-Shorter and three-census methods for correcting age data}},
volume = {15},
year = {1978}
}
@article{Gabrielli2019c,
author = {Gabrielli, Andrea},
doi = {10.2139/ssrn.3365517},
journal = {SSRN Electronic Journal},
month = {apr},
publisher = {Elsevier BV},
title = {{A Neural Network Boosted Double Over-Dispersed Poisson Claims Reserving Model}},
year = {2019}
}
@article{Currie,
author = {Currie, Iain D},
title = {{Fitting models of mortality with generalized linear and non-linear models}}
}
@article{Rubin1976,
author = {Rubin, Donald B},
isbn = {0006-3444},
journal = {Biometrika},
number = {3},
pages = {581--592},
title = {{Inference and missing data}},
volume = {63},
year = {1976}
}
@techreport{Park2019,
abstract = {Portfolio traders strive to identify dynamic portfolio allocation schemes so that their total budgets are well allocated through the investment horizon. This study proposes a novel portfolio trading strategy in which an intelligent agent is trained to identify an optimal trading action by using an algorithm called deep Q-learning. This study formulates a portfolio trading process as a Markov decision process in which the agent can learn about the financial market environment, and it identifies a deep neural network structure as an approximation of the Q-function. To ensure applicability to real-world trading, we devise three novel techniques that are both reasonable and implementable. First, the agent's action space is modeled as a combinatorial action space of trading directions with prespecified trading sizes for each asset. Second, we introduce a mapping function that can replace an initially-determined action that may be infeasible with a feasible action that is reasonably close to the original, ideal action. Last, we introduce a technique by which an agent simulates all feasible actions in each state and learns about these experiences to derive a multi-asset trading strategy that best reflects financial data. To validate our approach, we conduct backtests for two representative portfolios and demonstrate superior results over the benchmark strategies.},
archivePrefix = {arXiv},
arxivId = {1907.03665},
author = {Park, Hyungjun and Sim, Min Kyu and Choi, Dong Gu},
eprint = {1907.03665},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park, Sim, Choi - 2019 - An intelligent financial portfolio trading strategy using deep Q-learning.pdf:pdf},
keywords = {Deep Q-learning,Deep neural network,Markov decision process,Portfolio trading,Reinforcement learning},
title = {{An intelligent financial portfolio trading strategy using deep Q-learning}},
url = {http://arxiv.org/abs/1907.03665},
year = {2019}
}
@article{DeBrebisson2015,
author = {{De Br{\'{e}}bisson}, A and Simon, {\'{E}} and Auvolat, A and Vincent, P and Bengio, Y},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/De Br{\'{e}}bisson-2015-Artificial neural networks a(2).pdf:pdf},
journal = {arXiv},
shorttitle = {Artificial neural networks applied to taxi destina},
title = {{Artificial neural networks applied to taxi destination prediction}},
volume = {arXiv:1508},
year = {2015}
}
@article{Golden2016,
abstract = {An important development in personal lines of insurance in the United States is the use of credit history data for insurance risk classification to predict losses. This research presents the results of collaboration with industry conducted by a university at the request of its state legislature. The purpose was to see the viability and validity of the use of credit scoring to predict insurance losses given its controversial nature and criticism as redundant of other predictive variables currently used. Working with industry and government, this study analyzed more than 175,000 policyholders' information for the relationship between credit score and claims. Credit scores were significantly related to incurred losses, evidencing both statistical and practical significance. We investigate whether the revealed relationship between credit score and incurred losses was explainable by overlap with existing underwriting variables or whether the credit score adds new information about losses not contained in existing underwriting variables. The results show that credit scores contain significant information not already incorporated into other traditional rating variables (e.g., age, sex, driving history). We discuss how sensation seeking and self-control theory provide a partial explanation of why credit scoring works (the psycho-social perspective). This article also presents an overview of biological and chemical correlates of risk taking that helps explain why knowing risk-taking behavior in one realm (e.g., risky financial behavior and poor credit history) transits to predicting risk-taking behavior in other realms (e.g., automobile insurance incurred losses). Additional research is needed to advance new nontraditional loss prediction variables from social media consumer information to using information provided by technological advances. The evolving and dynamic nature of the insurance marketplace makes it imperative that professionals continue to evolve predictive variables and for academics to assist with understanding the whys of the relationships through theory development.},
author = {Golden, Linda L. and Brockett, Patrick L. and Ai, Jing and Kellison, Bruce},
doi = {10.1080/10920277.2016.1209118},
isbn = {1092-0277},
issn = {10920277},
journal = {North American Actuarial Journal},
number = {3},
pages = {233--251},
publisher = {Routledge},
title = {{Empirical Evidence on the Use of Credit Scoring for Predicting Insurance Losses with Psycho-social and Biochemical Explanations}},
url = {https://doi.org/10.1080/10920277.2016.1209118},
volume = {20},
year = {2016}
}
@article{Ihaka1996,
author = {Ihaka, Ross and Gentleman, Robert},
isbn = {1061-8600},
journal = {Journal of computational and graphical statistics},
number = {3},
pages = {299--314},
title = {{R: a language for data analysis and graphics}},
volume = {5},
year = {1996}
}
@article{NanLi,
author = {{Nan Li}, Patrick Gerland},
title = {{Using census data to estimate old-age mortality for developing countries}}
}
@article{CounciloftheEU2004,
author = {{Council of the EU}},
file = {:C$\backslash$:/R/refas/COUNCIL DIRECTIVE 2004-113-EC.pdf:pdf},
journal = {Official Journal of European Communities},
number = {L},
pages = {37--43},
title = {{Council Directive 2004/113/EC}},
volume = {373},
year = {2004}
}
@article{Gompertz1825,
author = {Gompertz, Benjamin},
isbn = {0261-0523},
journal = {Philosophical transactions of the Royal Society of London},
pages = {513--583},
title = {{On the nature of the function expressive of the law of human mortality, and on a new mode of determining the value of life contingencies}},
year = {1825}
}
@article{Smyl2020,
abstract = {This paper presents the winning submission of the M4 forecasting competition. The submission utilizes a dynamic computational graph neural network system that enables a standard exponential smoothing model to be mixed with advanced long short term memory networks into a common framework. The result is a hybrid and hierarchical forecasting method.},
author = {Smyl, Slawek},
doi = {10.1016/j.ijforecast.2019.03.017},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smyl - 2020 - A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Automatic differentiation,Dynamic computational graphs,Exponential smoothing,Forecasting competitions,Long short term memory (LSTM) networks,M4},
month = {jan},
number = {1},
pages = {75--85},
publisher = {Elsevier B.V.},
title = {{A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting}},
volume = {36},
year = {2020}
}
@misc{Sevcikova2020,
address = {New York},
author = {Sevcikova, H},
publisher = {United Nations Population Division},
title = {wpp2019},
url = {https://cran.r-project.org/web/packages/wpp2019/index.html},
year = {2020}
}
@inproceedings{Smith2017,
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate 'reasonable bounds' - linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
archivePrefix = {arXiv},
arxivId = {1506.01186},
author = {Smith, Leslie N.},
booktitle = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
doi = {10.1109/WACV.2017.58},
eprint = {1506.01186},
isbn = {9781509048229},
month = {may},
pages = {464--472},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Cyclical learning rates for training neural networks}},
year = {2017}
}
@article{Coale1990a,
author = {Coale, Ansley J and Kisker, Ellen E},
title = {{Defects in data on old-age mortality in the United States: new procedures for calculating mortality schedules and life tables at the highest ages}},
year = {1990}
}
@article{Deprez2017b,
abstract = {Various stochastic models have been proposed to estimate mortality rates. In this paper we illustrate how machine learning techniques allow us to analyze the quality of such mortality models. In addition, we present how these techniques can be used for differentiating the different causes of death in mortality modeling.},
archivePrefix = {arXiv},
arxivId = {1705.03396},
author = {Deprez, Philippe and Shevchenko, Pavel V. and W{\"{u}}thrich, Mario V.},
doi = {10.1007/s13385-017-0152-4},
eprint = {1705.03396},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Boosting,Cause-of-death mortality,Machine learning,Mortality modeling,Regression},
month = {dec},
number = {2},
pages = {337--352},
publisher = {Springer Berlin Heidelberg},
title = {{Machine learning techniques for mortality modeling}},
volume = {7},
year = {2017}
}
@book{hyndman2018forecasting,
abstract = {It presents the four basic stages of conducting an LCA: goal and scope definition, inventory analysis, impact assessment, and improvement analysis. The major stages in an LCA study are raw material acquisition, materials manufacture, production, use/reuse/maintenance, and waste management. The system boundaries, assumptions, and conventions to be addressed in each stage are presented. This document is designed to be an educational tool for someone who wants to learn the basics of LCA,},
author = {Papalambros, Panos Y. and Wilde, Douglass J.},
booktitle = {Principles of Optimal Design},
doi = {10.1017/9781316451038.010},
issn = {0028-0836},
pages = {421--455},
publisher = {OTexts},
title = {{Principles and Practice}},
year = {2018}
}
@article{StatsSA2008,
author = {{Stats SA}},
journal = {Statistical release P},
title = {{Mortality and causes of death in South Africa, 2006: Findings from death notification}},
volume = {309},
year = {2008}
}
@article{Joubert2012,
abstract = {The value of good-quality mortality data for public health is widely acknowledged. While effective civil registration systems remains the 'gold standard' source for continuous mortality measurement, less than 25{\%} of deaths are registered in most African countries. Alternative data collection systems can provide mortality data to complement those from civil registration, given an understanding of data source characteristics and data quality. We aim to document mortality data sources in post-democracy South Africa; to report on availability, limitations, strengths, and possible complementary uses of the data; and to make recommendations for improved data for mortality measurement. Civil registration and alternative mortality data collection systems, data availability, and complementary uses were assessed by reviewing blank questionnaires, death notification forms, death data capture sheets, and patient cards; legislation; electronic data archives and databases; and related information in scientific journals, research reports, statistical releases, government reports and books. Recent transformation has enhanced civil registration and official mortality data availability. Additionally, a range of mortality data items are available in three population censuses, three demographic surveillance systems, and a number of national surveys, mortality audits, and disease notification programmes. Child and adult mortality items were found in all national data sources, and maternal mortality items in most. Detailed cause-of-death data are available from civil registration and demographic surveillance. In a continent often reported as lacking the basic data to infer levels, patterns and trends of mortality, there is evidence of substantial improvement in South Africa in the availability of data for mortality assessment. Mortality data sources are many and varied, providing opportunity for comparing results and improved public health planning. However, more can and must be done to improve mortality measurement by improving data quality, triangulating data, and expanding analytic capacity. Cause data, in particular, must be improved.},
annote = {Joubert, Jane
Rao, Chalapati
Bradshaw, Debbie
Dorrington, Rob E
Vos, Theo
Lopez, Alan D
eng
Research Support, Non-U.S. Gov't
Review
Sweden
2013/01/01 06:00
Glob Health Action. 2012 Dec 27;5:1-19. doi: 10.3402/gha.v5i0.19263.},
author = {Joubert, J and Rao, C and Bradshaw, D and Dorrington, R E and Vos, T and Lopez, A D},
doi = {10.3402/gha.v5i0.19263},
isbn = {1654-9880 (Electronic)
1654-9880 (Linking)},
journal = {Glob Health Action},
keywords = {Adolescent,Adult,Age Distribution,Aged,Cause of Death,Child,Child, Preschool,Data Collection/*methods,Data Interpretation, Statistical,Demography/*trends,Female,Humans,Infant,Infant, Newborn,Male,Middle Aged,Mortality/*trends,Population Surveillance/*methods,Registries,Sex Distribution,South Africa/epidemiology,Young Adult},
pages = {1--19},
pmid = {23273252},
title = {{Characteristics, availability and uses of vital registration and other mortality data sources in post-democracy South Africa}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23273252},
volume = {5},
year = {2012}
}
@article{EuropeanCommission2015,
author = {{European Commission}},
file = {:C$\backslash$:/R/refas/MEMO-12-1012{\_}EN.pdf:pdf},
number = {December},
pages = {2012--2014},
title = {{Factsheet: EU rules on gender-neutral pricing in insurance}},
url = {http://europa.eu/rapid/press-release{\_}MEMO-12-1012{\_}en.htm},
year = {2015}
}
@article{Feeney1990,
author = {Feeney, Griffith and Kiyoshi, Hamano},
doi = {10.2307/132492},
isbn = {00956848},
journal = {Journal of Japanese Studies},
number = {1},
pages = {1--30},
publisher = {The Society for Japanese Studies},
title = {{Rice Price Fluctuations and Fertility in Late Tokugawa Japan}},
url = {http://www.jstor.org/stable/132492},
volume = {16},
year = {1990}
}
@incollection{Sriboonchitta2018,
abstract = {Traditionally, in statistics, it was implicitly assumed that models which are the best predictors also have the best explanatory power. Lately, many examples have been provided that show that the best predictive models are often different from the best explanatory models. In this paper, we provide a theoretical explanation for this difference.},
author = {Sriboonchitta, Songsak and Longpr{\'{e}}, Luc and Kreinovich, Vladik and Dumrongpokaphan, Thongchai},
booktitle = {Studies in Computational Intelligence},
doi = {10.1007/978-3-030-04263-9_12},
issn = {1860949X},
pages = {163--171},
publisher = {Springer Verlag},
title = {{Why the best predictive models are often different from the best explanatory models: A theoretical explanation}},
volume = {808},
year = {2018}
}
@article{Merli1998,
author = {Merli, M Giovanna},
isbn = {0070-3370},
journal = {Demography},
number = {3},
pages = {345--360},
title = {{Mortality in Vietnam, 1979–1989}},
volume = {35},
year = {1998}
}
@article{Guillen2012,
abstract = {Actuarial science has received an enormous influence from statistics since the early times. However, in the recent decades, the interplay between those two disciplines is somehow different compared to the past, with more emphasis on large-scale data analysis and prediction modelling based on customer historical records. In this respect, very little seems to have been said about the quality of data that are being stored in insurance companies' databases, even though data are the fuel for actuarial modelling. The first idea that came into my mind when the Editor invited me to write this guest editorial was to address gender discrimination. The new European regulation, that is supposed to be effective at the end of 2012, imposes a ban on insurance services which are not allowed to treat men and women as different types of customers. No news, on the grounds that this is already the case in some other parts of the globe. But then I thought this topic is just a small grain of sand in the desert, compared to amuch wider problem. How much data on customers is gathered by insurers? Is it worth storing billions of Megabytes? Who takes care about data quality? How dependent are we on the good quality of insurance data? Unfortunately, we all believe, and especially IT departments do, that a treasure is hidden in the databases. Something that is unknown, but extremely valuable. As a result, a significantly larger amount of budget is spent on backing up, than on looking at what is useful in the data files. Let me first briefly comment on the European gender nightmare and then, in the second part, I will return to the importance of data quality.},
author = {Guill{\'{e}}n, Montserrat},
doi = {10.1017/s1748499512000115},
issn = {1748-4995},
journal = {Annals of Actuarial Science},
number = {2},
pages = {231--234},
publisher = {Cambridge University Press (CUP)},
title = {{Sexless and beautiful data: from quantity to quality}},
volume = {6},
year = {2012}
}
@article{Horiuchi1988,
author = {Horiuchi, Shiro and Preston, Samuel H},
isbn = {0070-3370},
journal = {Demography},
number = {3},
pages = {429--441},
title = {{Age-specific growth rates: The legacy of past population dynamics}},
volume = {25},
year = {1988}
}
@article{Ayuso2016,
abstract = {In this paper we employ survival analysis methods to analyse the impact of driving patterns on distance travelled before a first claim is made by young drivers underwriting a pay-as-you-drive insurance scheme. An empirical application is presented in which we analyse real data collected by a GPS system from a leading Spanish insurer. We show that men have riskier driving patterns than women and, moreover, that there are gender differences in the impact driving patterns have on the risk of being involved in an accident. The implications of these results are discussed in terms of the 'no-gender' discrimination regulation.},
author = {Ayuso, Mercedes and Guill{\'{e}}n, Montserrat and {P{\'{e}}rez Mar{\'{i}}n}, Ana Mar{\'{i}}a},
doi = {10.1016/j.trc.2016.04.004},
issn = {0968090X},
journal = {Transportation Research Part C: Emerging Technologies},
keywords = {Gender discrimination,Motor insurance,Pricing,Survival analysis,Telematics},
month = {jul},
pages = {160--167},
publisher = {Elsevier Ltd},
title = {{Using GPS data to analyse the distance travelled to the first accident at fault in pay-as-you-drive insurance}},
volume = {68},
year = {2016}
}
@article{Rawat2017,
abstract = {Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have contributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications. We also introduce some of their current trends and remaining challenges.},
author = {Rawat, Waseem and Wang, Zenghui},
doi = {10.1162/NECO_a_00990},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Rawat-2017-Deep convolutional neural networks.pdf:pdf},
isbn = {0899-7667},
issn = {1530888X},
journal = {Neural Computation},
number = {9},
pages = {2352--2449},
shorttitle = {Deep convolutional neural networks for image class},
title = {{Deep convolutional neural networks for image classification: A comprehensive review}},
volume = {29},
year = {2017}
}
@article{Moro2017,
abstract = {The new Solvency II Directive and the upcoming IFRS 17 regime bring significant changes to current reporting of insurance entities, and particularly in relation to valuation of insurance liabilities. Insurers will be required to valuate their insurance liabilities on a risk-adjusted basis to allow for uncertainty inherent in cash flows that arise from the liability of insurance contracts. Whilst most European-based insurers are expected to adopt the Cost of Capital approach to calculate reserve risk margin - the risk adjustment method commonly agreed under Solvency II and IFRS 17, there is one additional requirement of IFRS 17 to also disclose confidence level of the risk margin. Given there is no specific guidance on the calculation of confidence level, the purpose of this paper is to explore and examine practical ways of estimating the risk margin confidence level measured by Probability of Sufficiency (PoS). The paper provides some practical approximation formulae that would allow one to quickly estimate the implied PoS of Solvency II risk margin for a given non-life insurance liability, the risk profile of which is specified by the type and characteristics of the liability (e.g. type/nature of business, liability duration and convexity, etc.), which, in turn, are associated with • the level of variability measured by Coefficient of Variation (CoV); • the degree of Skewness per unit of CoV; and • the degree of Kurtosis per unit of CoV2. The approximation formulae of PoS are derived for both the standalone class risk margin and the diversified risk margin at the portfolio level.},
author = {Moro, Eric Dal and Krvavych, Yuriy},
doi = {10.1017/asb.2017.12},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moro, Krvavych - 2017 - PROBABILITY of SUFFICIENCY of SOLVENCY II RESERVE RISK MARGINS PRACTICAL APPROXIMATIONS.pdf:pdf},
issn = {17831350},
journal = {ASTIN Bulletin},
keywords = {IFRS 17,IFRS confidence level of Solvency II risk margins,approximations,cost of capital approach,probability of sufficiency},
number = {3},
pages = {737--785},
title = {{PROBABILITY of SUFFICIENCY of SOLVENCY II RESERVE RISK MARGINS: PRACTICAL APPROXIMATIONS}},
volume = {47},
year = {2017}
}
@article{Lee1992,
abstract = {Time series methods are used to make long-run forecasts, with confidence intervals, of age-specific mortality in the United States from 1990 to 2065. First, the logs of the age-specific death rates are modeled as a linear function of an unobserved period-specific intensity index, with parameters depending on age. This model is fit to the matrix of U.S. death rates, 1933 to 1987, using the singular value decomposition (SVD) method; it accounts for almost all the variance over time in age-specific death rates as a group. Whereas e0 has risen at a decreasing rate over the century and has decreasing variability, k(t) declines at a roughly constant rate and has roughly constant variability, facilitating forecasting. k(t), which indexes the intensity of mortality, is next modeled as a time series (specifically, a random walk with drift) and forecast. The method performs very well on within-sample forecasts, and the forecasts are insensitive to reductions in the length of the base period from 90 to 30 years; some instability appears for base periods of 10 or 20 years, however. Forecasts of age-specific rates are derived from the forecasts of k, and other life table variables are derived and presented. These imply an increase of 10.5 years in life expectancy to 86.05 in 2065 (sexes combined), with a confidence band of plus 3.9 or minus 5.6 years, including uncertainty concerning the estimated trend. Whereas 46{\%} now survive to age 80, by 2065 46{\%} will survive to age 90. of the gains forecast for person-years lived over the life cycle from now until 2065, 74{\%} will occur at age 65 and over. These life expectancy forecasts are substantially lower than direct time series forecasts of e0, and have far narrower confidence bands; however, they are substantially higher than the forecasts of the Social Security Administration's Office of the Actuary. {\textcopyright} 1992 Taylor {\&} Francis Group, LLC.},
author = {Lee, Ronald D. and Carter, Lawrence R.},
doi = {10.1080/01621459.1992.10475265},
isbn = {0162-1459},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Demography,Forecast,Life expectancy,Mortality,Population,Projection},
number = {419},
pages = {659--671},
title = {{Modeling and forecasting U.S. mortality}},
volume = {87},
year = {1992}
}
@article{WorldHealthOrganization2015,
author = {{World Health Organization}},
title = {{World Health Statistics 2015}},
year = {2015}
}
@book{Koch2016,
address = {Port Elizabeth},
author = {Koch, R J},
publisher = {Van Zyl Rudd},
title = {{The Quantum Yearbook}},
year = {2016}
}
@misc{Gesmann2017,
author = {Gesmann, Markus and Murphy, Daniel and Zhang, Wayne and Carrato, Alessandro and Crupi, Giuseppe and Wuthrich, Mario},
booktitle = {Proc. of the R User Conference, August},
pages = {12--14},
title = {{ChainLadder: Statistical Methods and Models for Claims Reserving in General Insurance. R package version 0.2.0}},
url = {http://cran.r-project.org/package=ChainLadder},
year = {2015}
}
@article{Bank2018,
author = {{Prudential Authority}},
pages = {1--17},
publisher = {Governance of Insurers PRudential Standards},
title = {{Prudential Standard GOI 3 Risk Management and Internal Controls for Insurers}},
volume = {2017},
year = {2018}
}
@article{Dorrington2011,
author = {Dorrington, Rob E and Bradshaw, Debbie},
isbn = {1443-2447},
journal = {Journal of Population Research},
number = {1},
pages = {49--73},
title = {{Maternal mortality in South Africa: lessons from a case study in the use of deaths reported by households in censuses and surveys}},
volume = {28},
year = {2011}
}
@misc{Wilmoth2020,
author = {Wilmoth, John and Barbieri, Magali and Winant, Celeste and Coche, Arnaud and Boe, Carl and Andreeva, Mila and Yang, Lisa},
title = {{United States Mortality Database}},
url = {https://usa.mortality.org/},
urldate = {2020-06-15},
year = {2020}
}
@book{Chollet2017,
abstract = {applicability for this approach.},
author = {Ketkar, Nikhil},
booktitle = {Deep Learning with Python},
doi = {10.1007/978-1-4842-2766-4},
isbn = {1617294438},
publisher = {Manning Publications Co.},
title = {{Deep Learning with Python}},
year = {2017}
}
@article{Preston1993,
author = {Preston, Samuel H},
isbn = {0070-3370},
journal = {Demography},
number = {4},
pages = {593--606},
title = {{The contours of demography: Estimates and projections}},
volume = {30},
year = {1993}
}
@article{Cohen2006a,
author = {Cohen, Barney and Menken, Jane and Velkoff, Victoria A and Kowal, Paul R},
title = {{Aging in Sub-Saharan Africa: The Changing Demography of the Region}},
year = {2006}
}
@article{Boonen2017a,
abstract = {This paper examines the consequences for a life annuity insurance company if the solvency II solvency capital requirements (SCR) are calibrated based on expected shortfall (ES) instead of value-at-risk (VaR). We focus on the risk modules of the SCRs for the three risk classes equity risk, interest rate risk and longevity risk. The stress scenarios are determined using the calibration method proposed by EIOPA in 2014. We apply the stress-scenarios for these three risk classes to a fictitious life annuity insurance company. We find that for EIOPA's current quantile 99.5{\%} of the VaR, the stress scenarios of the various risk classes based on ES are close to the stress scenarios based on VaR. Might EIOPA choose to calibrate the stress scenarios on a smaller quantile, the longevity SCR is relatively larger and the equity SCR is relatively smaller if ES is used instead of VaR. We derive the same conclusion if stress scenarios are determined with empirical stress scenarios.},
author = {Boonen, Tim J.},
doi = {10.1007/s13385-017-0160-4},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Boonen-2017-Solvency II solvency capital requi.pdf:pdf},
isbn = {2190-9733},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Expected shortfall,Solvency II,Solvency capital requirement,Value-at-risk},
number = {2},
pages = {405--434},
title = {{Solvency II solvency capital requirement for life insurance companies based on expected shortfall}},
volume = {7},
year = {2017}
}
@article{Mack1993a,
author = {Mack, T},
isbn = {1783-1350},
journal = {ASTIN Bulletin},
number = {02},
pages = {213--225},
title = {{Distribution-free calculation of the standard error of chain ladder reserve estimates}},
volume = {23},
year = {1993}
}
@article{England1999,
abstract = {In England and Verrall [Insur. Math. Econ. 25 (1999) 281], an appropriate residual definition was considered for use in a bootstrap exercise to provide a computationally simple method of obtaining reserve prediction errors for the chain ladder model. However, calculation of the first two moments of the predictive distribution only was considered. In this paper, the method is extended by using a two-stage process: bootstrapping to obtain the estimation error and simulation to obtain the process error. This has the advantage of providing realisations from the whole predictive distribution, rather than just the first two moments. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {England, Peter},
doi = {10.1016/S0167-6687(02)00161-0},
isbn = {0167-6687},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Bootstrap,Claims reserving,Prediction errors},
number = {3},
pages = {461--466},
title = {{Addendum to "Analytic and bootstrap estimates of prediction errors in claims reserving"}},
volume = {31},
year = {2002}
}
@article{Gage1985,
author = {Gage, Timothy B},
isbn = {00113204, 15375382},
journal = {Current Anthropology},
number = {5},
pages = {644--647},
publisher = {[University of Chicago Press, Wenner-Gren Foundation for Anthropological Research]},
title = {{Demographic Estimation From Anthropological Data: New Methods}},
url = {http://www.jstor.org.ezproxy.uct.ac.za/stable/2743086},
volume = {26},
year = {1985}
}
@article{Howard2020,
abstract = {fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We have used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.},
archivePrefix = {arXiv},
arxivId = {2002.04688},
author = {Howard, Jeremy and Gugger, Sylvain},
doi = {10.3390/info11020108},
eprint = {2002.04688},
issn = {2078-2489},
journal = {Information},
keywords = {data processing pipelines,deep learning},
month = {feb},
number = {2},
pages = {108},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{fastai: A Layered API for Deep Learning}},
url = {https://www.mdpi.com/2078-2489/11/2/108 http://arxiv.org/abs/2002.04688},
volume = {11},
year = {2020}
}
@article{Bahdanau2015,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
eprint = {1409.0473},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - 2015 - Neural machine translation by jointly learning to align and translate.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--15},
title = {{Neural machine translation by jointly learning to align and translate}},
year = {2015}
}
@article{Sweeting2011b,
abstract = {This paper builds on the two-factor mortality model known as the Cairns-Blake-Dowd (CBD) model, which is used to project future mortality. It is shown that these two factors do not follow a random walk, as proposed in the original model, but that each should instead be modelled as a random fluctuation around a trend, the trend changing periodically. The paper uses statistical techniques to determine the points at which there are statistically significant changes in each trend. The frequency of change in each trend is then used to project the frequency of future changes, and the sizes of historical changes are used to project the sizes of future changes. The results are then presented as fan charts, and used to estimate the range of possible future outcomes for period life expectancies. These projections show that modelling mortality rates in this way leaves much greater uncertainty over future life expectancy in the long term.},
author = {Sweeting, P. J.},
doi = {10.1017/s1748499511000017},
issn = {1748-4995},
journal = {Annals of Actuarial Science},
keywords = {Cairns-Blake-Dowd,Lee-Carter,Mortality Projections,Mortality Risk,Mortality Trends,Stochastic Modelling,Stochastic Mortality},
month = {sep},
number = {2},
pages = {143--162},
publisher = {Cambridge University Press (CUP)},
title = {{A Trend-Change Extension of the Cairns-Blake-Dowd Model}},
volume = {5},
year = {2011}
}
@article{Bhat1990,
abstract = {This article outlines a method of estimating probabilities of gross transfers from one age to another due to misreporting of age. An essential ingredient in the computation is the information on the true age structure of the population, which may be estimated by using generalized stable population relationships. The method consists essentially of iteratively adjusting rows and columns of an initial guess matrix so that the application of the resultant transition matrix to the true age distribution produces the recorded age distribution. The initial guess matrix can be either empirically based or theoretically derived. The method is illustrated by using data on India, 1971-1981. The application reveals that in India, although the tendency to exaggerate age is strong at adult ages, the bias does not increase appreciably at older ages, as is commonly believed. {\textcopyright} 1990 Population Association of America.},
author = {Bhat, P. N Mari},
doi = {10.2307/2061559},
isbn = {0070-3370},
issn = {00703370},
journal = {Demography},
number = {1},
pages = {149--163},
pmid = {2303136},
publisher = {Springer},
title = {{Estimating transition probabilities of age misstatement}},
volume = {27},
year = {1990}
}
@article{Depoid1973,
author = {D{\'{e}}poid, Fran{\c{c}}oise},
isbn = {0032-4663},
journal = {Population (french edition)},
pages = {755--792},
title = {{La mortalit{\'{e}} des grands vieillards}},
year = {1973}
}
@article{lecun1998mnist,
abstract = {The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.$\backslash$r$\backslash$nIt is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.},
author = {LeCun, Yann and Cortes, Corinna and Burges, Christopher},
journal = {The Courant Institute of Mathematical Sciences},
pages = {1--10},
title = {{THE MNIST DATABASE of handwritten digits}},
year = {1998}
}
@article{Zhao2016,
author = {Zhao, Qingyuan and Hastie, Trevor},
file = {:C$\backslash$:/R/refas/pdp{\_}zhao.pdf:pdf},
title = {{CAUSAL INTERPRETATIONS OF BLACK-BOX MODELS QINGYUAN ZHAO AND TREVOR HASTIE Department of Statistics, Stanford University}},
year = {2016}
}
@misc{Ng2017,
author = {Gao, Tianxiang},
number = {24 June},
pages = {1--23},
publisher = {Coursera},
title = {{Deep Learning Specialization}},
url = {https://www.coursera.org/specializations/deep-learning},
volume = {2018},
year = {2018}
}
@article{Kuo2019,
abstract = {We propose a novel approach for loss reserving based on deep neural networks. The approach allows for joint modeling of paid losses and claims outstanding, and incorporation of heterogeneous inputs. We validate the models on loss reserving data across lines of business, and show that they improve on the predictive accuracy of existing stochastic methods. The models require minimal feature engineering and expert input, and can be automated to produce forecasts more frequently than manual workflows.},
archivePrefix = {arXiv},
arxivId = {1804.09253},
author = {Kuo, Kevin},
doi = {10.3390/risks7030097},
eprint = {1804.09253},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuo - 2019 - Deeptriangle A deep learning approach to loss reserving.pdf:pdf},
issn = {22279091},
journal = {Risks},
keywords = {Loss reserving,Machine learning,Neural networks},
month = {sep},
number = {3},
publisher = {MDPI AG},
title = {{Deeptriangle: A deep learning approach to loss reserving}},
volume = {7},
year = {2019}
}
@article{Carnes1996,
abstract = {"Scientists have long attempted to explain why closely similar age patterns of death are characteristic of highly diverse human and non human populations....Early efforts [to develop a general law of mortality] were conducted using mortality curves based on all causes of death. The authors predict that if comparisons of mortality are based instead on [inverted question mark]intrinsic' causes of death (i.e., deaths that reflect the basic biology of the organism), then age patterns of mortality consistent with the historical concept of a law might be revealed. Using data on laboratory animals and humans, they demonstrate that age patterns of intrinsic mortality overlap when graphed on a biologically comparable time scale. These results are consistent with the existence of a law of mortality following sexual maturity, as originally asserted by Benjamin Gompertz and Raymond Pearl. The societal, medical, and research implications of such a law are discussed." (SUMMARY IN SPA AND FRE) excerpt},
author = {Carnes, Bruce A. and Olshansky, S. Jay and Grahn, Douglas},
doi = {10.2307/2137434},
isbn = {0098-7921},
issn = {00987921},
journal = {Population and Development Review},
number = {2},
pages = {231},
title = {{Continuing the Search for a Law of Mortality}},
volume = {22},
year = {1996}
}
@article{Currie2016,
abstract = {Many common models of mortality can be expressed compactly in the language of either generalized linear models or generalized non-linear models. The R language provides a description of these models which parallels the usual algebraic definitions but has the advantage of a transparent and flexible model specification. We compare eight model structures for mortality. For each structure, we consider (a) the Poisson models for the force of mortality with both log and logit link functions and (b) the binomial models for the rate of mortality with logit and complementary log–log link functions. Part of this work shows how to extend the usual smooth two-dimensional P-spline model for the force of mortality with Poisson error and log link to the other smooth two-dimensional P-spline models with Poisson and binomial errors defined in (a) and (b). Our comments are based on the results of fitting these models to data from six countries: Australia, France, Japan, Sweden, UK and USA. We also discuss the possibility of forecasting with these models; in particular, the introduction of cohort terms generally leads to an improvement in overall fit, but can also make forecasting with these models problematic.},
author = {Currie, Iain D.},
doi = {10.1080/03461238.2014.928230},
isbn = {0346-1238},
issn = {16512030},
journal = {Scandinavian Actuarial Journal},
keywords = {R language,constraints,forecasting,generalized linear models,identifiability,mortality},
number = {4},
pages = {356--383},
title = {{On fitting generalized linear and non-linear models of mortality}},
volume = {2016},
year = {2016}
}
@inproceedings{Saxena1985,
author = {Saxena, P C and Gogte, B H},
booktitle = {Asian and Pacific Census Forum},
number = {3},
pages = {5--9},
title = {{On Feeneys method for correcting age distributions for heaping on multiples of five}},
volume = {11},
year = {1985}
}
@article{Heligman1980,
author = {Heligman, Larry and Pollard, John H},
journal = {Journal of the Institute of Actuaries},
number = {01},
pages = {49--80},
title = {{The age pattern of mortality}},
volume = {107},
year = {1980}
}
@article{Hill2009,
author = {Hill, Kenneth and You, Danzhen and Choi, Yoonjoung},
doi = {10.4054/DemRes.2009.21.9},
isbn = {1435-9871},
journal = {Demographic Research},
pages = {235--254},
title = {{Death distribution methods for estimating adult mortality}},
volume = {21},
year = {2009}
}
@inproceedings{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
booktitle = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/v1/d14-1179},
eprint = {1406.1078},
isbn = {9781937284961},
pages = {1724--1734},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Learning phrase representations using RNN encoder-decoder for statistical machine translation}},
year = {2014}
}
@article{Freund1997,
abstract = {We consider the problem of dynamically apportionlng resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update rule of Littlestone and Warmuth [10] can be adapted to this model yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games and prediction of points in ]{\~{}}n We also show how the weight-update rule can be used to derive a new boosting algorithm which does not require prior knowledge about the performance of the weak learning algorithm.},
author = {Freund, Yoav and Schapire, Robert E.},
doi = {10.1006/jcss.1997.1504},
isbn = {9783540591191},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {1},
pages = {23--37},
title = {{A decision-theoretic generalization of on-line learning and an application to boosting}},
volume = {904},
year = {1995}
}
@techreport{Dutang2018,
abstract = {As the level of competition increases, pricing optimization is gaining a central role in most mature insurance markets, forcing insurers to optimise their rating and consider customer behaviour; the modeling scene for the latter is one currently dominated by frameworks based on Generalised Linear Models (GLMs). In this paper, we explore the applicability of novel machine learning techniques such as tree boosted models to optimise the proposed premium on prospective policyholders. Given their predictive gain over GLMs, we carefully analyse both the advantages and disadvatanges induced by their use.},
author = {Dutang, Christophe and Petrini, Leonardo},
booktitle = {Variance},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dutang, Petrini - 2018 - Machine Learning Methods to Perform Pricing Optimization. A Comparison with Standard GLMs.pdf:pdf},
number = {1},
pages = {69--89},
title = {{Machine Learning Methods to Perform Pricing Optimization. A Comparison with Standard GLMs}},
url = {http://www.variancejournal.org/issues/12-01/69.pdf},
volume = {12},
year = {2018}
}
@article{Himes1994,
author = {Himes, Christine L and Preston, Samuel H and Condran, Gretchen A},
isbn = {0032-4728},
journal = {Population Studies},
number = {2},
pages = {269--291},
title = {{A relational model of mortality at older ages in low mortality countries}},
volume = {48},
year = {1994}
}
@article{Rosenwaike1984,
abstract = {Recent official statistics show that life expectancy among both males and females in Puerto Rico exceeds that for males and females in the United States. Furthermore, the population of Puerto Rico appears to be one of the most longevous in the world, with levels of survival after age 45 exceeding that in all but a few countries. The validity of these longevity estimates is examined using various demographic techniques to investigate the accuracy of census and vital statistics data. The findings provide strong evidence of widespread overstatement of age at the older ages. Such overstatement produces mortality rates that are too low and thus overstate the expectation of life; as a result current life tables need to be modified.},
author = {Rosenwaike, I. and Preston, S. H.},
isbn = {0018-7143},
issn = {00187143},
journal = {Human Biology},
number = {3},
pages = {503--525},
title = {{Age overstatement and Puerto Rican longevity}},
volume = {56},
year = {1984}
}
@article{Bzdok2019a,
abstract = {Recent decades have seen dramatic progress in brain research. These advances were often buttressed by probing single variables to make circumscribed discoveries, typically through null hypothesis significance testing. New ways for generating massive data fueled tension between the traditional methodology that is used to infer statistically relevant effects in carefully chosen variables, and pattern-learning algorithms that are used to identify predictive signatures by searching through abundant information. In this article we detail the antagonistic philosophies behind two quantitative approaches: certifying robust effects in understandable variables, and evaluating how accurately a built model can forecast future outcomes. We discourage choosing analytical tools via categories such as ‘statistics' or ‘machine learning'. Instead, to establish reproducible knowledge about the brain, we advocate prioritizing tools in view of the core motivation of each quantitative analysis: aiming towards mechanistic insight or optimizing predictive accuracy.},
author = {Bzdok, Danilo and Ioannidis, John P.A.},
doi = {10.1016/j.tins.2019.02.001},
file = {:C$\backslash$:/Users/user-pc/Desktop/10.1016@j.tins.2019.02.001.pdf:pdf},
issn = {1878108X},
journal = {Trends in Neurosciences},
keywords = {big-data analytics,black box models,data science,deep learning,precision medicine,reproducibility},
number = {4},
pages = {251--262},
pmid = {30808574},
publisher = {Elsevier Ltd},
title = {{Exploration, Inference, and Prediction in Neuroscience and Biomedicine}},
url = {https://doi.org/10.1016/j.tins.2019.02.001},
volume = {42},
year = {2019}
}
@article{Preston1999,
author = {Preston, Samuel H and Elo, Irma T and Stewart, Quincy},
isbn = {0032-4728},
journal = {Population Studies},
number = {2},
pages = {165--177},
title = {{Effects of age misreporting on mortality estimates at older ages}},
volume = {53},
year = {1999}
}
@book{Meyers2015,
abstract = {The purpose of the monograph is to provide access to generalized linear models for loss reserving but initially with strong emphasis on the chain ladder. The chain ladder is formulated in a GLM context, as is the statistical distribution of the loss reserve. This structure is then used to test the need for departure from the chain ladder model and to formulate any required model extensions. The chain ladder is by far the most widely used method for loss reserving. The chain ladder algorithm itself is non-stochastic, but Mack (1993) defined a stochastic version of the model and showed how a mean square error of prediction may be associated with any loss reserve obtained from this model. There are, however, two families of stochastic model which generate the chain ladder algorithm for the estimation of loss reserve, as discussed in Taylor (2011). They require differing treatments for the estimation of mean square error of prediction. Both families of model may be formulated as generalized linear models. This is not widely appreciated of the Mack model. The monograph commences with the identification of these two families and their respective GLM formulations. GLM formulation naturally invites the use of a bootstrap to estimate prediction error. The bootstrap estimates the entire distribution of loss reserve rather than just the mean square error of prediction obtainable from Mack's algorithm. The monograph discusses both parametric and semi-parametric forms of the GLM bootstrap.},
author = {Meyers, Glenn},
booktitle = {ASTIN Colloquium, May},
isbn = {9780962476273},
number = {1},
publisher = {Casualty Actuarial Society New York},
title = {{Stochastic Loss Reserving Using Bayesian MCMC Models}},
url = {http://www.casact.org/pubs/monographs/papers/01-Meyers.PDF},
year = {2013}
}
@techreport{Nakagawa2019,
abstract = {Stock return predictability is an important research theme as it reflects our economic and social organization, and significant efforts are made to explain the dynamism therein. Statistics of strong explanative power, called "factor" have been proposed to summarize the essence of predictive stock returns. Although machine learning methods are increasingly popular in stock return prediction, an inference of the stock returns is highly elusive, and still most investors, if partly, rely on their intuition to build a better decision making. The challenge here is to make an investment strategy that is consistent over a reasonably long period, with the minimum human decision on the entire process. To this end, we propose a new stock return prediction framework that we call Ranked Information Coefficient Neural Network (RIC-NN). RIC-NN is a deep learning approach and includes the following three novel ideas: (1) nonlinear multi-factor approach, (2) stopping criteria with ranked information coefficient (rank IC), and (3) deep transfer learning among multiple regions. Experimental comparison with the stocks in the Morgan Stanley Capital International (MSCI) indices shows that RIC-NN outperforms not only off-the-shelf machine learning methods but also the average return of major equity investment funds in the last fourteen years.},
archivePrefix = {arXiv},
arxivId = {1910.01491},
author = {Nakagawa, Kei and Abe, Masaya and Komiyama, Junpei},
eprint = {1910.01491},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nakagawa, Abe, Komiyama - 2019 - A Robust Transferable Deep Learning Framework for Cross-sectional Investment Strategy.pdf:pdf},
title = {{A Robust Transferable Deep Learning Framework for Cross-sectional Investment Strategy}},
url = {http://arxiv.org/abs/1910.01491},
year = {2019}
}
@book{StatsSA2014a,
author = {{Stats SA}},
publisher = {Statistics South Africa},
title = {{Mortality and Causes of Death in South Africa, 2011: Findings from Death Notification}},
year = {2014}
}
@misc{CenterforSystemsScienceandEngineeringCSSEatJohnsHopkinsUniversity2020,
abstract = {Novel Coronavirus (COVID-19) Cases, provided by JHU CSSE},
author = {{Center for Systems Science and Engineering (CSSE) at Johns Hopkins University}},
language = {en},
month = {jun},
title = {{COVID-19 Data Repository}},
url = {https://github.com/CSSEGISandData/COVID-19},
urldate = {2020-06-02},
year = {2020}
}
@inproceedings{Cairns2019a,
address = {Zurich},
author = {Cairns, Andrew J G},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cairns - Unknown - 'Medium' Data and Socio-Economic Mortality.pdf:pdf},
title = {{'Medium' Data and Socio-Economic Mortality}},
url = {www.actuaries.org.uk/arc},
year = {2019}
}
@article{Gabrielli2018,
author = {Gabrielli, A and W{\"{u}}thrich, M},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Gabrielli-2018-An Individual Claims History Si.pdf:pdf},
journal = {Risks},
number = {2},
pages = {29},
title = {{An Individual Claims History Simulation Machine}},
volume = {6},
year = {2018}
}
@article{danesi2015forecasting,
abstract = {The relative performance of multipopulation stochastic mortality models is investigated. When targeting mortality rates, we consider five extensions of the well known Lee-Carter single population extrapolative approach. As an alternative, we consider similar structures when mortality improvement rates are targeted. We use a dataset of deaths and exposures of Italian regions for the years 1974-2008 to conduct a comparison of the models, running a battery of tests to assess the relative goodness of fit and forecasting capability of different approaches. Results show that the preferable models are those striking a balance between complexity and flexibility.},
author = {Danesi, Ivan Luciano and Haberman, Steven and Millossovich, Pietro},
doi = {10.1016/j.insmatheco.2015.03.010},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Common period indices,Improvement rates,Lee-Carter model,Mortality forecasting,Related populations},
pages = {151--161},
publisher = {Elsevier},
title = {{Forecasting mortality in subpopulations using Lee-Carter type models: A comparison}},
volume = {62},
year = {2015}
}
@article{Wilson2020,
abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
archivePrefix = {arXiv},
arxivId = {2002.08791},
author = {Wilson, Andrew Gordon and Izmailov, Pavel},
eprint = {2002.08791},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gordon, Izmailov - Unknown - Bayesian Deep Learning and a Probabilistic Perspective of Generalization.pdf:pdf},
title = {{Bayesian Deep Learning and a Probabilistic Perspective of Generalization}},
url = {https://github.com/izmailovpavel/ http://arxiv.org/abs/2002.08791},
year = {2020}
}
@article{Nojilana2009,
author = {Nojilana, Beatrice and Groenewald, Pam and Bradshaw, Debbie and Reagon, Gavin},
isbn = {0038-2469},
journal = {South African Medical Journal},
number = {9},
pages = {648--652},
title = {{Quality of cause of death certification at an academic hospital in Cape Town, South Africa: original article}},
volume = {99},
year = {2009}
}
@misc{Feeney1997,
author = {Feeney, G},
pages = {6},
title = {{Census Survival Ratio Consistency Check}},
url = {http://www.gfeeney.com/research-notes/1997.census.survival.ratio.check/1997-census-survival-ratio-check.pdf},
year = {1997}
}
@article{McCulloch1975,
abstract = {The article examines the tax-adjusted yield curve. Tax-induced bias can substantially alter the shape of the yield curve if it is constructed from quotations on bonds selling below par. The inclusion of tax effects can actually reverse qualitative conclusions concerning the direction in which investors expect interest rates to move under the expectations hypothesis. The article modifies the author's technique for regression fitting the term structure of interest rates, described in an earlier paper, to eliminate tax-induced bias and reconcile observations on high and low coupon bonds.},
author = {McCulloch, J. Huston},
doi = {10.2307/2326860},
issn = {00221082},
journal = {The Journal of Finance},
month = {jun},
number = {3},
pages = {811},
publisher = {JSTOR},
title = {{The Tax-Adjusted Yield Curve}},
volume = {30},
year = {1975}
}
@article{Bhat1998,
abstract = {This study generates estimates of fertility and mortality in India, and compares the estimates generated by different methods and data during 1941-91. Data were obtained for pre-1970 from censuses, while post-1971 data was available from the Sample Registration System (SRS) and other demographic surveys. Mortality estimates were derived using the variable-r methodology of Preston and Bennet (1983) for 1971, 1981, and 1991. Interpolation factors used in estimating persons-years lived were taken from Bhat (1987). Analysis of life expectancy ratios between the census and SRS indicates that the census undercounted. Average estimated life expectancy for ages 10 and 15 (derived from cohort interpolation in the variable-r method) indicated reasonable estimates of adult mortality, when census completeness was stable. Infant and child mortality showed declines from the early 1970s based on adjusted SRS data and indirect estimates. Extrapolation methods of child mortality showed a slower decline than in the model tables and shifts between the West and South models. Census data on child/woman ratios indicate that fertility peaked during 1956-65 at 6.4-6.7 births/woman. Comparative estimates indicate that birth incompleteness in the SRS was around 10{\%} in the 1970s, about 5{\%} in the early 1980s, and about 10{\%} in the late 1980s. The analysis suggests that age specific mortality rates from the SRS do not require significant correction, but growth rates are erroneous. Crude birth rate SRS estimates showed a 9{\%} underregistration during 1971-81 and 7{\%} during 1981-91. The SRS correctly estimated age specific mortality risk but underreported births and deaths by 5-10{\%}.},
author = {Bhat, P N},
journal = {Demography India},
keywords = {INDIA RESEARCH REPORT COMPARATIVE STUDIES ESTIMATI},
number = {1},
pages = {23--57},
pmid = {137982},
title = {{Demographic estimates for post-independence India: a new integration}},
volume = {27},
year = {1998}
}
@article{Mathers2014,
author = {Mathers, Colin D and Stevens, Gretchen A and Boerma, Ties and White, Richard A and Tobias, Martin I},
isbn = {0140-6736},
journal = {The Lancet},
title = {{Causes of international increases in older age life expectancy}},
year = {2014}
}
@misc{Ishii2020,
author = {Ishii, Futoshi and Hayashi, Reiko and Izumida, Nobuyuki and Yamamoto, Katsuya and Beppu, Motomi and Korekawa, Yu},
title = {{The Japanese Mortality Database | National Institute of Population and Social Security Research}},
url = {http://www.ipss.go.jp/p-toukei/JMD/index-en.asp},
urldate = {2020-06-15},
year = {2020}
}
@article{Perks1932,
author = {Perks, Wilfred},
doi = {10.2307/41137425},
isbn = {00202681},
journal = {Journal of the Institute of Actuaries (1886-1994)},
number = {1},
pages = {12--57},
publisher = {Cambridge University Press on behalf of the Institute and Faculty of Actuaries},
title = {{ON SOME EXPERIMENTS IN THE GRADUATION OF MORTALITY STATISTICS}},
url = {http://www.jstor.org/stable/41137425},
volume = {63},
year = {1932}
}
@article{Grize2020,
abstract = {The literature on analytical applications in insurance tends to be either very general or rather technical, which may hold back the adoption of new important tools by industrial practitioners. Our goal is to stress that machine learning (ML) algorithms will play a significant role in the insurance industry in the near future and thus to encourage practitioners to learn and apply these techniques. After discussing the increasing relevance of data for nonlife insurance and briefly reviewing the major impact of digital technology on this business, we restrict our discussion to technical analytical applications and indicate where ML algorithms can add most value. We present two real examples: first a comparison of retention models for household insurance and then a dynamic pricing problem for online motor insurance. Both applications illustrate the advantages but also some of the difficulties of applying ML tools in practice. Finally, we mention some challenges posed by the use of ML in the industry and formulate a few recommendations for successful applications in insurance. This article is neither a tutorial nor an exhaustive review of technical ML applications in nonlife insurance. However, references for additional learning materials are provided.},
author = {Grize, Yves Laurent and Fischer, Wolfram and L{\"{u}}tzelschwab, Christian},
doi = {10.1002/asmb.2543},
file = {:C$\backslash$:/Users/user-pc/Downloads/asmb.2543.pdf:pdf},
issn = {15264025},
journal = {Applied Stochastic Models in Business and Industry},
keywords = {dynamic pricing,machine learning,nonlife insurance,retention models,statistics in business and industry},
month = {may},
pages = {asmb.2543},
publisher = {John Wiley and Sons Ltd},
title = {{Machine learning applications in nonlife insurance}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.2543},
year = {2020}
}
@misc{UNPopulationDivision1982,
address = {New York: United Nations, Department of Economic and Social Affairs, ST/ESA/SER.A/77.},
author = {{UN Population Division}},
title = {{Model life tables for developing countries}},
year = {1982}
}
@article{Doctor2012,
author = {Doctor, Henry V},
doi = {10.1155/2012/194187},
isbn = {2090-4029
2090-4037},
journal = {International Journal of Population Research},
pages = {1--5},
title = {{Estimates of Age-Specific Mortality Rates from Sequential Cross-Sectional Data in Malawi}},
volume = {2012},
year = {2012}
}
@article{Arel2010,
abstract = {Brain signal variation across different subjects and sessions significantly impairs the accuracy of most brain-computer interface (BCI) systems. Herein, we present a classification algorithm that minimizes such variation, using linear programming support-vector machines (LP-SVM) and their extension to multiple kernel learning methods. The minimization is based on the decision boundaries formed in classifiers' feature spaces and their relation to BCI variation. Specifically, we estimate subject/session-invariant features in the reproducing kernel Hilbert spaces (RKHS) induced with Gaussian kernels. The idea is to construct multiple subject/session-dependent RKHS and to perform classification with LP-SVMs. To evaluate the performance of the algorithm, we applied it to oxy-hemoglobin data sets acquired from eight sessions and seven subjects as they performed two different mental tasks. Results show that our classifiers maintain good performance when applied to random patterns across varying sessions/subjects. {\textcopyright} 2013 IPEM.},
author = {Abibullaev, Berdakh and An, Jinung and Jin, Sang Hyeon and Lee, Seung Hyun and Moon, Jeon Il},
doi = {10.1016/j.medengphy.2013.08.009},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Arel-2010-Deep machine learning-a new frontier.pdf:pdf},
isbn = {1556-603X},
issn = {13504533},
journal = {Medical Engineering and Physics},
keywords = {Brain-computer interfaces,Functional near-infrared spectroscopy,Inter-subject variability,Multiple kernel learning,RKHS,Support vector machines},
number = {12},
pages = {1811--1818},
shorttitle = {Deep machine learning-a new frontier in artificial},
title = {{Minimizing inter-subject variability in fNIRS-based brain-computer interfaces via multiple-kernel support vector learning}},
volume = {35},
year = {2013}
}
@misc{Hill2004,
address = {Marconi Conference Center, Marin County, California},
author = {Hill, K and Choi, Y},
booktitle = {Paper presented to workshop on “Adult Mortality in the Developing World: Methods and Measures”,},
title = {{Performance of GGB and SEG given various simulated data errors.}},
year = {2004}
}
@article{brouhns2002poisson,
abstract = {This paper implements Wilmoth's [Computational methods for fitting and extrapolating the Lee-Carter model of mortality change, Technical report, Department of Demography, University of California, Berkeley] and Alho's [North American Actuarial Journal 4 (2000) 91] recommendation for improving the Lee-Carter approach to the forecasting of demographic components. Specifically, the original method is embedded in a Poisson regression model, which is perfectly suited for age-sex-specific mortality rates. This model is fitted for each sex to a set of age-specific Belgian death rates. A time-varying index of mortality is forecasted in an ARIMA framework. These forecasts are used to generate projected age-specific mortality rates, life expectancies and life annuities net single premiums. Finally, a Brass-type relational model is proposed to adapt the projections to the annuitants population, allowing for estimating the cost of adverse selection in the Belgian whole life annuity market. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Brouhns, Natacha and Denuit, Michael and Vermunt, Joroen K.},
doi = {10.1016/S0167-6687(02)00185-3},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Adverse selection,Age-sex-specific mortality,Annuities,Life insurance,Lifetable functions},
month = {dec},
number = {3},
pages = {373--393},
publisher = {Elsevier},
title = {{A Poisson log-bilinear regression approach to the construction of projected lifetables}},
volume = {31},
year = {2002}
}
@article{Ovadia2019,
abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\{}$\backslash$em uncertainty{\}}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
archivePrefix = {arXiv},
arxivId = {1906.02530},
author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
eprint = {1906.02530},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ovadia et al. - Unknown - Can You Trust Your Model's Uncertainty Evaluating Predictive Uncertainty Under Dataset Shift.pdf:pdf},
title = {{Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift}},
url = {https://github.com/google-research/google-research/tree/master/uq http://arxiv.org/abs/1906.02530},
year = {2019}
}
@article{Currie2006,
author = {Currie, Iain D and Durban, Maria and Eilers, Paul H C},
isbn = {1467-9868},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
number = {2},
pages = {259--280},
title = {{Generalized linear array models with applications to multidimensional smoothing}},
volume = {68},
year = {2006}
}
@article{Richman2019f,
abstract = {Deep Learning models are currently being introduced into business processes to support decision-making in insurance companies. At the same time model risk is recognized as an increasingly relevant field within the management of operational risk that tries to mitigate the risk of poor business decisions because of flawed models or inappropriate model use. In this paper we try to determine how Deep Learning models are different from established actuarial models currently in use in insurance companies and how these differences might necessitate changes in the model risk management framework. We analyse operational risk in the development and implementation of Deep Learning models using examples from pricing and mortality forecasting to illustrate specific model risks and controls to mitigate those risks. We discuss changes in model governance and the role that model risk managers could play in providing assurance on the appropriate use of Deep Learning models.},
author = {Richman, Ronald and von Rummell, Nicolai and Wuthrich, Mario V.},
doi = {10.2139/ssrn.3444833},
file = {:C$\backslash$:/Users/user-pc/Desktop/Believing the Bot.pdf:pdf},
journal = {SSRN Electronic Journal},
keywords = {deep learning,insurance modelling,model risk,mortality forecasting,pricing},
pages = {1--40},
title = {{Believing the Bot - Model Risk in the Era of Deep Learning}},
year = {2019}
}
@book{lam2014enterprise,
author = {Lam, James},
publisher = {John Wiley {\&} Sons},
title = {{Enterprise risk management - from incentives to controls{\_}LAM.pdf}},
year = {2003}
}
@article{Stupp1988,
author = {Stupp, Paul W},
isbn = {0032-4701},
journal = {Population index},
pages = {209--224},
title = {{Estimating intercensal age schedules by intracohort interpolation}},
year = {1988}
}
@article{Pelizzari2019,
abstract = {The post-neoadjuvant setting in early breast cancer represents an attractive scenario for adjuvant clinical trials, offering the opportunity to test new drugs or combinations in high-risk patients who did not achieve pathologic complete response after primary treatment. No standard therapies are routinely proposed to patients with residual disease after neoadjuvant chemotherapy and few trials have explored this setting. To date, only one randomized phase III study showed the benefit of additional capecitabine after neoadjuvant chemotherapy, and international guidelines recommend at least to consider its use, particularly for triple negative breast cancer. Therefore, the management of these patients is still a clinical challenge, with limited data supporting the use of an additional adjuvant non-cross-resistant chemotherapy. Escalation strategies are currently under evaluation, with new agents proposed as supplementary post-neoadjuvant treatment (e.g. platinum salts, capecitabine, poly ADP-ribose polymerase inhibitors, immune checkpoint inhibitors, cyclin-dependent kinase 4/6 inhibitors). Based on these premises, selection criteria are critical to identify patients who may benefit from post-neoadjuvant therapies, through the validation of prognostic and predictive biomarkers for a reliable risk assessment and estimation of benefit. The present review summarizes the efforts in introducing new therapeutic options for patients with breast cancer and residual disease after neoadjuvant treatment, with a particular focus on the ongoing clinical trials and useful biomarkers for risk stratification.},
author = {Pelizzari, Giacomo and Gerratana, Lorenzo and Basile, Debora and Fanotto, Valentina and Bartoletti, Michele and Liguori, Alessia and Fontanella, Caterina and Spazzapan, Simon and Puglisi, Fabio},
doi = {10.1016/j.ctrv.2018.10.014},
file = {:C$\backslash$:/Users/user-pc/Desktop/1-s2.0-S0305737218301841-main.pdf:pdf},
issn = {15321967},
journal = {Cancer Treatment Reviews},
keywords = {Breast cancer,Post-neoadjuvant treatment,Residual disease,pCR},
number = {July 2018},
pages = {7--14},
publisher = {Elsevier},
title = {{Post-neoadjuvant strategies in breast cancer: From risk assessment to treatment escalation}},
url = {https://doi.org/10.1016/j.ctrv.2018.10.014},
volume = {72},
year = {2019}
}
@article{Terblanche2015,
author = {Terblanche, Wilma and Wilson, Tom},
doi = {10.1371/journal.pone.0123692},
isbn = {1932-6203},
journal = {Plos One},
number = {4},
pages = {e0123692},
title = {{An Evaluation of Nearly-Extinct Cohort Methods for Estimating the Very Elderly Populations of Australia and New Zealand}},
volume = {10},
year = {2015}
}
@article{England2002,
author = {England, Peter and Verrall, R. J.},
isbn = {2044-0456},
journal = {British Actuarial Journal},
number = {03},
pages = {443--518},
title = {{Stochastic claims reserving in general insurance}},
volume = {8},
year = {2002}
}
@article{Kohonen1990,
author = {Kohonen, T},
isbn = {0018-9219},
journal = {Proceedings of the IEEE},
number = {9},
pages = {1464--1480},
title = {{The self-organizing map}},
volume = {78},
year = {1990}
}
@article{Lee2004,
author = {Lee, Ronald},
isbn = {0098-7921},
journal = {Population and Development Review},
pages = {153--175},
title = {{Quantifying our ignorance: Stochastic forecasts of population and public budgets}},
year = {2004}
}
@article{Nigri2019,
abstract = {In the field of mortality, the Lee–Carter based approach can be considered the milestone to forecast mortality rates among stochastic models. We could define a “Lee–Carter model family” that embraces all developments of this model, including its first formulation (1992) that remains the benchmark for comparing the performance of future models. In the Lee–Carter model, the k t parameter, describing the mortality trend over time, plays an important role about the future mortality behavior. The traditional ARIMA process usually used to model k t shows evident limitations to describe the future mortality shape. Concerning forecasting phase, academics should approach a more plausible way in order to think a nonlinear shape of the projected mortality rates. Therefore, we propose an alternative approach the ARIMA processes based on a deep learning technique. More precisely, in order to catch the pattern of kt series over time more accurately, we apply a Recurrent Neural Network with a Long Short-Term Memory architecture and integrate the Lee–Carter model to improve its predictive capacity. The proposed approach provides significant performance in terms of predictive accuracy and also allow for avoiding the time-chunks' a priori selection. Indeed, it is a common practice among academics to delete the time in which the noise is overflowing or the data quality is insufficient. The strength of the Long Short-Term Memory network lies in its ability to treat this noise and adequately reproduce it into the forecasted trend, due to its own architecture enabling to take into account significant long-term patterns.},
author = {Nigri, Andrea and Levantesi, Susanna and Marino, Mario and Scognamiglio, Salvatore and Perla, Francesca},
doi = {10.3390/risks7010033},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nigri et al. - 2019 - A deep learning integrated Lee–Carter model(2).pdf:pdf},
issn = {22279091},
journal = {Risks},
keywords = {Deep learning,Forecasting,Lee–Carter model,Long short-term memory,Mortality},
month = {mar},
number = {1},
pages = {33},
title = {{A deep learning integrated Lee–Carter model}},
url = {https://www.mdpi.com/2227-9091/7/1/33},
volume = {7},
year = {2019}
}
@misc{Lecun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
booktitle = {Nature},
doi = {10.1038/nature14539},
issn = {14764687},
month = {may},
number = {7553},
pages = {436--444},
pmid = {26017442},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{Hainaut2018,
abstract = {This article proposes a neural-network approach to predict and simulate human mortality rates. This semi-parametric model is capable to detect and duplicate non-linearities observed in the evolution of log-forces of mortality. The method proceeds in two steps. During the first stage, a neural-network-based generalization of the principal component analysis summarizes the information carried by the surface of log-mortality rates in a small number of latent factors. In the second step, these latent factors are forecast with an econometric model. The term structure of log-forces of mortality is next reconstructed by an inverse transformation. The neural analyzer is adjusted to French, UK and US mortality rates, over the period 1946-2000 and validated with data from 2001 to 2014. Numerical experiments reveal that the neural approach has an excellent predictive power, compared to the Lee-Carter model with and without cohort effects.},
address = {Vienna, Austria},
author = {Hainaut, Donatien},
doi = {10.1017/asb.2017.45},
edition = {01/09},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Viola-2001-Rapid object detection using a boos.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Bengio-2009-Learning deep architectures for AI.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hinton-2006-Reducing the dimensionality of dat.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/LeCun-1998-Gradient-based learning applied to.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Nair-2010-Rectified linear units improve restr.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Bengio-2003-A neural probabilistic language mo.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Arel-2010-Deep machine learning-a new frontier.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hinton-2012-Improving neural networks by preve.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Bengio-Unsupervised feature learning and deep.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hochreiter-1997-Long short-term memory.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Krizhevsky-2012-Imagenet classification with d.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/LeCun-2015-Deep learning.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Gan-2015-Valuation of large variable annuity p.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Schmidhuber-2015-Deep learning in neural netwo.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Guo-2016-Entity embeddings of categorical vari.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Dong-2016-Characterizing driving styles with d.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Goldberg-2016-A primer on neural network model.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Dong-2017-Autoencoder regularized network for.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Smith-2016-A method of parameterising a feed f.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Goldberg-2017-Neural network methods for natur.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/W{\"{u}}thrich-2017-Covariate selection from telemat.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Weidner-2016-Classification of scale-sensitive.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/LLP-2017-The Chaotic Middle.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hejazi-2017-Efficient valuation of SCR via a n.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Ezzini-2018-Who is behind the wheel{\_} Driver id.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Khan-2018-A Guide to Convolutional Neural Netw.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Wijnands-2018-Identifying behavioural change a.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Kuo-2018-DeepTriangle{\_} A Deep Learning Approac.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Zarkadoulas-2017-Neural network algorithms for.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Gao-2018-Claims Frequency Modeling Using Telem.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Gao-2017-Feature Extraction from Telematics Ca.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Noll-2018-Case Study{\_} French Motor Third-Party.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/W{\"{u}}thrich-2018-Neural networks applied to chain.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/R-Unravelling the predictive power of telemati.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/drive.ai-Drive.ai Announces On-Demand Self-Dri.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hainaut-2018-A neural-network analyzer for mor.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Rawat-2017-Deep convolutional neural networks.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Gabrielli-2018-An Individual Claims History Si.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Haberman-1988-Generalised Lienar Models in Act.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hainaut-2018-A self-organizing predictive map.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lecun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2009 - Learning deep architectures for AI.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breeden, Leonova - 2019 - When Big Data Isn't Enough Solving the long-range forecasting problem in supervised learning.pdf:pdf},
isbn = {0515-0361},
issn = {17831350},
journal = {ASTIN Bulletin},
keywords = {Lee-Carter,Longevity,Mortality,Neural network,Perceptron,Weidner2016},
mendeley-tags = {Weidner2016},
month = {sep},
number = {2},
pages = {481--508},
pmid = {26017442},
publisher = {Cambridge University Press},
shorttitle = {A neural-network analyzer for mortality forecast},
title = {{A neural-network analyzer for mortality forecast}},
volume = {48},
year = {2018}
}
@article{Kirkwood2015,
author = {Kirkwood, Thomas B L},
isbn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
number = {1666},
pages = {20140379},
title = {{Deciphering death: a commentary on Gompertz (1825)‘On the nature of the function expressive of the law of human mortality, and on a new mode of determining the value of life contingencies'}},
volume = {370},
year = {2015}
}
@article{villegas2017comparative,
abstract = {Longevity swaps have been one of the major success stories of pension scheme de-risking in recent years. However, with some few exceptions, all of the transactions to date have been bespoke longevity swaps based upon the mortality experience of a portfolio of named lives. In order for this market to start to meet its true potential, solutions will ultimately be needed that provide protection for all types of members, are cost effective for large and smaller schemes, are tradable, and enable access to the wider capital markets. Index-based solutions have the potential to meet this need; however, concerns remain with these solutions. In particular, the basis risk emerging from the potential mismatch between the underlying forces of mortality for the index reference portfolio and the pension fund/annuity book being hedged is the principal issue that has, to date, prevented many schemes progressing their consideration of index-based solutions. Two-population stochastic mortality models offer an alternative to overcome this obstacle as they allow market participants to compare and project the mortality experience for the reference and target populations and thus assess the amount of demographic basis risk involved in an index-based longevity hedge. In this paper, we systematically assess the suitability of several multi-population stochastic mortality models for assessing basis risks and provide guidelines on how to use these models in practical situations paying particular attention to the data requirements for the appropriate calibration and forecasting of such models.},
author = {Villegas, Andr{\'{e}}s M. and Haberman, Steven and Kaishev, Vladimir K. and Millossovich, Pietro},
doi = {10.1017/asb.2017.18},
issn = {17831350},
journal = {ASTIN Bulletin},
keywords = {Stochastic morality,hedge effectiveness,index-based hedges,longevity basis risk,two-population mortality models},
number = {3},
pages = {631--679},
publisher = {Cambridge University Press},
title = {{A COMPARATIVE STUDY of TWO-POPULATION MODELS for the ASSESSMENT of BASIS RISK in LONGEVITY HEDGES}},
volume = {47},
year = {2017}
}
@article{Richman2018,
abstract = {Rapid advances in Artificial Intelligence and Machine Learning are creating products and services with the potential not only to change the environment in which actuaries operate, but also to provide new opportunities within actuarial science. These advances are based on a modern approach to designing, fitting and applying neural networks, generally referred to as "Deep Learning". This paper investigates how actuarial science may adapt and evolve in the coming years to incorporate these new techniques and methodologies. After providing some background on machine learning and deep learning, and providing a heuristic for where actuaries might benefit from applying these techniques, the paper surveys emerging applications of artificial intelligence in actuarial science, with examples from mortality modelling, claims reserving, non-life pricing and telematics. For some of the examples, code has been provided on GitHub so that the interested reader can experiment with these techniques for themselves. The paper concludes with an outlook on the potential for actuaries to integrate deep learning into their activities.},
author = {Richman, R},
doi = {10.2139/ssrn.3218082},
journal = {SSRN Electronic Journal},
month = {oct},
publisher = {Elsevier BV},
title = {{AI in Actuarial Science}},
year = {2018}
}
@article{Wilmoth2008,
author = {Wilmoth, J R and Shkolnikov, V},
journal = {University of California, Berkeley (USA), and Max Planck Institute for Demographic Research (Germany)},
title = {{Human mortality database. 2008}},
year = {2008}
}
@phdthesis{Marandu2011,
author = {Marandu, Simon Hlomayi},
title = {{Full life tables for South Africa from vital registration data, 2006-2008}},
year = {2011}
}
@inproceedings{Feeney1990a,
author = {Feeney, Griffith},
booktitle = {Asia and Pacific Population Forum},
pages = {13--20},
title = {{Untilting age distributions: A transformation for graphical analysis}},
volume = {4},
year = {1990}
}
@inproceedings{Chung2015,
abstract = {In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.},
archivePrefix = {arXiv},
arxivId = {1502.02367},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.02367},
isbn = {9781510810587},
pages = {2067--2075},
title = {{Gated feedback recurrent neural networks}},
volume = {3},
year = {2015}
}
@book{Kuonen2004,
abstract = {There are many books that are excellent sources of knowledge about individual stastical tools (survival models, general linear models, etc.), but the art of data analysis is about choosing and using multiple tools. In the words of Chatfield "...students typically know the technical details of regressin for example, but not necessarily when and how to apply it. This argues the need for a better balance in the literature and in statistical teaching between techniques and problem solving strategies." Whether analyzing risk factors, adjusting for biases in observational studies, or developing predictive models, there are common problems that few regression texts address. For example, there are missing data in the majority of datasets one is likely to encounter (other than those used in textbooks!) but most regression texts do not include methods for dealing with such data effectively, and texts on missing data do not cover regression modeling.},
author = {Kuonen, Diego},
booktitle = {Statistical Methods in Medical Research},
doi = {10.1177/096228020401300512},
file = {:C$\backslash$:/Users/user-pc/Desktop/2015{\_}Book{\_}RegressionModelingStrategies.pdf:pdf},
isbn = {9783319194240},
issn = {0962-2802},
number = {5},
pages = {415--416},
title = {{Book Review: Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis}},
volume = {13},
year = {2004}
}
@article{Renshaw1991,
author = {Renshaw, A E},
journal = {Journal of the Institute of Actuaries},
number = {02},
pages = {295--312},
title = {{Actuarial graduation practice and generalised linear and non-linear models}},
volume = {118},
year = {1991}
}
@article{Laourou1995,
author = {Laourou, Martin H},
isbn = {0380-1721},
journal = {Cahiers qu{\'{e}}b{\'{e}}cois de d{\'{e}}mographie},
number = {1},
pages = {129--162},
title = {{Une nouvelle approche pour l'{\'{e}}valuation de la couverture des d{\'{e}}c{\`{e}}s}},
volume = {24},
year = {1995}
}
@misc{Villegas2015,
abstract = {In this paper we mirror the framework of generalised (non-)linear models to de ne the family of generalised Age-Period-Cohort stochastic mortality models which encompasses the vast majority of stochastic mortality projection models proposed to date, including the well-known Lee-Carter and Cairns-Blake-Dowd models. We also introduce the R package StMoMo which exploits the unifying framework of the generalised Age-Period-Cohort family to provide tools for  tting stochastic mortality models, assessing their goodness of  t and performing mortality projections. We illustrate some of the capabilities of the package by performing a comparison of several stochastic mortality models applied to the England and Wales population. },
author = {Villegas, Andrrs and Kaishev, Vladimir K. and Millossovich, Pietro},
booktitle = {SSRN Electronic Journal},
doi = {10.2139/ssrn.2698729},
publisher = {CRAN},
title = {{StMoMo: An R Package for Stochastic Mortality Modelling}},
url = {https://cran.r-project.org/web/packages/StMoMo/index.html},
year = {2015}
}
@article{Preston1982,
author = {Preston, Samuel H and Coale, Ansley J},
isbn = {0032-4701},
journal = {Population Index},
pages = {217--259},
title = {{Age structure, growth, attrition, and accession: A new synthesis}},
year = {1982}
}
@article{Chege2012,
abstract = {European Union anti-discrimination law does not contain any explicit binding provision concerning multi-dimensional discrimination. Instead, it prohibits several grounds of discrimination. A single-ground approach may not do justice to the full breadth of discrimination that persons may experience in reality. It is clear that the European Union legislator has this in mind-as the references to multiple discrimination in directive recitals indicate. An examination of the legal framework for the equal treatment of persons reveals several shortcomings that may however rob European Union equality law of its ability to bite in the face of multi-dimensional discrimination. This article intends to investigate the practical implications thereof and possible ways forward. {\textcopyright} 2012 ERA.},
author = {Chege, Victoria},
doi = {10.1007/s12027-012-0260-1},
file = {:C$\backslash$:/R/refas/chege2012european.pdf:pdf},
isbn = {1202701202601},
issn = {16123093},
journal = {ERA Forum},
keywords = {Equality law,Intersectionality,Multi-dimensional discrimination,Multiple discrimination},
number = {2},
pages = {275--293},
title = {{The European Union anti-discrimination directives and European Union equality law: The case of multi-dimensional discrimination}},
volume = {13},
year = {2012}
}
@misc{Team2018,
abstract = {R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.},
address = {Vienna, Austria},
author = {{R Core Development Team}},
institution = {R Foundation for Statistical Computing},
publisher = {R Foundation for Statistical Computing},
shorttitle = {R: A Language and Environment for Statistical Comp},
title = {{A language and environment for statistical computing.}},
url = {http://www.r-project.org/},
volume = {1},
year = {2013}
}
@article{Lee1982,
author = {Lee, Ronald Demos},
title = {{Correcting census age distributions: extensions and applications of the Demeny-Shorter technique}},
year = {1982}
}
@book{Braiman1996,
abstract = {We study low-voltage dynamics in highly discrete one-dimensional arrays of Josephson junctions. In particular, we focus on the resonant solutions emerging from the locking between the time period of the oscillations of the single junction with the spatial period of the wave propagating across the array. We find that the average voltage across the array scales as V∝($\kappa$-$\kappa$c)1/2, where $\kappa$c is the critical value of the coupling. The connections to high voltage solutions are discussed. {\textcopyright} 1996 American Institute of Physics.},
author = {Braiman, Y. and Family, F. and Hentschel, H. G.E.},
booktitle = {Applied Physics Letters},
doi = {10.1063/1.115817},
file = {:C$\backslash$:/Users/user-pc/Desktop/2016{\_}Book{\_}IntroductionToTimeSeriesAndFor.pdf:pdf},
isbn = {9783319298528},
issn = {00036951},
number = {22},
pages = {3180--3182},
title = {{Discrete voltage states in one-dimensional parallel array of Josephson junctions}},
volume = {68},
year = {1996}
}
@book{Mitchell1997a,
abstract = {Mitchell covers the field of machine learning, the study of algorithms that allow computer programs to automatically improve through experience and that automatically infer general laws from specific data. 1. Introduction -- 2. Concept Learning and the General-to-Specific Ordering -- 3. Decision Tree Learning -- 4. Artificial Neural Networks -- 5. Evaluating Hypotheses -- 6. Bayesian Learning -- 7. Computational Learning Theory -- 8. Instance-Based Learning -- 9. Genetic Algorithms -- 10. Learning Sets of Rules -- 11. Analytical Learning -- 12. Combining Inductive and Analytical Learning -- 13. Reinforcement Learning.},
author = {Boyarshinov, Victor},
booktitle = {Computer},
isbn = {0071154671},
number = {April},
pages = {414},
publisher = {McGraw-Hill Boston, MA},
title = {{Machine Learning Machine Learning}},
url = {https://books.google.ca/books?id=EoYBngEACAAJ{\&}dq=mitchell+machine+learning+1997{\&}hl=en{\&}sa=X{\&}ved=0ahUKEwiomdqfj8TkAhWGslkKHRCbAtoQ6AEIKjAA},
volume = {2005},
year = {2005}
}
@article{Arel2010,
abstract = {Mimicking the efficiency and robustness by which the human brain represents information has been a core challenge in artificial intelligence research for decades. Humans are exposed to myriad of sensory data received every second of the day and are somehow able to capture critical aspects of this data in a way that allows for its future use in a concise manner. Over 50 years ago, Richard Bellman, who introduced dynamic programming theory and pioneered the field of optimal control, asserted that high dimensionality of data is a fundamental hurdle in many science and engineering applications. The main difficulty that arises, particularly in the context of pattern classification applications, is that the learning complexity grows exponentially with linear increase in the dimensionality of the data. He coined this phenomenon the curse of dimensionality [1]. The mainstream approach of overcoming the curse has been to pre-process the data in a manner that would reduce its dimensionality to that which can be effectively processed, for example by a classification engine. This dimensionality reduction scheme is often referred to as feature extraction. As a result, it can be argued that the intelligence behind many pattern recognition systems has shifted to the human-engineered feature extraction process, which at times can be challenging and highly application-dependent [2]. Moreover, if incomplete or erroneous features are extracted, the classification process is inherently limited in performance. {\textcopyright} 2006 IEEE.},
author = {Arel, Itamar and Rose, Derek and Karnowski, Thomas},
doi = {10.1109/MCI.2010.938364},
issn = {1556603X},
journal = {IEEE Computational Intelligence Magazine},
number = {4},
pages = {13--18},
shorttitle = {Deep machine learning-a new frontier in artificial},
title = {{Deep machine learning-A new frontier in artificial intelligence research}},
volume = {5},
year = {2010}
}
@inproceedings{Krizhevsky2012,
author = {Krizhevsky, A and Sutskever, I and Hinton, G},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1097--1105},
shorttitle = {Imagenet classification with deep convolutional ne},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@article{Elo1994,
abstract = {This paper evaluates the quality of vital statistics and census data for estimating African-American mortality over a period of six decades. The authors employ intercensal cohort comparisons and extinct generation estimates to demonstrate that conventionally constructed African-American death rates may be seriously flawed as early as age 50. Using the crude death rate at ages 50+ for 1978-1982 in conjunction with estimated growth rates and two model life table systems, the authors estimate black age-specific death rates in 1978-1982. These results suggest that if a racial crossover in death rates occurs, the age pattern of mortality among African-Americans must be far outside the range observed in populations with more accurate data. {\textcopyright} 1994 Population Association of America.},
author = {Elo, Irma T. and Preston, Samuel H.},
doi = {10.2307/2061751},
isbn = {0070-3370},
issn = {00703370},
journal = {Demography},
number = {3},
pages = {427--458},
title = {{Estimating African-American mortality from inaccurate data}},
volume = {31},
year = {1994}
}
@article{Bailey1986,
author = {Bailey, R. A.},
file = {:C$\backslash$:/R/refas/ruppert1994multivariate.pdf:pdf},
journal = {Annals of Statistics},
number = {2},
pages = {590--606},
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Statistics. {\textregistered} www.jstor.org}},
volume = {14},
year = {1986}
}
@book{Goodfellow2016,
author = {Goodfellow, I and Bengio, Y and Courville, A},
isbn = {0262337371},
publisher = {MIT Press},
shorttitle = {Deep Learning},
title = {{Deep Learning}},
year = {2016}
}
@book{HastieTrevorTibshiraniRobertFriedman2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Friedman, J and Hastie, T and Tibshirani, R},
booktitle = {Springer series in statistics},
doi = {10.1007/978-0-387-84858-7},
eprint = {arXiv:1011.1669v3},
isbn = {978-0-387-84858-7},
issn = {00111287},
keywords = {Data Mining,Inference,Neural Nets,Prediction,Statistical Learning},
number = {2},
pages = {282},
pmid = {12377617},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning The Elements of Statistical Learning Data Mining, Inference, and Prediction, Second Edition}},
volume = {27},
year = {2009}
}
@article{castellani2018investigation,
author = {Castellani, Gilberto and Fiore, Ugo and Marino, Zelda and Passalacqua, Luca and Perla, Francesca and Scognamiglio, Salvatore and Zanetti, Paolo},
doi = {10.2139/ssrn.3303296},
journal = {SSRN Electronic Journal},
title = {{An Investigation of Machine Learning Approaches in the Solvency II Valuation Framework}},
year = {2019}
}
@article{Goldberg2016,
abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
author = {{Yoav Goldberg}},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Goldberg-2016-A primer on neural network model.pdf:pdf},
isbn = {1076-9757},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
pages = {345--420},
shorttitle = {A primer on neural network models for natural lang},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
url = {http://www.jair.org/papers/paper4992.html},
volume = {57},
year = {2016}
}
@misc{Oeppen2002,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
author = {Oeppen, Jim and Vaupel, James W.},
booktitle = {Science},
doi = {10.1126/science.1069675},
issn = {00368075},
month = {may},
number = {5570},
pages = {1029--1031},
title = {{Demography: Broken limits to life expectancy}},
volume = {296},
year = {2002}
}
@article{Gompertz1871,
author = {Gompertz, Benjamin},
isbn = {2046-1674},
journal = {Journal of the Institute of Actuaries and Assurance Magazine},
pages = {329--344},
title = {{On one uniform law of mortality from birth to extreme old age, and on the law of sickness}},
year = {1871}
}
@article{Jdanov2005,
author = {Jdanov, Dmitri A and Scholz, Rembrandt D and Shkolnikov, Vladimir M},
title = {{Official population statistics and the Human Mortality Database estimates of populations aged 80+ in Germany and nine other European countries}},
year = {2005}
}
@article{Belkin2019,
abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
archivePrefix = {arXiv},
arxivId = {1812.11118},
author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
doi = {10.1073/pnas.1903070116},
eprint = {1812.11118},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Belkin et al. - 2019 - Reconciling modern machine-learning practice and the classical bias–variance trade-off.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Bias–variance trade-off,Machine learning,Neural networks},
number = {32},
pages = {15849--15854},
title = {{Reconciling modern machine-learning practice and the classical bias–variance trade-off}},
volume = {116},
year = {2019}
}
@misc{Dorrington2013c,
address = {Paris},
author = {Dorrington, R E},
booktitle = {Tools for Demographic Estimation},
editor = {Moultrie, T and Dorrington, R and Hill, A and Hill, K and Timaeus, I and Zaba, B},
publisher = {International Union for the Scientific Study of Population},
title = {{The Preston-Coale method}},
url = {http://demographicestimation.iussp.org/content/generalized-growth-balance-method},
year = {2013}
}
@inproceedings{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1409.3215},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to sequence learning with neural networks.pdf:pdf},
issn = {10495258},
number = {January},
pages = {3104--3112},
title = {{Sequence to sequence learning with neural networks}},
volume = {4},
year = {2014}
}
@article{Richards2014,
abstract = {Longevity risk faced by annuity portfolios and defined-benefit pension schemes is typically long-term, i.e. the risk is of an adverse trend which unfolds over a long period of time. However, there are circumstances when it is useful to know by how much expectations of future mortality rates might change over a single year. Such an approach lies at the heart of the one-year, value-at-risk view of reserves, and also for the pending Solvency II regime for insurers in the European Union. This paper describes a framework for determining how much a longevity liability might change based on new information over the course of one year. It is a general framework and can accommodate a wide choice of stochastic projection models, thus allowing the user to explore the importance of model risk. A further benefit of the framework is that it also provides a robustness test for projection models, which is useful in selecting an internal model for management purposes.},
author = {Richards, S. J. and Currie, I. D. and Ritchie, G. P.},
doi = {10.1017/s1357321712000451},
file = {:C$\backslash$:/Users/user-pc/Desktop/valueatrisk{\_}framework{\_}for{\_}longevity{\_}trend{\_}risk.pdf:pdf},
issn = {1357-3217},
journal = {British Actuarial Journal},
keywords = {age-period-,cairns-blake-dowd,cohort,lee-carter,longevity risk,model risk,mortality projections,p -spline,solvency ii,value-at-risk},
number = {1},
pages = {116--139},
title = {{A Value-at-Risk framework for longevity trend risk}},
volume = {19},
year = {2014}
}
@article{Dorrington1996,
author = {Dorrington, R E and Rosenberg, S B},
journal = {Transactions of the Actuarial Society of South Africa},
number = {1},
pages = {91--144},
title = {{Graduation of the 1985–90 assured life mortality experience}},
volume = {11},
year = {1996}
}
@article{Lewisa,
author = {Lewis, P L and Rossouw, L J},
journal = {2007 Convention},
publisher = { Actuarial Society of South Africa},
title = {{IBNR versus EBNER–An Alternative Method of Allowing for Late Reported Claims}}
}
@article{Zeileis2008,
abstract = {Recursive partitioning is embedded into the general and well-established class of parametric models that can be fitted using M-type estimators (including maximum likelihood). An algorithm for model-based recursive partitioning is suggested for which the basic steps are: (1) fit a parametric model to a dataset; (2) test for parameter instability over a set of partitioning variables; (3) if there is some overall parameter instability, split the model with respect to the variable associated with the highest instability; (4) repeat the procedure in each of the daughter nodes. The algorithm yields a partitioned (or segmented) parametric model that can be effectively visualized and that subject-matter scientists are used to analyzing and interpreting. {\textcopyright} 2008 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
author = {Zeileis, Achim and Hothorn, Torsten and Hornik, Kurt},
doi = {10.1198/106186008X319331},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Change points,Maximum likelihood,Parameter instability},
number = {2},
pages = {492--514},
publisher = {American Statistical Association},
title = {{Model-based recursive partitioning}},
volume = {17},
year = {2008}
}
@article{ayuso2016telematics,
abstract = {Pay-as-you-drive (PAYD), or usage-based automobile insurance (UBI), is a policy agreement tied to vehicle usage. In this paper we analyze the effect of the distance traveled on the risk of accidents among young drivers with a PAYD policy. We use regression models for survival data to estimate how long it takes them to have their first accident at fault during the coverage period. Our empirical application with real data is presented and shows that gender differences are mainly attributable to the intensity of use. Indeed, although gender has a significant effect in explaining the time to the first crash, this effect is no longer significant when the average distance traveled per day is introduced in the model. This suggests that gender differences in the risk of accidents are, to a large extent, attributable to the fact that men drive more often than women. Estimates of the time to the first accident for different driver risk types are presented. We conclude that no gender discrimination is necessary if telematics provides enough information on driving habits.},
author = {Ayuso, Mercedes and Guillen, Montserrat and P{\'{e}}rez-Mar{\'{i}}n, Ana},
doi = {10.3390/risks4020010},
issn = {2227-9091},
journal = {Risks},
number = {2},
pages = {10},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Telematics and Gender Discrimination: Some Usage-Based Evidence on Whether Men's Risk of Accidents Differs from Women's}},
volume = {4},
year = {2016}
}
@article{Bengio2009a,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the stateof-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks. {\textcopyright} 2009 Y. Bengio.},
author = {Bengio, Yoshua},
doi = {10.1561/2200000006},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2009 - Learning deep architectures for AI.pdf:pdf},
issn = {19358237},
journal = {Foundations and Trends in Machine Learning},
number = {1},
pages = {1--27},
title = {{Learning deep architectures for AI}},
volume = {2},
year = {2009}
}
@article{Currie2011,
author = {Currie, Iain D},
isbn = {1783-1350},
journal = {Astin Bulletin},
number = {02},
pages = {419--427},
title = {{Modelling and forecasting the mortality of the very old}},
volume = {41},
year = {2011}
}
@techreport{ActuarialSocietyofSouthAfrica2009,
address = {Cape Town},
author = {{Actuarial Society of South Africa}},
title = {{AIDS and Demographic Model}},
url = {Rob Dorrington, personal communication},
year = {2008}
}
@article{Friedman2001,
abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization iti function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitives highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
author = {Friedman, J},
doi = {10.2307/2699986},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Boosting,Decision trees,Function estimation,Robust nonparametric regression},
number = {5},
pages = {1189--1232},
title = {{Greedy function approximation: A gradient boosting machine}},
volume = {29},
year = {2001}
}
@article{Strumbelj2010,
abstract = {We present a general method for explaining individual predictions of classification models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method's initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efficient and that the explanations are intuitive and useful. {\textcopyright} 2010 Erik {\v{S}}trumbelj and Igor Kononenko.},
author = {{\v{S}}trumbelj, Erik and Kononenko, Igor},
file = {:C$\backslash$:/R/refas/JMLR-Strumbelj-Kononenko.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Classification,Data postprocessing,Explanation,Visualization},
number = {xxxx},
pages = {1--18},
title = {{An efficient explanation of individual classifications using game theory}},
volume = {11},
year = {2010}
}
@misc{DanilovaI.MesleF.JdanovD.PechholdovaM.JasilionisD.ShkolnikovV.M.&Vallin,
author = {Danilova, I. and Mesl{\'{e}}, F. and Jdanov, D. and Pechholdov{\'{a}}, M. and Jasilionis, D. and Shkolnikov, V. M. and Vallin, J},
title = {{HCD The Human Cause-of-Death Database}},
url = {https://www.causesofdeath.org/cgi-bin/main.php},
year = {2020}
}
@article{Hinton2006a,
abstract = {This dialogue arises from some of the issues exposed in Professors Fuller's recent book, 'Science'. It breaks the mould of the 'science wars', in which the idea of science as a search for objective truth has been questioned, by leaving room for compromise and agreement between the practitioners and interpreters of science. Through sustained, non-acrimonious discussion, the protagonists release the debate from its adversarial, science v. antiscience shackles, finding common ground in their concern for the effects of market forces on the uses of science and on the freedom of scientists. Professor Barnett begins with a dispassionate review of 'Science'.},
author = {Barnett, S. A. and Fuller, Steve},
doi = {10.1179/isr.1998.23.4.331},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hinton-2006-Reducing the dimensionality of dat.pdf:pdf},
isbn = {9781405164535},
issn = {03080188},
journal = {Interdisciplinary Science Reviews},
number = {4},
pages = {331--339},
shorttitle = {Reducing the dimensionality of data with neural ne},
title = {{Science}},
volume = {23},
year = {1998}
}
@article{Rosenwaike1981,
author = {Rosenwaike, Ira},
isbn = {0070-3370},
journal = {Demography},
pages = {257--266},
title = {{A Note on New Estimates of the Mortality of the Extreme Aged}},
year = {1981}
}
@article{Makridakis2018b,
abstract = {The M4 competition is the continuation of three previous competitions started more than 45 years ago whose purpose was to learn how to improve forecasting accuracy, and how such learning can be applied to advance the theory and practice of forecasting. The purpose of M4 was to replicate the results of the previous ones and extend them into three directions: First significantly increase the number of series, second include Machine Learning (ML) forecasting methods, and third evaluate both point forecasts and prediction intervals. The five major findings of the M4 Competitions are: 1. Out Of the 17 most accurate methods, 12 were “combinations” of mostly statistical approaches. 2. The biggest surprise was a “hybrid” approach that utilized both statistical and ML features. This method's average sMAPE was close to 10{\%} more accurate than the combination benchmark used to compare the submitted methods. 3. The second most accurate method was a combination of seven statistical methods and one ML one, with the weights for the averaging being calculated by a ML algorithm that was trained to minimize the forecasting. 4. The two most accurate methods also achieved an amazing success in specifying the 95{\%} prediction intervals correctly. 5. The six pure ML methods performed poorly, with none of them being more accurate than the combination benchmark and only one being more accurate than Na{\"{i}}ve2. This paper presents some initial results of M4, its major findings and a logical conclusion. Finally, it outlines what the authors consider to be the way forward for the field of forecasting.},
author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
doi = {10.1016/j.ijforecast.2018.06.001},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Benchmarking methods,Forecasting accuracy,Forecasting competitions,M Competitions,Machine Learning (ML) methods,Practice of forecasting,Prediction intervals (PIs),Time series methods},
month = {oct},
number = {4},
pages = {802--808},
publisher = {Elsevier B.V.},
title = {{The M4 Competition: Results, findings, conclusion and way forward}},
volume = {34},
year = {2018}
}
@article{Guiahi2018,
abstract = {Analysis of insurance data provides input for making decisions regarding underwriting, pricing of insurance products, and claims, as well as profitability analysis. In this paper, we consider graphical modeling as a vehicle to reveal dependency structure of categorical variables used in the Australian Auto­ mobile data. The methodology developed here may supplement the traditional approach to ratemaking. Topics considered are the description of the automobile data set, preprocessing of the variables, visualization tools suitable for contingency tables, classical test of independence, log­linear models, the concept of conditional independence, and graphical modeling as a vehicle to explore the dependency structure among categorical variables, as well as a review of frequency rates by rating class.},
author = {Guiahi, Farrokh},
journal = {Variance},
keywords = {Categorical variables,chi-square test,cliques,conditional independence,frequency rates,graphical modeling,log-linear models,mosaic plots,visualization of categorical variables},
title = {{Applying Graphical Models to Automobile Insurance Data}},
url = {http://www.businessandeconomics.mq.edu.au/},
year = {2018}
}
@inproceedings{Zehnwirth,
author = {Zehnwirth, Ben},
title = {{Probabilistic development factor models with applications to loss reserve variability, prediction intervals and risk based capital}}
}
@incollection{Canny1987,
abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of theso-lution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for severalcom-mon image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle be-tween detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient mag-nitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature syn-thesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector perfor-mance improves considerably as the operator point spread function is extended along the edge. This detection scheme uses several elongated operators at each point, and the directional operator outputs are in-tegrated with the gradient maximum detector. {\textcopyright} 1986 IEEE},
author = {Canny, John},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.1986.4767851},
issn = {01628828},
keywords = {Edge detection,feature extraction,image processing,machine vision,multiscale image analysis},
number = {6},
pages = {679--698},
pmid = {21869365},
publisher = {Elsevier},
title = {{A Computational Approach to Edge Detection}},
volume = {PAMI-8},
year = {1986}
}
@misc{IASB2020,
author = {IASB},
month = {jul},
publisher = {International Accounting Standards Board},
title = {{IFRS - IFRS 17 Insurance Contracts}},
year = {2020}
}
@techreport{EIOPA2014,
address = {Frankfurt, Germany},
author = {EIOPA},
publisher = {EIOPA},
title = {{The underlying assumptions in the standard formla for the Solvency Capital Requirement calculation}},
year = {2014}
}
@article{Elman1990,
abstract = {Recurrent Neural Networks (RNN) are a class of artificial neural network which became more popular in the recent years. The RNN is a special network, which has unlike feedforward networks recurrent connections. The major benefit is that with these connections the network is able to refer to last states and can therefore process arbitrary sequences of input. RNN are a very huge topic and are here introduced very shortly. This article specializes on the combination of RNN with CNN.},
author = {Wiest, Lukas},
doi = {10.1207/s15516709cog1402_1},
isbn = {0364-0213},
journal = {Cognitive Science},
number = {2},
pages = {179--211},
publisher = {Wiley/Blackwell (10.1111)},
title = {{Recurrent Neural Networks - Combination of RNN and CNN}},
url = {http://doi.wiley.com/10.1207/s15516709cog1402{\_}1},
volume = {07},
year = {2017}
}
@article{Richman2015a,
address = {Actuarial Science/Demography Seminar, University of Cape Town},
author = {Richman, R},
title = {{South African Old Age Mortality – Experience from 1996-2011}},
year = {2015}
}
@article{Feeney2001,
author = {Feeney, Griffith},
isbn = {1728-4457},
journal = {Population and Development Review},
number = {4},
pages = {771--780},
title = {{The impact of HIV/AIDS on adult mortality in Zimbabwe}},
volume = {27},
year = {2001}
}
@book{manning1999foundations,
abstract = {"Statistical natural-language processing is, in my estimation, one of the most fast-moving and exciting areas of computer science these days. Anyone who wants to learn this field would be well advised to get this book. For that matter, the same goes for anyone who is already in the field. I know that it is going to be one of the most well-thumbed books on my bookshelf." - Eugene Charniak, Department of Computer Science, Brown University Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications. More on this book},
author = {Manning, Christopher D. and Sch{\"{u}}tze, Hinrich and Weikurn, Gerhard},
booktitle = {SIGMOD Record},
doi = {10.1145/601858.601867},
issn = {01635808},
number = {3},
pages = {37--38},
publisher = {MIT press},
title = {{Foundations of Statistical Natural Language Processing}},
volume = {31},
year = {2002}
}
@article{Terblanche2014,
author = {Terblanche, Wilma and Wilson, Tom},
isbn = {1874-7884},
journal = {Journal of Population Ageing},
number = {4},
pages = {301--322},
title = {{Understanding the Growth of Australia's Very Elderly Population, 1976 to 2012}},
volume = {7},
year = {2014}
}
@article{Gao2020,
author = {Gao, Guangyuan and Wang, He and Wuthrich, Mario V.},
file = {:C$\backslash$:/Users/user-pc/Desktop/SSRN-id3596034.pdf:pdf},
journal = {SSRN},
keywords = {Claims frequency modeling,Combined actuarial neural network,Convolutional neural network,Densely connected neural network,Generalized linear model,Poisson regression,Regression tree,Telematics car driving data,Telematics heatmap},
month = {may},
title = {{Boosting Poisson regression models with telematics car driving data}},
year = {2020}
}
@article{Kusner2017a,
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
archivePrefix = {arXiv},
arxivId = {1703.06856},
author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
eprint = {1703.06856},
file = {:C$\backslash$:/R/refas/CounterfactualFairness.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {4067--4077},
title = {{Counterfactual fairness}},
volume = {2017-Decem},
year = {2017}
}
@article{Dacorogna2018,
abstract = {The development of risk model for managing portfolio of financial institutions and insurance companies require both from the regulatory and management points of view a strong validation of the quality of the results provided by internal risk models. In Solvency II for instance, regulators ask for independent validation reports from companies who apply for the approval of their internal models.  Unfortunately, the usual statistical techniques do not work for the validation of risk models as we lack enough data to significantly test the results of the models. We will certainly never have enough data to statistically estimate the significance of the VaR at a probability of 1 over 200 years, which is the risk measure required by Solvency II. Instead, we need to develop various strategies to test the reasonableness of the model.  In this paper, we review various ways, management and regulators can gain confidence in the quality of models. It all starts by ensuring a good calibration of the risk models and the dependencies between the various risk drivers. Then applying stress tests to the model and various empirical analysis, in particular the probability integral transform, we build a full and credible framework to validate risk models.},
author = {Dacorogna, Michel M.},
doi = {10.2139/ssrn.2983837},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dacorogna - 2018 - Approaches and Techniques to Validate Internal Model Results.pdf:pdf},
journal = {SSRN Electronic Journal},
keywords = {risk models,solvency,statistical tests,stress tests,validation},
pages = {1--19},
title = {{Approaches and Techniques to Validate Internal Model Results}},
year = {2018}
}
@book{Kuhn2013,
abstract = {Applied Predictive Modeling covers the overall predictive modeling process, beginning with the crucial steps of data preprocessing, data splitting and foundations of model tuning. The text then provides intuitive explanations of numerous common and modern regression and classification techniques, always with an emphasis on illustrating and solving real data problems. The text illustrates all parts of the modeling process through many hands-on, real-life examples, and every chapter contains extensive R code for each step of the process. This multi-purpose text can be used as an introduction to predictive models and the overall modeling process, a practitioner's reference handbook, or as a text for advanced undergraduate or graduate level predictive modeling courses. To that end, each chapter contains problem sets to help solidify the covered concepts and uses data available in the book's R package. This text is intended for a broad audience as both an introduction to predictive models as well as a guide to applying them. Non-mathematical readers will appreciate the intuitive explanations of the techniques while an emphasis on problem-solving with real data across a wide variety of applications will aid practitioners who wish to extend their expertise. Readers should have knowledge of basic statistical ideas, such as correlation and linear regression analysis. While the text is biased against complex equations, a mathematical background is needed for advanced topics.},
author = {Kuhn, Max and Johnson, Kjell},
booktitle = {Applied Predictive Modeling},
doi = {10.1007/978-1-4614-6849-3},
isbn = {9781461468493},
issn = {0006-341X},
pages = {1--600},
publisher = {Springer},
title = {{Applied predictive modeling}},
volume = {26},
year = {2013}
}
@article{Murray2010,
abstract = {BACKGROUND: One of the fundamental building blocks for determining the burden of disease in populations is to reliably measure the level and pattern of mortality by age and sex. Where well-functioning registration systems exist, this task is relatively straightforward. Results from many civil registration systems, however, remain uncertain because of a lack of confidence in the completeness of death registration. Incomplete registration systems mean not all deaths are counted, and resulting estimates of death rates for the population are then underestimated. Death distribution methods (DDMs) are a suite of demographic methods that attempt to estimate the fraction of deaths that are registered and counted by the civil registration system. Although widely applied and used, the methods have at least three types of limitations. First, a wide range of variants of these methods has been applied in practice with little scientific literature to guide their selection. Second, the methods have not been extensively validated in real population conditions where violations of the assumptions of the methods most certainly occur. Third, DDMs do not generate uncertainty intervals. METHODS AND FINDINGS: In this paper, we systematically evaluate the performance of 234 variants of DDM methods in three different validation environments where we know or have strong beliefs about the true level of completeness of death registration. Using these datasets, we identify three variants of the DDMs that generally perform the best. We also find that even these improved methods yield uncertainty intervals of roughly +/- one-quarter of the estimate. Finally, we demonstrate the application of the optimal variants in eight countries. CONCLUSIONS: There continues to be a role for partial vital registration data in measuring adult mortality levels and trends, but such results should only be interpreted alongside all other data sources on adult mortality and the uncertainty of the resulting levels, trends, and age-patterns of adult death considered. Please see later in the article for the Editors' Summary.},
annote = {Murray, Christopher J L
Rajaratnam, Julie Knoll
Marcus, Jacob
Laakso, Thomas
Lopez, Alan D
eng
Research Support, Non-U.S. Gov't
2010/04/21 06:00
PLoS Med. 2010 Apr 13;7(4):e1000262. doi: 10.1371/journal.pmed.1000262.},
author = {Murray, C J and Rajaratnam, J K and Marcus, J and Laakso, T and Lopez, A D},
doi = {10.1371/journal.pmed.1000262},
isbn = {1549-1676 (Electronic)
1549-1277 (Linking)},
journal = {PLoS Med},
keywords = {Death Certificates,Humans,Mortality,Population Dynamics,Registries/*statistics {\&} numerical data},
number = {4},
pages = {e1000262},
pmid = {20405002},
title = {{What can we conclude from death registration? Improved methods for evaluating completeness}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20405002},
volume = {7},
year = {2010}
}
@misc{USCensusBureau2005,
address = {Washington DC: International Programs Center},
author = {{US Census Bureau}},
title = {{International Database}},
year = {2005}
}
@article{DBLP:journals/corr/HeZRS15,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
url = {http://arxiv.org/abs/1512.03385},
volume = {2016-Decem},
year = {2016}
}
@inproceedings{Graves2013,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score. {\textcopyright} 2013 IEEE.},
archivePrefix = {arXiv},
arxivId = {1303.5778},
author = {Graves, Alex and Mohamed, Abdel Rahman and Hinton, Geoffrey},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {1303.5778},
isbn = {9781479903566},
issn = {15206149},
keywords = {deep neural networks,recurrent neural networks,speech recognition},
pages = {6645--6649},
publisher = {IEEE},
title = {{Speech recognition with deep recurrent neural networks}},
year = {2013}
}
@misc{OfficeforNationalStatistics2020,
author = {{Office for National Statistics}},
title = {{The different uses of figures on deaths related to COVID-19 published by DHSC and the ONS - Office for National Statistics}},
url = {https://www.ons.gov.uk/news/statementsandletters/thedifferentusesoffiguresondeathsfromcovid19publishedbydhscandtheons},
urldate = {2020-06-03},
year = {2020}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
journal = {arXiv preprint arXiv:1412.6980},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Black,
abstract = {How should statistical models used for assigning prices or eligibility be implemented when there is concern about discrimination? In many settings, factors such as race, gender, and age are prohibited. However, the use of variables that correlate with these omitted characteristics (e.g., zip codes, credit scores) is often contentious. We provide a framework to address these issues and propose a method that can eliminate proxy effects while maintaining predictive accuracy relative to an approach that restricts the use of contentious variables outright. We illustrate the value of our proposed method using data from the Worker Profiling and Reemployment Services system.},
author = {Pope, Devin G. and Sydnor, Justin R},
doi = {10.1257/pol.3.3.206},
issn = {19457731},
journal = {American Economic Journal: Economic Policy},
number = {3},
pages = {206--231},
title = {{Implementing anti-discrimination policies in statistical profiling models}},
url = {http://www.aeaweb.org/articles.php?doi=10.1257/pol.3.3.206},
volume = {3},
year = {2011}
}
@article{Thatcher1987,
author = {Thatcher, A Roger},
journal = {Journal of the Institute of Actuaries},
number = {02},
pages = {327--338},
title = {{Mortality at the highest ages}},
volume = {114},
year = {1987}
}
@misc{Timæus2004,
address = {Ougadougou},
author = {Tim{\ae}us, I M},
booktitle = {Seminar of the IUSSP Committee "Emerging Health Threats" HIV, Resurgent Infections and Population Change in Africa},
title = {{Impact of HIV on mortality in Southern Africa: Evidence from demographic surveillance}},
year = {2004}
}
@article{Horiuchi1990,
author = {Horiuchi, Shiro and Coale, Ansley J},
isbn = {0889-8480},
journal = {Mathematical Population Studies},
number = {4},
pages = {245--267},
title = {{Age patterns of mortality for older women: An analysis using the age‐specific rate of mortality change with age}},
volume = {2},
year = {1990}
}
@book{Sutton2018,
author = {Sutton, R S and Barto, A G},
number = {1},
publisher = {MIT Press},
title = {{Reinforcement learning: An introduction, Second Edition}},
volume = {1},
year = {2018}
}
@article{England2002a,
abstract = {In England and Verrall [Insur. Math. Econ. 25 (1999) 281], an appropriate residual definition was considered for use in a bootstrap exercise to provide a computationally simple method of obtaining reserve prediction errors for the chain ladder model. However, calculation of the first two moments of the predictive distribution only was considered. In this paper, the method is extended by using a two-stage process: bootstrapping to obtain the estimation error and simulation to obtain the process error. This has the advantage of providing realisations from the whole predictive distribution, rather than just the first two moments. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {England, Peter},
doi = {10.1016/S0167-6687(02)00161-0},
isbn = {0167-6687},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Bootstrap,Claims reserving,Prediction errors},
number = {3},
pages = {461--466},
title = {{Addendum to "Analytic and bootstrap estimates of prediction errors in claims reserving"}},
url = {http://www.sciencedirect.com/science/article/pii/S0167668702001610},
volume = {31},
year = {2002}
}
@article{Llaguno,
author = {Llaguno, Len and Bardis, Manolis and Chin, Robert and Gwilliam, Tina and Hagerstrand, Julie and Petzoldt, Evan},
journal = {Casualty Actuarial Society E-Forum},
keywords = {breakage estimation,claims triage,data mining,data organization,individual claims reserving,loyalty program liability,predictive modeling,reserving methods,snapshot,ultimate redemption rate},
number = {1},
pages = {1--18},
title = {{Reserving with Machine Learning: Applications for Loyalty Programs and Individual Insurance Claims}},
volume = {Summer 1},
year = {2017}
}
@misc{D.1997,
abstract = {Mutuality is the principle of private, commerical insurance; individuals enter the pool for sharing losses, and pay according to the best estimate of the risk they bring with them. Solidarity is the sharing of losses with payment according to some other scheme; this is the principle of state social insurance; essential features of solidarity are comprehensiveness and compulsion. Private insurance is subject to the uberrima fides principle, or utmost good faith; each side declares all it knows about the risk. The Disability Discrimination Act requires insurers to justify disability discrimination on the basis of relevant information, acturial, statistical or medical, on which it is reasonable to rely. It could be very damaging to private insurance to abandon uberrima fides. However, although some genetic, information is clearly useful to underwriters, other information may be so general as to be of little use. The way in which mortality rates are assessed is also explained.},
author = {Wilkie, D},
booktitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
isbn = {0962-8436},
keywords = {*cost,*genetic screening,*health insurance,economics,female,human,male,review,risk management},
number = {1357},
pages = {1039--1044},
title = {{Mutuality and solidarity: Assessing risks and sharing losses}},
url = {https://about.jstor.org/terms http://ovidsp.ovid.com/ovidweb.cgi?T=JS{\&}PAGE=reference{\&}D=emed4{\&}NEWS=N{\&}AN=9304668},
volume = {352},
year = {1997}
}
@article{Arthur1984,
abstract = {This paper extends the Lotka system of stable population equations to any population. The authors present this new general system and describe its duality with the recent Preston-Coale system. They derive these results by considering the calculus of change on the surface of population density defined over age and time. They show that analysis of this Lexis surface leads to all the known fundamental relationships of the dynamics of single-region human populations, several interesting new relationships, and a duality between period and cohort life tables.},
author = {Arthur, W. B. and Vaupel, J. W.},
doi = {10.2307/2736755},
isbn = {0032-4701},
issn = {00324701},
journal = {Population index},
number = {2},
pages = {214--226},
title = {{Some general relationships in population dynamics.}},
volume = {50},
year = {1984}
}
@article{Guo2016,
abstract = {We map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables. We applied it successfully in a recent Kaggle competition and were able to reach the third position with relative simple features. We further demonstrate in this paper that entity embedding helps the neural network to generalize better when the data is sparse and statistics is unknown. Thus it is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit. We also demonstrate that the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead. As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering.},
archivePrefix = {arXiv},
arxivId = {1604.06737},
author = {Guo, Cheng and Berkhahn, Felix},
eprint = {1604.06737},
journal = {arXiv},
shorttitle = {Entity embeddings of categorical variables},
title = {{Entity Embeddings of Categorical Variables}},
url = {http://arxiv.org/abs/1604.06737},
volume = {arXiv:1604},
year = {2016}
}
@article{Preston1991,
author = {Preston, Samuel H and Lahiri, Subrata},
isbn = {0889-8480},
journal = {Mathematical population studies},
number = {1},
pages = {39--51},
title = {{A short‐cut method for estimating death registration completeness in destabilized populations∗}},
volume = {3},
year = {1991}
}
@article{doi:10.1080/10920277.2013.847781,
abstract = {This article initiates a discussion regarding Plural Rationality Theory, which began to be used as a tool for understanding risk 40 years ago in the field of social anthropology. This theory is now widely applied and can provide a powerful paradigm to understand group behaviors. The theory has only recently been utilized in business and finance, where it provides insights into perceptions of risk and the dynamics of firms and markets. Plural Rationality Theory highlights four competing views of risk with corresponding strategies applied in four distinct risk environments. We explain how these rival perspectives are evident on all levels, from roles within organizations to macro level economics. The theory is introduced and the concepts are applied with business terms and examples such as company strategy, where the theory has a particularly strong impact on risk management patterns. The principles are also shown to have been evident in the run up to-and the reactions after-the 2008 financial crisis. Traditional "risk management" is shown to align with only one of these four views of risk, and the consequences of that singular view are discussed. Additional changes needed to make risk management more comprehensive, widely acceptable, and successful are introduced. {\textcopyright} 2013 Copyright Taylor and Francis Group, LLC.},
author = {Ingram, David and Bush, Elijah},
doi = {10.1080/10920277.2013.847781},
issn = {10920277},
journal = {North American Actuarial Journal},
number = {4},
pages = {297--305},
publisher = {Routledge},
title = {{Collective Approaches to Risk in Business: An Introduction to Plural Rationality Theory}},
url = {https://doi.org/10.1080/10920277.2013.847781},
volume = {17},
year = {2013}
}
@article{Levantesi2019,
abstract = {Estimation of future mortality rates still plays a central role among life insurers in pricing their products and managing longevity risk. In the literature on mortality modeling, a wide number of stochastic models have been proposed, most of them forecasting future mortality rates by extrapolating one or more latent factors. The abundance of proposed models shows that forecasting future mortality from historical trends is non-trivial. Following the idea proposed in Deprez et al. (2017), we use machine learning algorithms, able to catch patterns that are not commonly identifiable, to calibrate a parameter (the machine learning estimator), improving the goodness of fit of standard stochastic mortality models. The machine learning estimator is then forecasted according to the Lee-Carter framework, allowing one to obtain a higher forecasting quality of the standard stochastic models. Out-of sample forecasts are provided to verify the model accuracy.},
author = {Levantesi, Susanna and Pizzorusso, Virginia},
doi = {10.3390/risks7010026},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Levantesi, Pizzorusso - 2019 - Application of machine learning to mortality modeling and forecasting.pdf:pdf},
issn = {22279091},
journal = {Risks},
keywords = {Forecasting,Lee-Carter model,Machine learning,Mortality},
month = {mar},
number = {1},
publisher = {MDPI AG},
title = {{Application of machine learning to mortality modeling and forecasting}},
volume = {7},
year = {2019}
}
@article{Benktander1976,
author = {Benktander, G},
journal = {Actuarial Review},
number = {7},
pages = {7--31},
title = {{An approach to credibility in calculating IBNR for casualty excess reinsurance}},
volume = {3},
year = {1976}
}
@article{Shisana2002a,
author = {Shisana, Olive and Simbayi, Leickness},
journal = {South African National HIV prevalence, Behavioural Risks and Mass Media},
pages = {58--59},
title = {{Nelson Mandela/hsrc Study of hiv/aids}},
year = {2002}
}
@article{Thatcher1998,
author = {Thatcher, A Roger and Kannisto, V{\"{a}}in{\"{o}} and Vaupel, James W},
title = {{The force of mortality at ages 80 to 120}},
year = {1998}
}
@book{Pade2014,
abstract = {This book provides an introduction into the fundamentals of non-relativistic quantum mechanics. In Part 1, the essential principles are developed. Applications and extensions of the formalism can be found in Part 2. The book includes not only material that is presented in traditional textbooks on quantum mechanics, but also discusses in detail current issues such as interaction-free quantum measurements, neutrino oscillations, various topics in the field of quantum information as well as fundamental problems and epistemological questions, such as the measurement problem, entanglement, Bell's inequality, decoherence, and the realism debate. A chapter on current interpretations of quantum mechanics concludes the book. To develop quickly and clearly the main principles of quantum mechanics and its mathematical formulation, there is a systematic change between wave mechanics and algebraic representation in the first chapters. The required mathematical tools are introduced step by step. Moreover, the appendix collects compactly the most important mathematical tools that supplementary literature can be largely dispensed. In addition, the appendix contains advanced topics, such as Quantum- Zeno effect, time-delay experiments, Lenz vector and the Shor algorithm.},
author = {Pade, Jochen},
doi = {10.1007/978-3-319-00798-4},
file = {:C$\backslash$:/Users/user-pc/Desktop/2018{\_}Book{\_}QuantumMechanicsForPedestrians.pdf:pdf},
isbn = {978-3-319-00797-7},
pages = {452},
title = {{Quantum Mechanics for Pedestrians 1: Fundamentals}},
url = {http://link.springer.com/10.1007/978-3-319-00798-4},
year = {2014}
}
@inproceedings{Abadi2016,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {OSDI},
eprint = {1605.08695},
pages = {265--283},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
volume = {16},
year = {2016}
}
@article{Efron1979,
author = {Efron, Bradley},
isbn = {0090-5364},
journal = {The Annals of Statistics},
pages = {1--26},
title = {{Bootstrap methods: another look at the jackknife}},
year = {1979}
}
@article{Rosenwaike1979,
author = {Rosenwaike, Ira},
isbn = {0070-3370},
journal = {Demography},
number = {2},
pages = {279--288},
title = {{A new evaluation of United States census data on the extreme aged}},
volume = {16},
year = {1979}
}
@inproceedings{Richman2019a,
address = {Johannesburg},
author = {Richman, R and von Rummell, N and W{\"{u}}thrich, Mario V.},
booktitle = {Actuarial Society of South Africa Convention 2019},
file = {:C$\backslash$:/Users/user-pc/Desktop/2019 RichmanVRummell{\&}Wuthrich 2D Authors.pdf:pdf},
month = {sep},
publisher = {Actuarial Society of South Africa},
title = {{Believing the Bot - Model Risk in the Era of Deep Learning}},
url = {https://www.actuarialsociety.org.za/wp-content/uploads/2019/10/2019-RichmanVRummellWuthrich-FIN.pdf},
year = {2019}
}
@article{Currie2013,
author = {Currie, Iain D},
isbn = {1471-082X},
journal = {Statistical Modelling},
number = {1},
pages = {69--93},
title = {{Smoothing constrained generalized linear models with an application to the Lee-Carter model}},
volume = {13},
year = {2013}
}
@article{Hannun2014,
abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0{\%} error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
archivePrefix = {arXiv},
arxivId = {1412.5567},
author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
eprint = {1412.5567},
journal = {arXiv},
title = {{Deep Speech: Scaling up end-to-end speech recognition}},
url = {http://arxiv.org/abs/1412.5567},
volume = {arXiv:1412},
year = {2014}
}
@article{Jowett1992,
author = {Jowett, A John and Li, Yuan-Qing},
isbn = {0343-2521},
journal = {GeoJournal},
number = {4},
pages = {427--442},
title = {{Age—heaping: contrasting patterns from china}},
volume = {28},
year = {1992}
}
@book{Keyfitz2005,
author = {Keyfitz, Nathan and Caswell, Hal and Caswell, Hal and Keyfitz, Nathan},
publisher = {Springer},
title = {{Applied mathematical demography}},
volume = {47},
year = {2005}
}
@inproceedings{Gluck1997,
author = {Gluck, Spencer M},
booktitle = {Pcas},
pages = {482--532},
publisher = {Citeseer},
title = {{Balancing Development and Trend in Loss Reserve Analysis}},
volume = {84},
year = {1997}
}
@article{Myers1976,
author = {Myers, Robert J},
isbn = {0070-3370},
journal = {Demography},
pages = {577--580},
title = {{An instance of reverse heaping of ages}},
year = {1976}
}
@misc{Ohnsman2019,
abstract = {The Alphabet Inc. company also has a new definition for autonomous: if you need a driver's license it's not self-driving},
author = {Ohnsman, Alan},
booktitle = {Forbes},
language = {en},
title = {{Waymo Says More Of Its Self-Driving Cars Operating ‘Rider Only' With No One At Wheel}},
url = {https://www.forbes.com/sites/alanohnsman/2019/10/28/waymos-autonomous-car-definition-if-you-need-a-drivers-license-its-not-self-driving/},
urldate = {2019-11-10},
year = {2019}
}
@article{Hill1997,
author = {Hill, Mark E and Preston, Samuel H and Elo, Irma T and Rosenwaike, Ira},
isbn = {0037-7732},
journal = {Social Forces},
number = {3},
pages = {1007--1030},
title = {{Age-linked institutions and age reporting among older African Americans}},
volume = {75},
year = {1997}
}
@techreport{Alberg2017,
abstract = {On a periodic basis, publicly traded companies are required to report fundamentals: financial data such as revenue, operating income, debt, among others. These data points provide some insight into the financial health of a company. Academic research has identified some factors, i.e. computed features of the reported data, that are known through retrospective analysis to outperform the market average. Two popular factors are the book value normalized by market capitalization (book-to-market) and the operating income normalized by the enterprise value (EBIT/EV). In this paper: we first show through simulation that if we could (clairvoyantly) select stocks using factors calculated on future fundamentals (via oracle), then our portfolios would far outperform a standard factor approach. Motivated by this analysis, we train deep neural networks to forecast future fundamentals based on a trailing 5-years window. Quantitative analysis demonstrates a significant improvement in MSE over a naive strategy. Moreover, in retrospective analysis using an industry-grade stock portfolio simulator (backtester), we show an improvement in compounded annual return to 17.1{\%} (MLP) vs 14.4{\%} for a standard factor model.},
archivePrefix = {arXiv},
arxivId = {1711.04837},
author = {Alberg, John and Lipton, Zachary C.},
eprint = {1711.04837},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alberg, Lipton - 2017 - Improving Factor-Based Quantitative Investing by Forecasting Company Fundamentals.pdf:pdf},
isbn = {1711.04837v2},
title = {{Improving Factor-Based Quantitative Investing by Forecasting Company Fundamentals}},
url = {http://arxiv.org/abs/1711.04837},
year = {2017}
}
@article{Cairns2019,
abstract = {We introduce a new modelling framework to explain socio-economic differences in mortality in terms of an affluence index that combines information on individual wealth and income. The model is illustrated using data on older Danish males over the period 1985-2012 reported in the Statistics Denmark national register database. The model fits the historical mortality data well, captures their key features, generates smoothed death rates that allow us to work with a larger number of sub-groups than has previously been considered feasible, and has plausible projection properties.},
author = {Cairns, Andrew J.G. and Kallestrup-Lamb, Malene and Rosenskjold, Carsten and Blake, David and Dowd, Kevin},
doi = {10.1017/asb.2019.14},
issn = {17831350},
journal = {ASTIN Bulletin},
keywords = {Affluence index,CBD-X model,Danish mortality data,forward correlation term structure,gravity model,multipopulation mortality modelling},
month = {sep},
publisher = {Cambridge University Press},
title = {{MODELLING SOCIO-ECONOMIC DIFFERENCES in MORTALITY USING A NEW AFFLUENCE INDEX}},
year = {2019}
}
@article{Geladi1986,
abstract = {Metalloporphyrins such as Rh(III)TPPCI and Zr(IV)TPPCI2[1] were used as ionophores to prepare nanostructurated polymeric membrane sensors for thiocyanate and fluoride. Results from Potentiometric experiments suggest the optimal membrane composition with these molecular systems [2] which is essential to improve the selectivity and the stability of the Potentiometric response of the sensors. Sensors prepared exhibit rapid, fully reversible and Nernstian response towards thiocyanate and fluoride, in the large concentration range Two functional model of the sensors with internal solid contact and internal liquid (filling solution) contact were elaborated and tested. Their optimal construction is simple and robust enough. The sensors were successfully tested for monitoring thiocyanate and fluoride.},
author = {Vlascici, D. and Pica, Elena Maria and Fagadar-Cosma, E. and Cosma, V. and Bizerea, O.},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Geladi-1986-Partial least-squares regression{\_}.pdf:pdf},
isbn = {0003-2670},
issn = {14544164},
journal = {Journal of Optoelectronics and Advanced Materials},
keywords = {Fluoride,Internal filling solution contact,Internal solid contact,Ion - Selective electrodes,Metalloporphyrins,Thiocyanate},
number = {9},
pages = {2303--2306},
shorttitle = {Partial least-squares regression: a tutorial},
title = {{Thiocyanate and fluoride electrochemical sensors based on nanostructurated metalloporphyrin systems}},
volume = {10},
year = {2008}
}
@article{CentralStatisticalService1992,
author = {CSS, Central Statistics Service},
title = {{Population census, 1991. Adjustment for the undercount}},
year = {1991}
}
@article{Kestenbaum1992,
author = {Kestenbaum, Bert},
isbn = {0070-3370},
journal = {Demography},
number = {4},
pages = {565--580},
title = {{A description of the extreme aged population based on improved Medicare enrollment data}},
volume = {29},
year = {1992}
}
@book{Friedman2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression {\&} path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote apopular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
address = {New York},
author = {Ruppert, David},
booktitle = {Journal of the American Statistical Association},
doi = {10.1198/jasa.2004.s339},
isbn = {9780387848587 0387848584},
issn = {0162-1459},
language = {English},
number = {466},
pages = {567--567},
publisher = {Springer-Verlag},
title = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}},
url = {http://dx.doi.org/10.1007/978-0-387-84858-7},
volume = {99},
year = {2004}
}
@article{Oeppen1993,
author = {Oeppen, Jim},
isbn = {0032-4728},
journal = {Population Studies},
number = {2},
pages = {245--267},
title = {{Back projection and inverse projection: members of a wider class of constrained projection models}},
volume = {47},
year = {1993}
}
@article{Takeuchi2006,
abstract = {In regression, the desired estimate of y|x is not always given by a conditional mean, although this is most common. Sometimes one wants to obtain a good estimate that satisfies the property that a proportion, $\tau$, of y|x, will be below the estimate. For $\tau$ = 0.5 this is an estimate of the median. What might be called median regression, is subsumed under the term quantile regression. We present a nonparametric version of a quantile estimator, which can be obtained by solving a simple quadratic programming problem and provide uniform convergence statements and bounds on the quantile property of our estimator. Experimental results show the feasibility of the approach and competitiveness of our method with existing ones. We discuss several types of extensions including an approach to solve the quantile crossing problems, as well as a method to incorporate prior qualitative knowledge such as monotonicity constraints.},
author = {Takeuchi, Ichiro and Le, Quoc V and Sears, Timothy D and Smola, Alexander J},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Takeuchi et al. - 2006 - Nonparametric Quantile Estimation.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Estimation with constraints,Kernel methods,Nonparametric techniques,Quantile estimation,Support vector machines},
pages = {1231--1264},
title = {{Nonparametric quantile estimation}},
volume = {7},
year = {2006}
}
@article{Dorrington2001a,
author = {Dorrington, Rob and Bourne, David and Bradshaw, Debbie and Laubscher, Ria and Tim{\ae}us, Ian M},
title = {{The impact of HIV/AIDS on adult mortality in South Africa}},
year = {2001}
}
@article{Verbelen2018,
abstract = {A data set from a Belgian telematics product aimed at young drivers is used to identify how car insurance premiums can be designed based on the telematics data collected by a black box installed in the vehicle. In traditional pricing models for car insurance, the premium depends on self-reported rating variables (e.g. age and postal code) which capture characteristics of the policy(holder) and the insured vehicle and are often only indirectly related to the accident risk. Using telematics technology enables tailor-made car insurance pricing based on the driving behaviour of the policyholder. We develop a statistical modelling approach using generalized additive models and compositional predictors to quantify and interpret the effect of telematics variables on the expected claim frequency. We find that such variables increase the predictive power and render the use of gender as a rating variable redundant.},
author = {Verbelen, Roel and Antonio, Katrien and Claeskens, Gerda},
doi = {10.1111/rssc.12283},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/R-Unravelling the predictive power of telemati.pdf:pdf},
issn = {14679876},
journal = {Journal of the Royal Statistical Society. Series C: Applied Statistics},
keywords = {Compositional predictors,Generalized additive models,Pay as you drive insurance,Risk classification,Structural 0s,Usage-based insurance},
number = {5},
pages = {1275--1304},
shorttitle = {Unravelling the predictive power of telematics dat},
title = {{Unravelling the predictive power of telematics data in car insurance pricing}},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12283},
volume = {67},
year = {2018}
}
@article{Turner2007,
abstract = {The gnm package provides facilities for fitting generalized nonlinear models, i.e., regression models in which the link- transformed mean is described as a sum of predictor terms, some of which may be non-linear in the unknown parameters. Linear and generalized linear models, as handled by the lm and glm functions in R, are included in the class of generalized nonlinear models, as the special case in which there is no nonlinear term. This document gives an extended overview of the gnm package, with some examples of applications. The primary package documentation in the form of standard help pages, as viewed in R by, for example, ?gnm or help(gnm), is supplemented rather than replaced by the present document. We begin below with a preliminary note (Section 2) on some ways in which the gnm package extends R's facilities for specifying, fitting and working with generalized linear models. Then (Section 3 onwards) the facilities for nonlinear terms are introduced, explained and exemplified.},
author = {Turner, Heather and Firth, David},
journal = {Social Research},
pages = {1--55},
title = {{Generalized nonlinear models in R : An overview of the gnm package Generalized linear models Preamble}},
year = {2008}
}
@misc{Noll2018,
abstract = {We provide a tutorial that compares a classical generalized linear model for claims frequency modeling to regression tree, boosting machine and neural network approaches. We explore these methods, discuss their calibration and study their predictive power on an explicit motor third-party liability insurance data set. The results of the case study show that a simple generalized linear model does not capture interactions of feature components appropriately, whereas the other methods are able to address these interactions.},
author = {Noll, Alexander and Salzmann, Robert and W{\"{u}}thrich, Mario V.},
booktitle = {SSRN Electronic Journal},
doi = {10.2139/ssrn.3164764},
howpublished = {SSRN},
shorttitle = {Case Study: French Motor Third-Party Liability Cla},
title = {{Case Study: French Motor Third-Party Liability Claims}},
url = {https://ssrn.com/abstract=3164764},
year = {2018}
}
@article{Bhat2002a,
abstract = {In the preceding issue of this journal, a generalized version of the Brass growth balance method was proposed that made it applicable to populations that are not stable and are open to migration. In this companion paper, the results of applying this new procedure to data from India's Sample Registration System for the decades 1971-80 and 1981-90 are discussed. The results at the national level show that, during the decade 1981-90, 5 per cent of the deaths among men, 12 per cent of the deaths among women, and about 7 per cent of births were being missed by the system. Further, it is estimated that the level of under-enumeration in the 1991 Census was more than that of the 1981 Census by 0.7 per cent for males and 1.4 per cent for females. The paper also presents results for major Indian states.},
author = {Bhat, P. N.Mari},
doi = {10.1080/00324720215930},
isbn = {0032-4728
1477-4747},
issn = {00324728},
journal = {Population Studies},
number = {2},
pages = {119--134},
title = {{Completeness of India's sample registration system: An assessment using the general growth balance method}},
volume = {56},
year = {2002}
}
@misc{Wikipedi2019,
abstract = {The Dartmouth Summer Research Project on Artificial Intelligence was a 1956 summer workshop widely considered to be the founding event of artificial intelligence as a field. The project lasted approximately six to eight weeks, and was essentially an extended brainstorming session. Eleven mathematicians and scientists originally planned to attend; not all of them attended, but more than ten others came for short times.},
author = {Wikipedia},
booktitle = {Wikipedia},
language = {en},
month = {sep},
title = {{Dartmouth workshop}},
url = {https://en.wikipedia.org/w/index.php?title=Dartmouth{\_}workshop{\&}oldid=916260002},
year = {2019}
}
@misc{UniversityofCalifornia2019,
author = {{University of California}, Berkeley (USA)},
title = {{United States Mortality DataBase}},
url = {usa.mortality.org},
urldate = {2020-03-30},
year = {2019}
}
@book{Meyers2019,
address = {Arlington, VA},
author = {Meyers, Glenn},
edition = {2},
publisher = {Casualty Actuarial Society},
title = {{CAS Monograph No. 8 - Stochastic Loss Reserving Using Bayesian MCMC Models (2nd edition)}},
url = {https://www.casact.org/pubs/monographs/index.cfm?fa=meyers-monograph08},
volume = {8},
year = {2019}
}
@article{Rosenwaike1996,
author = {Rosenwaike, Ira and Hill, Mark E},
isbn = {0164-0275},
journal = {Research on Aging},
number = {3},
pages = {310--324},
title = {{The accuracy of age reporting among elderly african americans evidence of a birth registration effect}},
volume = {18},
year = {1996}
}
@article{Shea1985,
author = {Shea, Gary S.},
doi = {10.2307/2328063},
issn = {00221082},
journal = {The Journal of Finance},
month = {mar},
number = {1},
pages = {319},
publisher = {JSTOR},
title = {{Interest Rate Term Structure Estimation with Exponential Splines: A Note}},
volume = {40},
year = {1985}
}
@inproceedings{Nair2010,
abstract = {A typical gait analysis data collection consists of a series of discrete trials, where a participant initiates gait, walks through a motion capture volume, and then terminates gait. This is not a normal ‘everyday' gait pattern, yet measurements are considered representative of normal walking. However, walking speed, a global descriptor of gait quality that can affect joint kinematics and kinetics, may be different during discrete trials, compared to continuous walking. Therefore, the purpose of this study was to investigate the effect of continuous walking versus discrete trials on walking speed and walking speed variability. Data were collected for 25 healthy young adults performing 2 walking tasks. The first task represented a typical gait data collection session, where subjects completed repeated trials, beginning from a standstill and walking along a 12-m walkway. The second task was continuous walking along a “figure-of-8” circuit, with 1 section containing the same 12-m walkway. Walking speed was significantly higher during the discrete trials compared to the continuous trials (p {\textless} .001), but there were no significant differences in walking speed variability between the conditions. The results suggest that choice of gait protocol may affect results where variables are sensitive to walking speed.},
author = {Brown, Marcus J. and Hutchinson, Laura A. and Rainbow, Michael J. and Deluzio, Kevin J. and {De Asha}, Alan R.},
booktitle = {Journal of Applied Biomechanics},
doi = {10.1123/jab.2016-0355},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Nair-2010-Rectified linear units improve restr.pdf:pdf},
issn = {15432688},
keywords = {Gait,Methodology,Walking speed},
number = {5},
pages = {384--387},
shorttitle = {Rectified linear units improve restricted boltzman},
title = {{A comparison of self-selected walking speeds and walking speed variability when data are collected during repeated discrete trials and during continuous walking}},
volume = {33},
year = {2017}
}
@article{Hinton2006a,
author = {Hinton, G and Salakhutdinov, R},
issn = {0036-8075},
journal = {Science},
number = {5786},
pages = {504--507},
shorttitle = {Reducing the dimensionality of data with neural ne},
title = {{Reducing the dimensionality of data with neural networks}},
volume = {313},
year = {2006}
}
@article{Bradshaw2012,
author = {Bradshaw, Debbie and Dorrington, Rob},
journal = {Cape Town: South African Medical Research Council},
title = {{Rapid Mortality Surveillance Report 2011}},
year = {2012}
}
@article{Vaupel1986,
author = {Vaupel, James W},
isbn = {0032-4728},
journal = {Population Studies},
number = {1},
pages = {147--157},
title = {{How change in age-specific mortality affects life expectancy}},
volume = {40},
year = {1986}
}
@article{Pedregosa2011,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algo-rithms for medium-scale supervised and unsupervised problems. This package focuses on bring-ing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependen-cies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
author = {{Pedregosa FABIANPEDREGOSA}, Fabian and {Alexandre Gramfort}, Normalesuporg and Michel, Vincent and {Thirion BERTRANDTHIRION}, Bertrand and Grisel, Olivier and Blondel, Mathieu and {Prettenhofer PETERPRETTENHOFER}, Peter and Weiss, Ron and Dubourg, Vincent and {Vanderplas VANDERPLAS}, Jake and Passos, Alexandre and Cournapeau, David and Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Thirion, Bertrand and Prettenhofer, Peter and Vanderplas, Jake and Brucher, Matthieu and {Perrot an Edouard Duchesnay PEDREGOSA}, Matthieu and {Matthieu Brucher MATTHIEUBRUCHER}, Al and {Perrot MATTHIEUPERROT}, Matthieu and {Edouard Duchesnay EDOUARDDUCHESNAY}, Cea F},
journal = {Journal of Machine Learning Research},
number = {Oct},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in {\{}P{\}}ython}},
volume = {12},
year = {2011}
}
@article{Gallop2005,
abstract = {As in other developed countries, mortality rates at advanced ages have fallen quite dramatically over the last century in the United Kingdom. However, data have not been readily available for the countries of the United Kingdom (England, Scotland, Wales and Northern Ireland) to calculate mortality rates at individual ages 85 and over. The paper gives an overview of the data that are available and discusses the problems encountered in estimating mortality rates at old ages in the UK. It describes some of the methodologies used to construct mortality rates at advanced ages for official life tables and recent work undertaken by the UK Government Actuary's Department to construct a database of historical mortality rates for the United Kingdom going back to 1961. Possible methods for projecting mortality rates at advanced ages are also discussed.},
author = {Gallop, AP and Macdonald, AS},
journal = {Living to 100 and Beyond Monograph},
pages = {1--35},
title = {{Mortality at advanced ages in the United Kingdom}},
url = {http://www.soa.org/library/monographs/life/living-to-100/2002/mono-2002-m-li-02-1-gallop.pdf},
year = {2005}
}
@techreport{Services2017,
author = {{Tata Consultancy Services}},
title = {{Getting Smarter by the Day: How AI is Elevating the Performance of Global Companies}},
url = {https://sites.tcs.com/artificial-intelligence/{\#}},
year = {2017}
}
@article{Spoorenberg2008,
author = {Spoorenberg, Thomas},
doi = {10.4054/DemRes.2008.18.10},
isbn = {1435-9871},
journal = {Demographic Research},
pages = {285--310},
title = {{What can we learn from indirect estimations on mortality in Mongolia, 1969-1989?}},
volume = {18},
year = {2008}
}
@article{Rosenblatt1958a,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
issn = {0033295X},
journal = {Psychological Review},
keywords = {PERCEPTION, AS INFORMATION STORAGE MODEL INFORMATI},
month = {nov},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
volume = {65},
year = {1958}
}
@article{Preston1983a,
author = {Preston, Samuel H and Bennett, Neil G},
isbn = {0032-4728},
journal = {Population Studies},
number = {1},
pages = {91--104},
title = {{A census-based method for estimating adult mortality}},
volume = {37},
year = {1983}
}
@misc{Richman2017,
address = {Cape Town},
author = {Richman, R},
publisher = {University of Cape Town},
title = {{Old age mortality in South Africa}},
year = {2017}
}
@article{Caldwell1971,
abstract = {Censuses were held and data subsequently published in Nigeria in 1952/3 and 1963. Age data and other aspects of the enumerations gave rise to considerable doubts. In 1969 the Demographic Training and Research Unit of the University of Ife carried out a census of ages amongst 10,000 persons in the Western State of Nigeria employing in succession orthodox methods, supporting historical records, and two approaches involving the identification of single-year cohorts of contemporaries. Certain other data were also collected for explanatory purposes. The project confirmedthe tendency for age misstatement to exaggerate the number of females aged 20-44 and the number of older males, and to understate the number of persons aged 0-9 and females aged 50-59. It was shown that amongst females, aged 15-24, there is a significant tendency for the ages of the unmarried to be understated and those of the married, expecially those of higher parity, to be overstated. Such tendencies may be reduced by any type of enumeration if more time and care is allowed but anomalously this may increase the underenumeration of the very young. The more elaborate methods of enumeration are examinedto see whether they yield more accurate data and whether their use would be possible in a national census. Age statement was examined in some detail and it was shown that the majority of data originate neither with respondents nor enumerators but with third parties, who may wellbe continuing and untrainable sources of biassed error. The project provided more evidence than anticipated on the validity of the 1963 Census in the area, thus leaving the relative validity of the 1952/3 and 1963 censuses and hence the likely population of Nigeria still an open question. {\textcopyright} 1971 Taylor {\&} Francis Group, LLC.},
author = {Caldwell, J. C. and Igun, A. A.},
doi = {10.1080/00324728.1971.10405804},
isbn = {0032-4728},
issn = {14774747},
journal = {Population Studies},
number = {2},
pages = {287--302},
title = {{An experiment with census-type age enumeration in Nigeria}},
volume = {25},
year = {1971}
}
@phdthesis{Andreev1999,
address = {Odense},
author = {Andreev, Kirill},
publisher = {University of Southern Denmark},
title = {{Demographic Surfaces : Estimation, Assessment and Presentation , with Application to Danish Mortality, 1835-1995}},
url = {https://www.demogr.mpg.de/en/projects{\_}publications/publications{\_}1904/dissertations/demographic{\_}surfaces{\_}estimation{\_}assessment{\_}and{\_}presentation{\_}with{\_}application{\_}to{\_}danish{\_}mortality{\_}148.htm},
volume = {PhD},
year = {1999}
}
@techreport{loretan1997generating,
author = {Loretan, Mico},
booktitle = {Federal reserve board},
institution = {Federal Reserve Board},
pages = {23--60},
title = {{Generating market risk scenarios using principal components analysis: methodological and practical considerations}},
url = {https://www.bis.org/publ/ecsc07b.pdf},
year = {1997}
}
@inproceedings{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1310.4546},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Mikolov-2013-Distributed representations of wo.pdf:pdf},
issn = {10495258},
pages = {1--31},
title = {{Distributed representations ofwords and phrases and their compositionality}},
year = {2013}
}
@article{macdonald1997current,
author = {Macdonald, Angus S.},
doi = {10.1080/10920277.1997.10595624},
issn = {10920277},
journal = {North American Actuarial Journal},
number = {3},
pages = {24--35},
publisher = {Taylor {\&} Francis},
title = {{Current actuarial modeling practice and related issues and questions}},
volume = {1},
year = {1997}
}
@misc{Kassambara2018,
abstract = {The 'ggplot2' package is excellent and flexible for elegant data visualization in R. However the default generated plots requires some formatting before we can send them for publication. Furthermore, to customize a 'ggplot', the syntax is opaque and this raises the level of difficulty for researchers with no advanced R programming skills. 'ggpubr' provides some easy-to-use functions for creating and customizing 'ggplot2'- based publication ready plots.},
author = {Kassambara, Alboukadel},
booktitle = {https://CRAN.R-project.org/package=ggpubr},
doi = {R package version 0.1.8},
shorttitle = {ggpubr: 'ggplot2' Based Publication Ready Plots},
title = {{ggpubr: 'ggplot2' Based Publication Ready Plots. R package version 0.1.7.}},
url = {https://cran.r-project.org/web/packages/ggpubr/index.html},
year = {2018}
}
@article{fan2019selective,
abstract = {Deep learning has arguably achieved tremendous success in recent years. In simple words, deep learning uses the composition of many nonlinear functions to model the complex dependency between input features and labels. While neural networks have a long history, recent advances have greatly improved their performance in computer vision, natural language processing, etc. From the statistical and scientific perspective, it is natural to ask: What is deep learning? What are the new characteristics of deep learning, compared with classical methods? What are the theoretical foundations of deep learning? To answer these questions, we introduce common neural network models (e.g., convolutional neural nets, recurrent neural nets, generative adversarial nets) and training techniques (e.g., stochastic gradient descent, dropout, batch normalization) from a statistical point of view. Along the way, we highlight new characteristics of deep learning (including depth and over-parametrization) and explain their practical and theoretical benefits. We also sample recent results on theories of deep learning, many of which are only suggestive. While a complete understanding of deep learning remains elusive, we hope that our perspectives and discussions serve as a stimulus for new statistical research.},
archivePrefix = {arXiv},
arxivId = {1904.05526},
author = {Fan, Jianqing and Ma, Cong and Zhong, Yiqiao},
eprint = {1904.05526},
journal = {arXiv preprint arXiv:1904.05526},
title = {{A Selective Overview of Deep Learning}},
url = {http://arxiv.org/abs/1904.05526},
year = {2019}
}
@misc{Gesmann2020,
author = {Gesmann, Markus and Murphy, Daniel and Zhang, Wayne and Carrato, Alessandro and Crupi, Giuseppe and W{\"{u}}thrich, Mario V.},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Statistical Methods and Models for Claims Reserving in General Insurance [R package ChainLadder version 0.2.11]}},
year = {2020}
}
@article{black2018model,
abstract = {This paper presents latest thinking from the Institute and Faculty of Actuaries' Model Risk Working Party and follows on from their Phase I work, Model Risk: Daring to Open the Black Box. This is a more practical paper and presents the contributors' experiences of model risk gained from a wide range of financial and non-financial organisations with suggestions for good practice and proven methods to reduce model risk. After a recap of the Phase I work, examples of model risk communication are given covering communication: To the Board; to the regulator; and to external stakeholders. We present a practical framework for model risk management and quantification with examples of the key actors, processes and cultural challenge. Lessons learned are then presented from other industries that make extensive use of models and include the weather forecasting, software and aerospace industries. Finally, a series of case studies in practical model risk management and mitigation are presented from the contributors' own experiences covering primarily financial services.},
author = {Black, R. and Tsanakas, A. and Smith, A. D. and Beck, M. B. and Maclugash, I. D. and Grewal, J. and Witts, L. and Morjaria, N. and Green, R. J. and Lim, Z.},
doi = {10.1017/S1357321718000119},
issn = {20440456},
journal = {British Actuarial Journal},
keywords = {Model,Model error,Model risk,Model uncertainty,Risk culture},
publisher = {Cambridge University Press},
title = {{Model risk: Illuminating the black box}},
volume = {23},
year = {2018}
}
@article{verrall2000comments,
author = {Verrall, R.J and England, Peter},
doi = {10.1016/s0167-6687(99)00040-2},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
number = {1},
pages = {109--111},
publisher = {North-Holland},
title = {{Comments on: “A comparison of stochastic models that reproduce chain ladder reserve estimates”, by Mack and Venter}},
volume = {26},
year = {2000}
}
@article{Johnson2015,
author = {Johnson, Leigh F and Dorrington, Rob E and Laubscher, Ria and Hoffmann, Christopher J and Wood, Robin and Fox, Matthew P and Cornell, Morna and Schomaker, Michael and Prozesky, Hans and Tanser, Frank},
journal = {Journal of the International AIDS Society},
number = {1},
title = {{A comparison of death recording by health centres and civil registration in South Africans receiving antiretroviral treatment}},
volume = {18},
year = {2015}
}
@article{Maaten2008a,
author = {Maaten, L and Hinton, G},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Maaten-2008-Visualizing data using t-SNE.pdf:pdf},
journal = {Journal of machine learning research},
number = {Nov},
pages = {2579--2605},
title = {{Visualizing data using t-SNE}},
volume = {9},
year = {2008}
}
@article{Delong2020,
author = {Delong, Lukasz and Lindholm, Mathias and Wuthrich, Mario V.},
doi = {10.2139/ssrn.3582398},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {IBNR claims,RBNS claims,chain-ladder method,claims reserving,general insurance,individual claims data,micro-level reserving,neural networks,over-dispersed Poisson model},
month = {apr},
title = {{Collective Reserving using Individual Claims Data}},
url = {https://www.ssrn.com/abstract=3582398},
year = {2020}
}
@article{Whelan2009,
author = {Whelan, S F},
isbn = {1748-5002},
journal = {Annals of Actuarial Science},
number = {01},
pages = {33--66},
title = {{Mortality in Ireland at advanced ages, 1950-2006: Part 1: crude rates}},
volume = {4},
year = {2009}
}
@article{Preston2012,
author = {Preston, Samuel H and Stokes, Andrew},
isbn = {1728-4457},
journal = {Population and development review},
number = {2},
pages = {221--236},
title = {{Sources of population aging in more and less developed countries}},
volume = {38},
year = {2012}
}
@article{Imbens2018,
abstract = {The bootstrap, introduced by Efron (1982), has become a very popular method for estimating variances and constructing confidence intervals. A key insight is that one can approximate the properties of estimators by using the empirical distribution function of the sample as an approximation for the true distribution function. This approach views the uncertainty in the estimator as coming exclusively from sampling uncertainty. We argue that for causal estimands the uncertainty arises entirely, or partially, from a different source, corresponding to the stochastic nature of the treatment received. We develop a bootstrap procedure that accounts for this uncertainty, and compare its properties to that of the classical bootstrap.},
archivePrefix = {arXiv},
arxivId = {1807.02737},
author = {Imbens, Guido and Menzel, Konrad},
eprint = {1807.02737},
file = {:C$\backslash$:/R/refas/1807.02737.pdf:pdf},
keywords = {bootstrap,causality,copula,partial identification,potential outcomes,randomization inference},
number = {December 2018},
pages = {1--27},
title = {{A Causal Bootstrap}},
url = {http://arxiv.org/abs/1807.02737},
year = {2018}
}
@article{Makridakis2018a,
abstract = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
doi = {10.1371/journal.pone.0194889},
issn = {19326203},
journal = {PLoS ONE},
number = {3},
pages = {e0194889},
publisher = {Public Library of Science},
title = {{Statistical and Machine Learning forecasting methods: Concerns and ways forward}},
url = {https://doi.org/10.1371/journal.pone.0194889},
volume = {13},
year = {2018}
}
@misc{Parodi2016,
address = {Dublin},
author = {Parodi, P},
booktitle = {GIRO 2016},
title = {{Towards machine pricing}},
year = {2016}
}
@book{Frees2014,
abstract = {Chapter Preview. Predictive modeling involves the use of data to forecast future events. It relies on capturing relationships between explanatory variables and the predicted variables from past occurrences and exploiting them to predict future outcomes. The goal of this two-volume set is to build on the training of actuaries by developing the fundamentals of predictive modeling and providing corresponding applications in actuarial science, risk management, and insurance. This introduction sets the stage for these volumes by describing the conditions that led to the need for predictive modeling in the insurance industry. It then traces the evolution of predictive modeling that led to the current statistical methodologies that prevail in actuarial science today. Introduction A classic definition of an actuary is "one who determines the current financial impact of future contingent events." Actuaries are typically employed by insurance companies who job is to spread the cost of risk of these future contingent events. The day-to-day work of an actuary has evolved over time. Initially, the work involved tabulating outcomes for "like" events and calculating the average outcome. For example, an actuary might be called on to estimate the cost of providing a death benefit to each member of a group of 45-year-old men. As a second example, an actuary might be called on to estimate the cost of damages that arise from an automobile accident for a 45-year-old driver living in Chicago.},
author = {{Jed Frees}, Edward W. and Derrig, Richard A. and Meyers, Glenn and Frees, Edward W and Derrig, Richard A. and Meyers, Glenn},
booktitle = {Predictive Modeling Applications in Actuarial Science},
doi = {10.1017/CBO9781139342674.001},
isbn = {9781139342674},
keywords = {Finance and accountancy,Finance and insurance,Statistics for econometrics},
pages = {1--10},
publisher = {Cambridge University Press},
title = {{Predictive Modeling in Actuarial Science}},
volume = {1},
year = {2014}
}
@book{Cao2019,
author = {Cao, Larry},
editor = {Preece, Rhodri},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao - 2019 - AI Pioneers in Investment Management - An examination of the trends and use cases of AI and big data technologies in invest.pdf:pdf},
isbn = {9781942713784},
publisher = {CFA Institute},
title = {{AI Pioneers in Investment Management - An examination of the trends and use cases of AI and big data technologies in investments}},
url = {www.cfainstitute.org},
year = {2019}
}
@article{Gao2020a,
author = {Gao, Guangyuan and Wang, He and Wuthrich, Mario V.},
doi = {10.2139/ssrn.3596034},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {Claims frequency modeling,Combined actuarial neural network,Convolutional neural network,Densely connected neural network,Generalized linear model,Poisson regression,Regression tree,Telematics car driving data,Telematics heatmap},
month = {may},
title = {{Boosting Poisson Regression Models with Telematics Car Driving Data}},
url = {https://www.ssrn.com/abstract=3596034},
year = {2020}
}
@misc{Board,
author = {{Financial Services Board}},
editor = {Group, Solvency Assessment and Management: Pillar 1 Sub Committee Capital Requirements Task},
publisher = {Financial Services Board},
title = {{Discussion Document 64 (v 3) Life SCR - Longevity Risk}}
}
@article{Wang1996,
abstract = {This paper examines a class of premium functionals which are (i) comonotonic additive and (ii) stochastic dominance preservative. The representation for this class is a transformation of the decumulative distribution function. It has close connections with the recent developments in economic decision theory and non-additive measure theory. Among a few elementary members of this class, the proportional hazard transform seems to stand out as being most plausible for actuaries.},
author = {Wang, Shaun},
doi = {10.2143/ast.26.1.563234},
issn = {0515-0361},
journal = {ASTIN Bulletin},
keywords = {Premium calculation principle,comonotonicity,mean-variance analysis,proportional hazard transform,stochastic dominance},
month = {may},
number = {1},
pages = {71--92},
publisher = {Cambridge University Press (CUP)},
title = {{Premium Calculation by Transforming the Layer Premium Density}},
volume = {26},
year = {1996}
}
@misc{Albright2017a,
author = {KPMG},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/LLP-2017-The Chaotic Middle.pdf:pdf},
howpublished = {KPMG},
number = {June},
pages = {64},
publisher = {KPMG},
shorttitle = {The Chaotic Middle},
title = {{The chaotic middle}},
url = {https://assets.kpmg.com/content/dam/kpmg/us/pdf/2017/06/chaotic-middle-autonomous-vehicle-paper.pdf},
volume = {2018},
year = {2017}
}
@article{Perla2020,
author = {Perla, Francesca and Richman, Ronald and Scognamiglio, Salvatore and W{\"{u}}thrich, M. V.},
journal = {SSRN Electronic Journal},
keywords = {Human Mortality Database,Lee Carter model,Mortality forecasting,convolutional neural networks,recurrent neural networks,representation learning,time-series forecasting},
month = {may},
title = {{Time-Series Forecasting of Mortality Rates using Deep Learning}},
year = {2020}
}
@article{Parodi2012,
abstract = { This paper argues that most of the problems that actuaries have to deal with in the context of non-life insurance can be usefully cast in the framework of computational intelligence (a.k.a. artificial intelligence), the discipline that studies the design of agents which exhibit intelligent behaviour. Finding an adequate framework for actuarial problems has more than a simply theoretical interest: it also allows a technological transfer from the computational intelligence discipline to general insurance, wherever techniques have been developed for problems which are common to both contexts. This has already happened in the past (neural networks, clustering, data mining have all found applications to general insurance) but not in a systematic way. One of the objectives of this paper will therefore be to introduce some useful techniques such as sparsity-based regularisation and dynamic decision networks that are not yet known to the wider actuarial community. Whilst in the first part of this paper we dealt mainly with data-driven loss modelling under the assumption that all the data were accurate and fully relevant to the exercise, in this second part of the paper we explore how to deal with uncertain knowledge, whether this uncertainty comes from the fact that the data are not fully reliable (e.g. they are estimates) or from the fact that the knowledge is “soft” (e.g. expert beliefs) or not fully relevant (e.g. market information on a given risk). Most importantly, we will deal with the problem of making pricing, reserving and capital decisions under uncertainty. It will be concluded that a Bayesian framework is the most adequate for dealing with uncertainty, and we will present a number of computational intelligence techniques to do this in practice.},
author = {Parodi, Pietro},
doi = {10.1017/s1748499512000048},
edition = {05/22},
isbn = {1748-4995},
issn = {1748-4995},
journal = {Annals of Actuarial Science},
keywords = {Bayesian networks,Dynamic decision networks,Expert systems,Fuzzy set theory,Game theory,Intelligent agents,Kalman filtering,Markov decision processes,Non-monotonic reasoning,Regularisation,Risk agents},
number = {2},
pages = {344--380},
publisher = {Cambridge University Press},
title = {{Computational intelligence with applications to general insurance: a review}},
url = {https://www.cambridge.org/core/article/computational-intelligence-with-applications-to-general-insurance-a-review/53248EDD1DF63ABDBB6B055010D328A2},
volume = {6},
year = {2012}
}
@article{ONS2018,
author = {Evans, Alan},
journal = {Office for National Statistics},
number = {August},
pages = {1--14},
title = {{Changing trends in mortality: an international comparison: 2000 to 2016}},
url = {https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/lifeexpectancies/articles/changingtrendsinmortality/acrossukcomparison1981to2016},
year = {2018}
}
@article{Bengio2010,
abstract = {The family of decision tree learning algorithms is among the most widespread and studied. Motivated by the desire to develop learning algorithms that can generalize when learning highly varying functions such as those presumably needed to achieve artificial intelligence, we study some theoretical limitations of decision trees. We demonstrate formally that they can be seriously hurt by the curse of dimensionality in a sense that is a bit different from other nonparametric statistical methods, but most importantly, that they cannot generalize to variations not seen in the training set. This is because a decision tree creates a partition of the input space and needs at least one example in each of the regions associated with a leaf to make a sensible prediction in that region. A better understanding of the fundamental reasons for this limitation suggests that one should use forests or even deeper architectures instead of trees, which provide a form of distributed representation and can generalize to variations not encountered in the training data. {\textcopyright} 2010 Wiley Periodicals, Inc.},
author = {Bengio, Yoshua and Delalleau, Olivier and Simard, Clarence},
doi = {10.1111/j.1467-8640.2010.00366.x},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Delalleau, Simard - 2010 - Decision trees do not generalize to new variations.pdf:pdf},
issn = {08247935},
journal = {Computational Intelligence},
keywords = {Curse of dimensionality,Decision trees,Parity function},
number = {4},
pages = {449--467},
title = {{Decision trees do not generalize to new variations}},
volume = {26},
year = {2010}
}
@techreport{CSI2017,
address = {Cape Town},
author = {CSI},
institution = {Actuarial Society of South Africa},
language = {en},
month = {jul},
publisher = {Continuous Statistical Investigation Committee, Actuarial Society of South Africa},
title = {{Report on Pensioner Mortality 2005-2010}},
year = {2017}
}
@article{Jin2018,
abstract = {Car insurance is quickly becoming a big data industry, with usage-based insurance (UBI) poised to potentially change the business of insurance. Telematics data, which are transmitted from wireless devices in car, are widely used in UBI to obtain individual-level travel and driving characteristics. While most existing studies have introduced telematics data into car insurance pricing, the telematics-related characteristics are directly obtained from the raw data. In this study, we propose to quantify drivers' familiarity with their driving routes and develop models to quantify drivers' accident risks using the telematics data. In addition, we build a latent class model to study the heterogeneity in travel and driving styles based on the telematics data, which has not been investigated in literature. Our main results include: (1) the improvement to the model fit is statistically significant by adding telematics-related characteristics; (2) drivers' familiarity with their driving trips is critical to identify high risk drivers, and the relationship between drivers' familiarity and accident risks is non-linear; (3) the drivers can be classified into two classes, where the first class is the low risk class with 0.54{\%} of its drivers reporting accidents, and the second class is the high risk class with 20.66{\%} of its drivers reporting accidents; and (4) for the low risk class, drivers with high probability of reporting accidents can be identified by travel-behavior-related characteristics, while for the high risk class, they can be identified by driving-behavior-related characteristics. The driver's familiarity will affect the probability of reporting accidents for both classes.},
author = {Jin, Wen and Deng, Yinglu and Jiang, Hai and Xie, Qianyan and Shen, Wei and Han, Weijian},
doi = {10.1016/j.aap.2018.02.023},
issn = {00014575},
journal = {Accident Analysis and Prevention},
keywords = {Accident risk,Latent class model,Usage-based insurance,Vehicle telematics},
pages = {79--88},
title = {{Latent class analysis of accident risks in usage-based insurance: Evidence from Beijing}},
volume = {115},
year = {2018}
}
@book{James2013,
abstract = {File swarming (or file sharing) is one of the most important applications in P2P networks. In this paper, we propose a stochastic framework to analyze a file-swarming system under realistic setting: constraints in upload/download capacity, collaboration among peers and incentive for chunk exchange. We first extend the results in the coupon system [L. Massoulie, M. Vojnovic, Coupon replication systems, in: Proc. ACM SIGMETRICS, Banff, Alberta, Canada, 2005] by providing a tighter performance bound. Then we generalize the coupon system by considering peers with limited upload and download capacity. We illustrate the last-piece problem and show the effectiveness of using forward error-correction (FEC) code and/or multiple requests to improve the performance. Lastly, we propose a framework to analyze an incentive-based file-swarming system. The stochastic framework we propose can serve as a basis for other researchers to analyze and design more advanced features of file-swarming systems. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Lin, Minghong and Fan, Bin and Lui, John C.S. and Chiu, Dah Ming},
booktitle = {Performance Evaluation},
doi = {10.1016/j.peva.2007.06.006},
isbn = {9780387781884},
issn = {01665316},
keywords = {BitTorrent,P2P file sharing,Performance modeling},
number = {9-12},
pages = {856--875},
pmid = {10911016},
publisher = {Springer},
title = {{Stochastic analysis of file-swarming systems}},
url = {http://books.google.com/books?id=9tv0taI8l6YC},
volume = {64},
year = {2007}
}
@article{Maaten2008,
abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {{Van Der Maaten}, Laurens and Hinton, Geoffrey},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Dimensionality reduction,Embedding algorithms,Manifold learning,Multidimensional scaling,Visualization},
number = {Nov},
pages = {2579--2625},
title = {{Visualizing data using t-SNE}},
volume = {9},
year = {2008}
}
@article{Dorrington1999,
author = {Dorrington, Rob E and Bradshaw, Debbie and Wegner, Trevor},
title = {{Estimates of the level and shape of mortality rates in South Africa around 1985 and 1990 derived by applying indirect demographic techniques to reported deaths}},
year = {1999}
}
@article{VanDenOord,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Den Oord et al. - Unknown - WAVENET A GENERATIVE MODEL FOR RAW AUDIO.pdf:pdf},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@incollection{Bah2005,
author = {Bah, S M},
booktitle = {The Demography of South Africa},
editor = {Zuberi, Tukufu and Sibanda, Amson and Udjo, Eric},
isbn = {0765615630},
pages = {310},
title = {{Technical Appraisal of Official South African Life Tables}},
year = {2005}
}
@book{Bambi,
author = {Bambi, Cosimo},
file = {:C$\backslash$:/Users/user-pc/Desktop/2018{\_}Book{\_}IntroductionToGeneralRelativit.pdf:pdf},
isbn = {9789811310898},
title = {{Undergraduate Lecture Notes in Physics Introduction to General Relativity}}
}
@article{Forfar1988,
author = {Forfar, D O and McCutcheon, J J and Wilkie, A D},
journal = {Journal of the Institute of Actuaries},
number = {01},
pages = {1--149},
title = {{On graduation by mathematical formula}},
volume = {115},
year = {1988}
}
@article{Wijnands2018,
author = {Wijnands, J and Thompson, J and Aschwanden, G and Stevenson, M},
issn = {1369-8478},
journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
pages = {34--49},
shorttitle = {Identifying behavioural change among drivers using},
title = {{Identifying behavioural change among drivers using Long Short-Term Memory recurrent neural networks}},
volume = {53},
year = {2018}
}
@misc{Wuethrich2018f,
abstract = {Classical claims reserving methods act on so-called claims reserving triangles which are aggregated insurance portfolios. A crucial assumption in classical claims reserving is that these aggregated portfolios are sufficiently homogeneous so that a coarse reserving algorithm can be applied. We start from such a coarse reserving method, which in our case is Mack's chain–ladder method, and show how this approach can be refined for heterogeneity and individual claims feature information using neural networks.},
author = {W{\"{u}}thrich, Mario V.},
booktitle = {European Actuarial Journal},
doi = {10.1007/s13385-018-0184-4},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/W{\"{u}}thrich-2018-Neural networks applied to chain.pdf:pdf},
howpublished = {SSRN},
issn = {21909741},
keywords = {Claims covariates,Claims reserving,Individual claims features,Individual claims reserving,Mack's CL model,Micro-level reserving,Neural networks},
month = {dec},
number = {2},
pages = {407--436},
publisher = {SSRN},
shorttitle = {Neural networks applied to chain-ladder reserving},
title = {{Neural networks applied to chain–ladder reserving}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=2966126},
urldate = {2019-11-20},
volume = {8},
year = {2018}
}
@misc{Mi2020,
author = {Mi, Hong},
title = {{The Developing Countries Mortality Database (DCMD)}},
url = {http://www.lifetables.org/},
urldate = {2020-07-01},
year = {2020}
}
@inproceedings{Beard1959,
abstract = {Summary 10.1002/9780470715253.app1.abs This chapter contains section titled: * References},
author = {Beard, R. E.},
booktitle = {Ciba Foundation Symposium-The Lifespan of Animals (Colloquia on Ageing), Volume 5},
doi = {10.1002/9780470715253.app1},
isbn = {0470715251},
pages = {302--311},
publisher = {Wiley Online Library},
title = {{Appendix: Note on Some Mathematical Mortality Models}},
year = {2008}
}
@article{Lee1983,
author = {Lee, R and Lam, D},
isbn = {0032-4728},
journal = {Population studies},
number = {3},
pages = {445--464},
title = {{Age distribution adjustments for English censuses, 1821 to 1931}},
volume = {37},
year = {1983}
}
@article{Balkema1974,
abstract = {The asymptotic behaviour of the residual life time at time t is investigated (for t rightarrow infty). We derive weak limit laws and their domains of attraction and treat rates of convergence and moment convergence. The presentation exploits the close similarity with extreme value theory.},
author = {Balkema, a a and {De Haan}, L},
file = {:C$\backslash$:/R/refas/csiszar1975divergence.pdf:pdf},
issn = {0091-1798},
journal = {Statistics},
number = {5},
pages = {347--370},
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Probability. {\textregistered} www.jstor.org}},
url = {http://projecteuclid.org/euclid.aop/1176996548},
volume = {2},
year = {1974}
}
@article{Hainaut2018a,
author = {Hainaut, D},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hainaut-2018-A self-organizing predictive map.pdf:pdf},
journal = {SSRN},
number = {29 June},
title = {{A self-organizing predictive map for non-life insurance}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=3099979},
volume = {2018},
year = {2018}
}
@misc{Wuthrich2017a,
author = {W{\"{u}}thrich, M V and Buser, C},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wuthrich, Buser - 2017 - Data analytics for non-life insurance pricing.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wuthrich, Buser - 2017 - Data analytics for non-life insurance pricing(2).pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wuthrich, Buser - 2017 - Data analytics for non-life insurance pricing(3).pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Wuthrich-2017-Data analytics for non-life insu.pdf:pdf},
number = {17 June},
publisher = {Swiss Finance Institute Research Paper},
title = {{Data analytics for non-life insurance pricing}},
url = {https://ssrn.com/abstract=2870308},
volume = {2018},
year = {2017}
}
@article{Struzzieri1998,
abstract = {Currently, many actuaries produce a range of reasonable reserve estimates for IBNR loss and loss adjustment expense. NAIC Issue Paper No. 55 would effectively eliminate the possibility of booking any amount except management's "best estimate" within this range. The term "best estimate" has not been defined, neither in the issue paper nor in the actuarial literature. We propose to define the best estimate by describing a set of "best practices"-many already found in the actuarial literature-that a reserving actuary should follow, while minimizing the number of arbitrary judgments. Central to this paper is the recently introduced Generalized Cape Cod method. Many of these best practices have been shown to lead to minimum bias results. The best estimate, therefore, will be the outcome of following the framework contained in this paper.},
author = {Struzzieri, Paul J and Hussian, Paul R and Plaza, T P},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Struzzieri, Hussian - Unknown - Using Best Practices to Determine a Best Reserve Estimate.pdf:pdf},
journal = {CAS Forum},
keywords = {Reserving IBNR},
number = {212},
pages = {353--413},
title = {{Using best practices to determine a best reserve estimate}},
url = {http://www.casact.org/pubs/forum/98fforum/struhuss.pdf{\%}5Cnpapers2://publication/uuid/240855E4-35A3-4EB0-BBEA-DFB00A12D2A7},
volume = {10121},
year = {1998}
}
@article{Gao2018,
abstract = {We investigate the predictive power of covariates extracted from telematics car driving data using the speed-acceleration heatmaps of Gao, G. {\&} W{\"{u}}thrich, M. V. [(2017). Feature extraction from telematics car driving heatmaps. SSRN ID: 3070069]. These telematics covariates include K-means classification, principal components, and bottleneck activations from a bottleneck neural network. In the conducted case study it turns out that the first principal component and the bottleneck activations give a better out-of-sample prediction for claims frequencies than other traditional pricing factors such as driver's age. Based on these numerical examples we recommend the use of these telematics covariates for car insurance pricing.},
author = {Gao, Guangyuan and Meng, Shengwang and W{\"{u}}thrich, Mario V.},
doi = {10.1080/03461238.2018.1523068},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Gao-2018-Claims Frequency Modeling Using Telem.pdf:pdf},
issn = {16512030},
journal = {Scandinavian Actuarial Journal},
keywords = {K-means algorithm,Kullback-Leibler divergence,Telematics data,autoencoder,bottleneck neural network,car insurance pricing,claims frequency modeling,generalized additive model,pattern recognition,principal components analysis,v-a heatmap},
number = {2},
pages = {143--162},
shorttitle = {Claims Frequency Modeling Using Telematics Car Dri},
title = {{Claims frequency modeling using telematics car driving data}},
volume = {2019},
year = {2019}
}
@article{Noll2018a,
abstract = {We provide a tutorial that compares a classical generalized linear model for claims frequency modeling to regression tree, boosting machine and neural network approaches. We explore these methods, discuss their calibration and study their predictive power on an explicit motor third-party liability insurance data set. The results of the case study show that a simple generalized linear model does not capture interactions of feature components appropriately, whereas the other methods are able to address these interactions.},
author = {Noll, Alexander and Salzmann, Robert and W{\"{u}}thrich, Mario V.},
doi = {10.2139/ssrn.3164764},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Noll-2018-Case Study{\_} French Motor Third-Party.pdf:pdf},
howpublished = {SSRN},
journal = {SSRN Electronic Journal},
number = {17 June},
publisher = {SSRN},
shorttitle = {Case Study: French Motor Third-Party Liability Cla},
title = {{Case Study: French Motor Third-Party Liability Claims}},
url = {https://ssrn.com/abstract=3164764},
volume = {2018},
year = {2018}
}
@article{freund1995boosting,
abstract = {We present an algorithm for improving the accuracy of algorithms for learning binary concepts. The improvement is achieved by combining a large number of hypotheses, each of which is generated by training the given learning algorithm on a different set of examples. Our algorithm is based on ideas presented by Schapire and represents an improvement over his results, The analysis of our algorithm provides general upper bounds on the resources required for learning in Valiant's polynomial PAC learning framework, which are the best general upper bounds known today. We show that the number of hypotheses that are combined by our algorithm is the smallest number possible. Other outcomes of our analysis are results regarding the representational power of threshold circuits, the relation between learnability and compression, and a method for parallelizing PAC learning algorithms. We provide extensions of our algorithms to cases in which the concepts are not binary and to the case where the accuracy of the learning algorithm depends on the distribution of the instances. {\textcopyright} 1995 Academic Press, Inc.},
author = {Freund, Yoav},
doi = {10.1006/inco.1995.1136},
isbn = {1558601465},
issn = {10902651},
journal = {Information and Computation},
number = {2},
pages = {256--285},
publisher = {Elsevier},
title = {{Boosting a weak learning algorithm by majority}},
volume = {121},
year = {1995}
}
@article{Committee2016,
author = {Committee, Retirement Plans Expereince},
title = {{Mortality Improvement Scale MP-2015}},
year = {2016}
}
@article{Parodi2012a,
abstract = {This paper argues that most of the problems that actuaries have to deal with in the context of non-life insurance can be usefully cast in the framework of computational intelligence (a.k.a. artificial intelligence), the discipline that studies the design of agents which exhibit intelligent behaviour. Finding an adequate framework for actuarial problems has more than a simply theoretical interest: it also allows a knowledge transfer from the computational intelligence discipline to general insurance, wherever techniques have been developed for problems which are common to both contexts. This has already happened in the past (neural networks, clustering, data mining have all found applications to general insurance) but not systematically, with the result that many useful computational intelligence techniques such as sparsity-based regularisation schemes (a technique for feature selection) are virtually unknown to actuaries.In this first of two papers, we will explore the role of statistical learning in actuarial modelling. We will show that risk costing, which is at the core of pricing, reserving and capital modelling, can be described as a supervised learning problem. Many activities involved in exploratory analysis, such as data mining or feature construction, can be described as unsupervised learning. A comparison of different computational intelligence methods will be carried out, and practical insurance applications (rating factor selection, IBNER analysis) will also be presented.},
author = {Parodi, Pietro},
doi = {10.1017/S1748499512000036},
edition = {05/18},
isbn = {1748-4995},
issn = {1748-4995},
journal = {Annals of Actuarial Science},
keywords = {Artificial intelligence,Artificial neural network,Computational intelligence,Computer science,General insurance,Intelligent agent,Machine learning},
number = {2},
pages = {307--343},
publisher = {Cambridge University Press},
title = {{Computational intelligence with applications to general insurance: a review: I – The role of statistical learning}},
url = {https://www.cambridge.org/core/article/computational-intelligence-with-applications-to-general-insurance-a-review/DD2225744F5557E0C61309999C9BCD84},
volume = {6},
year = {2012}
}
@misc{RCoreTeam2018,
address = {Vienna, Austria},
author = {{R Core Team}},
publisher = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org},
year = {2018}
}
@techreport{Gotmare,
abstract = {The convergence rate and final performance of common deep learning models have significantly benefited from heuristics such as learning rate schedules, knowledge distillation, skip connections, and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit such analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz., mode con-nectivity and canonical correlation analysis (CCA), and hypothesize reasons for the success of the heuristics. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectiv-ity and CCA. Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed to the deeper layers.},
archivePrefix = {arXiv},
arxivId = {1810.13243v1},
author = {Gotmare, Akhilesh and {Shirish Keskar}, Nitish and Xiong, Caiming and Socher, Richard},
eprint = {1810.13243v1},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gotmare et al. - Unknown - A CLOSER LOOK AT DEEP LEARNING HEURISTICS LEARNING RATE RESTARTS, WARMUP AND DISTILLA-TION.pdf:pdf},
title = {{A CLOSER LOOK AT DEEP LEARNING HEURISTICS: LEARNING RATE RESTARTS, WARMUP AND DISTILLA-TION}}
}
@article{Wilmoth2010,
abstract = {Summary: The Human Mortality Database (HMD) was created to provide detailed mortality and population data to researchers, students, journalists, policy analysts, and others interested in the history of human longevity. The project began as an outgrowth of earlier ... $\backslash$n},
author = {{Human Mortality Database}},
journal = {University of California},
title = {{Human Mortality Database}},
year = {2010}
}
@article{Ezzini2018,
author = {Ezzini, S and Berrada, I and Ghogho, M},
issn = {2196-1115},
journal = {Journal of Big Data},
number = {1},
pages = {9},
shorttitle = {Who is behind the wheel? Driver identification and},
title = {{Who is behind the wheel? Driver identification and fingerprinting}},
volume = {5},
year = {2018}
}
@article{Bhat1992,
author = {Bhat, P.N. Mari},
journal = {Current Science},
number = {8},
pages = {440--448},
title = {{Changing demography of Elderly in India}},
volume = {63},
year = {1992}
}
@book{Oliver2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Oliver, J.},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/user-pc/Desktop/Bharath Ramsundar, Peter Eastman, Patrick Walters, Vijay Pande - Deep Learning for the Life Sciences. Genomics, Microscopy, Drug Discovery-O'Reilly (2019).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Deep Learning for the Life Sciences Applying Deep Learning to Genomics, Microscopy, Drug Discovery, and More}},
volume = {53},
year = {2013}
}
@article{Kim1985,
author = {Kim, Young J},
isbn = {0032-4701},
journal = {Population index},
pages = {3--6},
title = {{A comment on Coale's life table construction}},
year = {1985}
}
@article{Weidner2016,
abstract = {Using modern telematics technologies for car insurance, it is no particular challenge to produce an intractably large amount of kinematic and contextual information about driving profiles of motor vehicles. In order to evaluate this data with respect to both efficient and effective use in scoring and subsequent actuarial pricing, we propose a scale-sensitive approach that treats observations on semantically different levels. Furthermore we discuss the application of methods necessary to assess the information of different scale levels including signal processing, pattern recognition and Fourier analysis. In this way we show how maneuvers, trips and trip sections as well as the total insurance period can be analyzed to individually or collectively gain significantly scoreable insights into individual driving behaviour.},
author = {Weidner, W. and Transchel, F. W.G. and Weidner, R.},
doi = {10.1007/s13385-016-0127-x},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Weidner-2016-Classification of scale-sensitive.pdf:pdf},
isbn = {2190-9741},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Actuarial pricing,Car insurance,Driving behaviour,Pattern recognition,Telematics technologies,Weidner2016},
mendeley-tags = {Weidner2016},
month = {jul},
number = {1},
pages = {3--24},
shorttitle = {Classification of scale-sensitive telematic observ},
title = {{Classification of scale-sensitive telematic observables for riskindividual pricing}},
url = {https://doi.org/10.1007/s13385-016-0127-x},
volume = {6},
year = {2016}
}
@misc{SouthAfricanReserve2020,
author = {{South African Reserve Bank}},
booktitle = {SARB Website},
month = {mar},
title = {{Further amendments to the money market liquidity management strategy of the SARB}},
url = {https://www.resbank.co.za/Lists/News and Publications/Attachments/9805/Further amendments to the money market liquidity management strategy of the SARB.pdf},
urldate = {2020-04-02},
year = {2020}
}
@inproceedings{Goodfellow2014,
abstract = {In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.},
archivePrefix = {arXiv},
arxivId = {1612.04357},
author = {Huang, Xun and Li, Yixuan and Poursaeed, Omid and Hopcroft, John and Belongie, Serge},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.202},
eprint = {1612.04357},
isbn = {9781538604571},
issn = {1945788X},
pages = {1866--1875},
title = {{Stacked generative adversarial networks}},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Breeden2019,
author = {Breeden, Joseph L. and Leonova, Eugenia},
doi = {10.2991/smont-19.2019.51},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breeden, Leonova - 2019 - When Big Data Isn't Enough Solving the long-range forecasting problem in supervised learning.pdf:pdf},
month = {may},
publisher = {Atlantis Press},
title = {{When Big Data Isn't Enough: Solving the long-range forecasting problem in supervised learning}},
year = {2019}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning. {\textcopyright} 1979-2012 IEEE.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
number = {8},
pages = {1798--1828},
pmid = {23787338},
title = {{Representation learning: A review and new perspectives}},
volume = {35},
year = {2013}
}
@article{das1990reconstruction,
author = {{Das Gupta}, P},
journal = {Washington, DC},
title = {{Reconstruction of the Age Distribution of the Extreme Aged in the 1980 Census by the Method of Extinct Generations}},
volume = {20233},
year = {1990}
}
@article{croforum_mach_dec,
author = {Forum, C R O},
title = {{Machine Decisions: Governance of AI and Big Data Analytics}},
year = {2019}
}
@article{Gao2017a,
abstract = {Insurance companies have started to collect high-frequency GPS car driving data to analyze the driving styles of their policyholders. In previous work, we have introduced speed and acceleration heatmaps. These heatmaps were categorized with the K-means algorithm to differentiate varying driving styles. In many situations it is useful to have low-dimensional continuous representations instead of unordered categories. In the present work we use singular value decomposition and bottleneck neural networks (autoencoders) for principal component analysis. We show that a two-dimensional representation is sufficient to re-construct the heatmaps with high accuracy (measured by Kullback–Leibler divergences).},
author = {Gao, Guangyuan and W{\"{u}}thrich, Mario V.},
doi = {10.1007/s13385-018-0181-7},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Gao-2017-Feature Extraction from Telematics Ca.pdf:pdf},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Autoencoder,Bottleneck neural network,Driving styles,Image recognition,K-means algorithm,Kullback–Leibler divergence,Pattern recognition,Principal component analysis,Singular value decomposition,Telematics car driving data,Unsupervised learning},
number = {2},
pages = {383--406},
shorttitle = {Feature Extraction from Telematics Car Driving Hea},
title = {{Feature extraction from telematics car driving heatmaps}},
volume = {8},
year = {2018}
}
@misc{Khosravi2011,
abstract = {This paper evaluates the four leading techniques proposed in the literature for construction of prediction intervals (PIs) for neural network point forecasts. The delta, Bayesian, bootstrap, and mean-variance estimation (MVE) methods are reviewed and their performance for generating high-quality PIs is compared. PI-based measures are proposed and applied for the objective and quantitative assessment of each method's performance. A selection of 12 synthetic and real-world case studies is used to examine each method's performance for PI construction. The comparison is performed on the basis of the quality of generated PIs, the repeatability of the results, the computational requirements and the PIs variability with regard to the data uncertainty. The obtained results in this paper indicate that: 1) the delta and Bayesian methods are the best in terms of quality and repeatability, and 2) the MVE and bootstrap methods are the best in terms of low computational load and the width variability of PIs. This paper also introduces the concept of combinations of PIs, and proposes a new method for generating combined PIs using the traditional PIs. Genetic algorithm is applied for adjusting the combiner parameters through minimization of a PI-based cost function subject to two sets of restrictions. It is shown that the quality of PIs produced by the combiners is dramatically better than the quality of PIs obtained from each individual method. {\textcopyright} 2006 IEEE.},
author = {Khosravi, Abbas and Nahavandi, Saeid and Creighton, Doug and Atiya, Amir F.},
booktitle = {IEEE Transactions on Neural Networks},
doi = {10.1109/TNN.2011.2162110},
issn = {10459227},
keywords = {Bayesian,bootstrap,delta,mean-variance estimation,neural network,prediction interval},
month = {sep},
number = {9},
pages = {1341--1356},
title = {{Comprehensive review of neural network-based prediction intervals and new advances}},
volume = {22},
year = {2011}
}
@article{Castellani2012,
abstract = {Stochastic claims reserving has been developed mostly using models defined in the framework of the classical statistics. The recently proposed Time Series Chain Ladder (TSCL) is one of these models. In order to allow for a comparison with the Bayesian point of view, we propose a fully Bayesian model having the property of reproducing TSCL if improper priors are assumed. With "informative" priors the Bayesian model allows for incorporating into the reserving process relevant external data, e.g. expert opinions, which are largely used by the actuaries. We provide numerical examples using Markov Chain Monte Carlo methods. {\textcopyright} 2012 Springer-Verlag.},
author = {Castellani, Gilberto and {De Felice}, Massimo and Moriconi, Franco},
doi = {10.1007/978-3-642-31724-8_15},
isbn = {9783642317231},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Bayesian modelling,Chain Ladder,Claims reserving,Markov Chain Monte Carlo,Reserve Risk,Solvency II},
number = {PART 4},
pages = {134--145},
title = {{Claims reserving in non-life insurance: A fully Bayesian model}},
volume = {300 CCIS},
year = {2012}
}
@article{Rosenwaike1968,
author = {Rosenwaike, Ira},
isbn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {321},
pages = {29--40},
title = {{On measuring the extreme aged in the population}},
volume = {63},
year = {1968}
}
@book{Demeny1968,
author = {Demeny, Paul George and Shorter, Frederic Claiborne},
publisher = {publisher not identified},
title = {{Estimating Turkish Mortality, Fertility, and Age Structure: Application of Some New Techniques}},
year = {1968}
}
@book{Shisana2002,
author = {Shisana, Olive and Simbayi, Leickness Chisamu},
isbn = {0796920079},
publisher = {HSRC Press},
title = {{Nelson Mandela/HSRC study of HIV/AIDS: South African national HIV prevalence, behavioural risks and mass media: household survey 2002}},
year = {2002}
}
@article{Coale1986,
author = {Coale, Ansley J and Kisker, Ellen Eliason},
isbn = {0032-4728},
journal = {Population studies},
number = {3},
pages = {389--401},
title = {{Mortality crossovers: Reality or bad data?}},
volume = {40},
year = {1986}
}
@misc{Lenail2018,
author = {Lenail, A},
shorttitle = {NN-SVG},
title = {{NN-SVG}},
url = {https://github.com/zfrenchee/NN-SVG},
year = {2018}
}
@article{Hill2000b,
author = {Hill, Kenneth},
journal = {The Global Burden of Disease in Aging Populations–Research Paper},
number = {01.13},
title = {{Methods for measuring adult mortality in developing countries: a comparative review}},
year = {2000}
}
@article{Manton1995,
author = {Manton, Kenneth G and Vaupel, James W},
doi = {doi:10.1056/NEJM199511023331824},
journal = {New England Journal of Medicine},
number = {18},
pages = {1232--1235},
pmid = {7565998},
title = {{Survival after the Age of 80 in the United States, Sweden, France, England, and Japan}},
url = {http://www.nejm.org/doi/full/10.1056/NEJM199511023331824},
volume = {333},
year = {1995}
}
@article{Preston1983,
author = {Preston, Samuel H},
isbn = {0070-3370},
journal = {Demography},
number = {2},
pages = {213--226},
title = {{An integrated system for demographic estimation from two age distributions}},
volume = {20},
year = {1983}
}
@incollection{Yi2006,
author = {Yi, Zeng and Vaupel, James W},
booktitle = {Human Longevity, Individual Life Duration, and the Growth of the Oldest-Old Population},
isbn = {1402048467},
pages = {87--110},
publisher = {Springer},
title = {{Oldest-old mortality in China}},
year = {2006}
}
@article{Gavrilov2011,
author = {Gavrilov, Leonid A and Gavrilova, Natalia S},
isbn = {1092-0277},
journal = {North American actuarial journal},
number = {3},
pages = {432--447},
title = {{Mortality measurement at advanced ages: a study of the Social Security Administration Death Master File}},
volume = {15},
year = {2011}
}
@article{Preston1980,
author = {Preston, Samuel and Coale, Ansley J and Trussell, James and Weinstein, Maxine},
isbn = {0032-4701},
journal = {Population Index},
pages = {179--202},
title = {{Estimating the completeness of reporting of adult deaths in populations that are approximately stable}},
year = {1980}
}
@incollection{Tomas2014,
author = {Tomas, Julien and Planchet, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Computational Actuarial Science with R},
doi = {doi:10.1201/b17230-12},
editor = {Charpentier, A},
isbn = {978-1-4665-9259-9},
pages = {345--382},
publisher = {CRC Press},
title = {{Prospective Mortality Tables and Portfolio Experience}},
url = {http://dx.doi.org/10.1201/b17230-12},
year = {2014}
}
@article{Kreif2019,
abstract = {While machine learning (ML) methods have received a lot of attention in recent years, these methods are primarily for prediction. Empirical researchers conducting policy evaluations are, on the other hand, pre-occupied with causal problems, trying to answer counterfactual questions: what would have happened in the absence of a policy? Because these counterfactuals can never be directly observed (described as the "fundamental problem of causal inference") prediction tools from the ML literature cannot be readily used for causal inference. In the last decade, major innovations have taken place incorporating supervised ML tools into estimators for causal parameters such as the average treatment effect (ATE). This holds the promise of attenuating model misspecification issues, and increasing of transparency in model selection. One particularly mature strand of the literature include approaches that incorporate supervised ML approaches in the estimation of the ATE of a binary treatment, under the $\backslash$textit{\{}unconfoundedness{\}} and positivity assumptions (also known as exchangeability and overlap assumptions). This article reviews popular supervised machine learning algorithms, including the Super Learner. Then, some specific uses of machine learning for treatment effect estimation are introduced and illustrated, namely (1) to create balance among treated and control groups, (2) to estimate so-called nuisance models (e.g. the propensity score, or conditional expectations of the outcome) in semi-parametric estimators that target causal parameters (e.g. targeted maximum likelihood estimation or the double ML estimator), and (3) the use of machine learning for variable selection in situations with a high number of covariates.},
archivePrefix = {arXiv},
arxivId = {1903.00402},
author = {Kreif, No{\'{e}}mi and DiazOrdaz, Karla and Kreif, No{\'{e}}mi and DiazOrdaz, Karla},
doi = {10.1093/acrefore/9780190625979.013.256},
eprint = {1903.00402},
file = {:C$\backslash$:/R/refas/1903.00402.pdf:pdf},
journal = {Oxford Research Encyclopedia of Economics and Finance},
pages = {1--55},
title = {{Machine Learning in Policy Evaluation: New Tools for Causal Inference}},
year = {2019}
}
@incollection{Graves2012,
abstract = {Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to lo- calised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. Long short-term memory is an especially promising recurrent archi- tecture, able to bridge long time delays between relevant input and output events, and thereby access long range context. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks in general, and long short-term memory in particular. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the align- ment between the inputs and the labels is unknown, and (2) an extension of long short-term memory to multidimensional data, such as images and video sequences. Experimental results are presented on speech recognition, online and offline handwriting recognition, keyword spotting, image segmen- tation and image classification, demonstrating the advantages of advanced recurrent networks over other sequential algorithms, such as hidden Markov Models. ii},
address = {Berlin, Heidelberg},
author = {Graves, Alex},
booktitle = {Supervised sequence labelling with recurrent neural networks},
doi = {10.1007/978-3-642-24797-2_2},
pages = {5--13},
publisher = {Springer},
title = {{Supervised Sequence Labelling}},
year = {2012}
}
@article{Hinton2012,
abstract = {Twenty-six patients with primary lung cancer were studied in whose blood a significant increase in the levels of copper, ceruloplasmin, lactic-dehydrogenase and ??2-globulins was found. The role of copper in pulmonary cancerogenesis is discussed. It seems that the increase of the lactic-dehydrogenase in the tumor tissues and consequently in the blood serum of the neoplasic patients (11) would not be so much due to a process of tissue necrosis with the release of enzyme into the circulation but rather an alteration of the neoplasic cell in - cell i their respiratory system because of the fact that the lactic-dehydrogenase ceruloplasmin and ??2-globulin are significantly raised (p{\textless}0.001), the authors suspect the existence of a certain cellular necrosis. On the other hand, there is a linear correlation between these values and the plasma copper with a linear regression coefficient 0.96 which can have some importance in the etiology of initial lung cancer since it correlates a demonstrated cancerogenous agent, as in copper, and some analytical considerations of a neoplastic process, confirmed anatomico-pathologically.},
author = {{Martin Mateo}, M. C. and {Bustamante Bustamante}, J. and {Font Arellano}, I.},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hinton-2012-Improving neural networks by preve.pdf:pdf},
issn = {09702067},
journal = {Biomedicine},
number = {3},
pages = {66--68},
shorttitle = {Improving neural networks by preventing co-adaptat},
title = {{Serum copper, ceruloplasmin, lactic-dehydrogenase and ??2-globulin in lung cancer}},
volume = {31},
year = {1979}
}
@article{Dechter1990,
author = {Dechter, Aim{\'{e}}e R and Preston, Samuel H},
isbn = {0251-7604},
journal = {Population Bulletin of the United Nations},
number = {31-32},
pages = {1--16},
title = {{Age misreporting and its effects on adult mortality estimates in Latin America}},
year = {1990}
}
@article{Bacciu2019,
abstract = {The adaptive processing of graph data is a long-standing research topic which has been lately consolidated as a theme of major interest in the deep learning community. The snap increase in the amount and breadth of related research has come at the price of little systematization of knowledge and attention to earlier literature. This work is designed as a tutorial introduction to the field of deep learning for graphs. It favours a consistent and progressive introduction of the main concepts and architectural aspects over an exposition of the most recent literature, for which the reader is referred to available surveys. The paper takes a top-down view to the problem, introducing a generalized formulation of graph representation learning based on a local and iterative approach to structured information processing. It introduces the basic building blocks that can be combined to design novel and effective neural models for graphs. The methodological exposition is complemented by a discussion of interesting research challenges and applications in the field.},
archivePrefix = {arXiv},
arxivId = {1912.12693},
author = {Bacciu, Davide and Errica, Federico and Micheli, Alessio and Podda, Marco},
eprint = {1912.12693},
file = {:C$\backslash$:/Users/user-pc/Desktop/1912.12693.pdf:pdf},
keywords = {deep learning for graphs,graph neural networks,learning for,structured data,tutorial},
pages = {1--55},
title = {{A Gentle Introduction to Deep Learning for Graphs}},
url = {http://arxiv.org/abs/1912.12693},
year = {2019}
}
@incollection{Timæus2007,
author = {Tim{\ae}us, Ian M},
booktitle = {HIV, resurgent infections and population change in Africa},
isbn = {1402061722},
pages = {229--243},
publisher = {Springer},
title = {{Impact of HIV on mortality in Southern Africa: Evidence from demographic surveillance}},
year = {2007}
}
@inproceedings{lundberg2017unified,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
archivePrefix = {arXiv},
arxivId = {1705.07874},
author = {Lundberg, Scott M. and Lee, Su In},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1705.07874},
issn = {10495258},
pages = {4766--4775},
title = {{A unified approach to interpreting model predictions}},
volume = {2017-Decem},
year = {2017}
}
@article{Gomes2009,
author = {Gomes, Marilia Miranda Fortes and Turra, Cassio M},
journal = {Demographic Research},
number = {20},
pages = {495--502},
title = {{The number of centenarians in Brazil: indirect estimates based on death certificates}},
volume = {20},
year = {2009}
}
@misc{Haberman1988,
address = {Staple Inn Actuarial Society},
author = {Haberman, S and Renshaw, A E},
booktitle = {Sessional Meeting},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Haberman-1988-Generalised Lienar Models in Act.pdf:pdf},
shorttitle = {Generalised Lienar Models in Actuarial Work},
title = {{Generalised Lienar Models in Actuarial Work}},
year = {1988}
}
@article{Bennett1983,
abstract = {Are claims of extraordinarily low mortality levels in the USSR justifiable? Applying a recently developed methodology appropriate for nonstable populations to 1959 and 1970 census data from the Soviet Union, we find that mortality is generally understated for the country as a whole and for various regions and republics. This is particularly so for the republics composing the Central Asian region and the Caucasus. Age overstatement appears to be extremely pronounced in the oldest segments of the population. Using the new methodology, we can derive the age distribution that is uniquely implied by a given life table and a set of age-specific rates of growth obtained from two censuses. When we use the official Soviet life tables in this procedure, we find that the reported number of centenarians is at least 28.9 percent overstated for males and 7.5 percent for females. If one were to posit that Soviet mortality during 1959 to 1970 was, in fact, no better than the Swedish mortality experience during roughly the same time period, then the true number of centenarians could be no more than 2 percent of that reported. {\textcopyright} 1983 Population Association of America.},
author = {Bennett, Neil G. and Garson, Lea Keil},
doi = {10.2307/2061121},
isbn = {0070-3370},
issn = {00703370},
journal = {Demography},
number = {4},
pages = {587--606},
title = {{The centenarian question and old-age mortality in the Soviet Union, 1959-1970}},
volume = {20},
year = {1983}
}
@article{Medvedev1974,
author = {Medvedev, Zhores A},
isbn = {0016-9013},
journal = {The Gerontologist},
number = {5 Part 1},
pages = {381--387},
title = {{Caucasus and Altay longevity: A biological or social problem?}},
volume = {14},
year = {1974}
}
@article{Gu2013,
author = {Gu, Danan and Gerland, Patrick and Andreev, Kirill and Li, Nan and Spoorenberg, Thomas and Heilig, Gerhard},
journal = {Demographic Research},
number = {38},
pages = {999--1038},
title = {{Old age mortality in Eastern and South-Eastern Asia}},
volume = {29},
year = {2013}
}
@article{Holland1986,
abstract = {Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling. {\textcopyright} 1976 Taylor {\&} Francis Group, LLC.},
author = {Holland, Paul W.},
doi = {10.1080/01621459.1986.10478354},
file = {:C$\backslash$:/R/refas/EITM12.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Association,Causal effect,Causal model,Experiments,Granger causality,Hill's nine factors,Koch's postulates,Mill's methods,Path diagrams,Philosophy,Probabilistic causality},
number = {396},
pages = {945--960},
title = {{Statistics and causal inference}},
volume = {81},
year = {1986}
}
@techreport{Borovykh2018,
abstract = {We present a method for conditional time series forecasting based on an adaptation of the recent deep convolutional WaveNet architecture. The proposed network contains stacks of dilated convolutions that allow it to access a broad range of history when forecasting, a ReLU activation function and conditioning is performed by applying multiple convolutional filters in parallel to separate time series which allows for the fast processing of data and the exploitation of the correlation structure between the multivariate time series. We test and analyze the performance of the convolutional network both unconditionally as well as conditionally for financial time series forecasting using the S{\&}P500, the volatility index, the CBOE interest rate and several exchange rates and extensively compare it to the performance of the well-known autoregressive model and a long-short term memory network. We show that a convolutional network is well-suited for regression-type problems and is able to effectively learn dependencies in and between the series without the need for long historical time series, is a time-efficient and easy to implement alternative to recurrent-type networks and tends to outperform linear and recurrent models.},
archivePrefix = {arXiv},
arxivId = {1703.04691v5},
author = {Borovykh, Anastasia and Bohte, Sander and Oosterlee, Cornelis W},
eprint = {1703.04691v5},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Borovykh, Bohte, Oosterlee - 2018 - Conditional time series forecasting with convolutional neural networks.pdf:pdf},
keywords = {Convolutional neural network,deep learning,financial time series,forecasting,multivariate time series},
title = {{Conditional time series forecasting with convolutional neural networks}},
year = {2018}
}
@article{Randall2016,
author = {Randall, Sara and Coast, Ernestina},
isbn = {1435-9871},
journal = {Demographic Research},
pages = {143},
title = {{The quality of demographic data on older Africans}},
volume = {34},
year = {2016}
}
@book{Pade2018,
abstract = {This book provides an introduction into the fundamentals of non-relativistic quantum mechanics. In Part 1, the essential principles are developed. Applications and extensions of the formalism can be found in Part 2. The book includes not only material that is presented in traditional textbooks on quantum mechanics, but also discusses in detail current issues such as interaction-free quantum measurements, neutrino oscillations, various topics in the field of quantum information as well as fundamental problems and epistemological questions, such as the measurement problem, entanglement, Bell's inequality, decoherence, and the realism debate. A chapter on current interpretations of quantum mechanics concludes the book. To develop quickly and clearly the main principles of quantum mechanics and its mathematical formulation, there is a systematic change between wave mechanics and algebraic representation in the first chapters. The required mathematical tools are introduced step by step. Moreover, the appendix collects compactly the most important mathematical tools that supplementary literature can be largely dispensed. In addition, the appendix contains advanced topics, such as Quantum- Zeno effect, time-delay experiments, Lenz vector and the Shor algorithm.},
author = {Pade, Jochen},
doi = {10.1007/978-3-030-00467-5},
file = {:C$\backslash$:/Users/user-pc/Desktop/2018{\_}Book{\_}QuantumMechanicsForPedestrians (1).pdf:pdf},
isbn = {978-3-030-00466-8},
pages = {452},
title = {{Quantum Mechanics for Pedestrians 2}},
url = {http://link.springer.com/10.1007/978-3-319-00798-4{\%}0Ahttp://link.springer.com/10.1007/978-3-030-00467-5},
year = {2018}
}
@article{Myers1954,
author = {Myers, Robert J},
isbn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {268},
pages = {826--831},
title = {{Accuracy of age reporting in the 1950 United States census}},
volume = {49},
year = {1954}
}
@article{mack2000comparison,
abstract = {It is shown that the (over-dispersed) Poisson model is not the same as the distribution-free chain ladder model of Mack (Distribution-free calculation of the standard error of chain ladder reserve estimates, ASTIN Bulletin 23 (1993) 213-225) although both reproduce the historical chain ladder estimator for the claims reserve. For example, the true expected claims reserves, ignoring estimation issues, described by the two models are different. Moreover, the Poisson model deviates from the historical chain ladder algorithm in several aspects that the distribution-free chain ladder model does not. Therefore, only the latter can qualify to be referred to as the model underlying the chain ladder algorithm. {\textcopyright} 2000 Elsevier Science B.V.},
author = {Mack, Thomas and Venter, Gary},
doi = {10.1016/S0167-6687(99)00039-6},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Chain ladder,Distribution-free model,Over-dispersed Poisson model},
number = {1},
pages = {101--107},
publisher = {Elsevier},
title = {{A comparison of stochastic models that reproduce chain ladder reserve estimates}},
volume = {26},
year = {2000}
}
@article{AHearn2009,
abstract = {Age data frequently display excess frequencies at attractive numbers, such as multiples of five. We use this "age heaping" to measure cognitive ability in quantitative reasoning, or "numeracy." We construct a database of age heaping estimates with exceptional geographic and temporal coverage, and demonstrate a robust correlation of literacy and numeracy, where both can be observed. Extending the temporal and geographic range of our knowledge of human capital, we show that Western Europe had already diverged from the east and reached high numeracy levels by 1600, long before the rise of mass schooling or the onset of industrialization. {\textcopyright} 2009 The Economic History Association.},
author = {A'Hearn, Brian and Baten, J{\"{o}}rg and Crayen, Dorothee},
doi = {10.1017/S0022050709001120},
isbn = {1471-6372},
issn = {00220507},
journal = {Journal of Economic History},
number = {3},
pages = {783--808},
title = {{Quantifying quantitative literacy: Age heaping and the history of human capital}},
volume = {69},
year = {2009}
}
@article{Timaeus1991,
author = {Timaeus, Ian M},
isbn = {0032-4701},
journal = {Population Index},
pages = {552--568},
title = {{Measurement of adult mortality in less developed countries: a comparative review}},
year = {1991}
}
@article{Richards2008,
author = {Richards, S J},
isbn = {1467-985X},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
number = {1},
pages = {279--298},
title = {{Detecting year‐of‐birth mortality patterns with limited data}},
volume = {171},
year = {2008}
}
@article{Rosenberg1990,
author = {Rosenberg, Philip S},
isbn = {1525-4135},
journal = {JAIDS Journal of Acquired Immune Deficiency Syndromes},
number = {1},
pages = {49--54},
title = {{A simple correction of AIDS surveillance data for reporting delays}},
volume = {3},
year = {1990}
}
@phdthesis{Zarkadoulas2017,
address = {Lausanne},
author = {Zarkadoulas, A},
booktitle = {Faculty of HEC},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Zarkadoulas-2017-Neural network algorithms for.pdf:pdf},
publisher = {University of Lausanne},
school = {University of Lausanne},
shorttitle = {Neural network algorithms for the development of i},
title = {{Neural network algorithms for the development of individual losses}},
year = {2017}
}
@article{Brass1975,
abstract = {The major influence of population trends on social and economic development is now widely recognized. Hence, estimation of fertility and mortality rates has taken on increasing importance in recent years. Unfortunately, the data available for making such estimates are often fragmentary and subject to biases of unknown magnitude.The present monograph, which is based on a transcript of a seminar given by William Brass at the CELADE subcenter in San Jose, Costa Rica, reviews a variety of techniques for estimating levels of fertility and mortality from incomplete or defective data. The first section lists and discusses the basic principles underlying the adjustment of demographic data. Next, methods of estimating fertility are described. Those methods include adjustment of current fertility rates by use of data on children even born; an extension of this technique uses parity specific rates and parity distributions. Methods of analyzing maternity histories and of estimating fertility from incomplete registration data using parity information are also described.Estimates of age-specific mortality rates are developed from information on proportions of surviving children by age of mother and proportions of persons by age with parents surviving. Use of incomplete counts of deaths, such as might be available from civil registers, is also discussed.Use of the logit transformation in the evaluation and adjustment of data and in the construction of the life tables is described in detail. Techniques for estimating demographic rates through use of quasi-stable populations are also included.Emphasis is placed not only on the theoretical development of each method presented, but also on solutions to practical problems that frequently arise. The assumptions underlying each method and, in particular, the effect of departure from these assumptions is discussed.The final section of this monograph give examples of the application of selected techniques to data from various Latin American countries; it also reviews results obtained as well as appraises current approaches to the collection of demographic data.},
author = {Brass, William},
isbn = {0902657143},
journal = {An Occasional publication of the Centre for Population Studies},
pages = {i--159},
title = {{Methods for Estimating Fertility and Mortality from Limited and Defective Data}},
year = {1975}
}
@techreport{StatsSA2011,
author = {{Stats SA}},
booktitle = {Pretoria: Government Printer},
title = {{Social profile of vulnerable groups in South Africa, 2002–2010}},
year = {2011}
}
@article{Bourbeau2000a,
author = {Bourbeau, Robert and Lebel, Andr{\'{e}}},
doi = {10.4054/demres.2000.2.2},
isbn = {1435-9871},
journal = {Demographic Research},
title = {{Mortality statistics for the oldest-old}},
volume = {2},
year = {2000}
}
@techreport{Fontoura2019,
abstract = {Asset-Liability Management (ALM) is a technique used to optimize investment portfolios, considering a future flow of liabilities. Its stochastic nature and multi-period decision structure favors its modeling as a Markov Decision Process (MDP). Reinforcement Learning is a state-of-the-art group of algorithms for MDP solving, and with its recent performance boost provided by deep neural networks, problems with long time horizons can be handled in just a few hours. In this paper, we address an ALM problem with a variation of Deep Deterministic Policy Gradient algorithm. Opposed to all other approaches in the literature, our model does not use scenario discretization, which is a major contribution to ALM study. Our experimental results show that the Reinforcement Learning framework is well fitted to solve this kind of problem, and has the additional benefit of using continuous state spaces.},
author = {Fontoura, Alan and Haddad, Diego and Bezerra, Eduardo and Haddad, Diego Barreto},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fontoura et al. - 2019 - A Deep Reinforcement Learning Approach to Asset-Liability Management.pdf:pdf},
keywords = {Index Terms-ALM,deep determinis-tic policy gradient,reinforcement learning},
title = {{A Deep Reinforcement Learning Approach to Asset-Liability Management}},
url = {https://www.researchgate.net/publication/334761255},
year = {2019}
}
@article{Maddox2019,
abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of computer vision tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, and temperature scaling.},
archivePrefix = {arXiv},
arxivId = {1902.02476},
author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
eprint = {1902.02476},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maddox et al. - 2019 - A Simple Baseline for Bayesian Uncertainty in Deep Learning.pdf:pdf},
title = {{A Simple Baseline for Bayesian Uncertainty in Deep Learning}},
url = {http://arxiv.org/abs/1902.02476},
year = {2019}
}
@article{Currie2004,
author = {Currie, Iain D and Durban, Maria and Eilers, Paul H C},
isbn = {1471-082X},
journal = {Statistical modelling},
number = {4},
pages = {279--298},
title = {{Smoothing and forecasting mortality rates}},
volume = {4},
year = {2004}
}
@misc{StatsSA2015,
author = {{Stats SA}},
publisher = {SSA Pretoria},
title = {{Mid-year population estimates}},
year = {2015}
}
@book{StatsSA2006,
author = {{Stats SA}},
publisher = {Statistics South Africa},
title = {{Mortality and Causes of Death in South Africa, 2003 and 2004: Findings from Death Notification}},
year = {2006}
}
@article{Renshaw1998,
abstract = {This paper presents a statistical model underlying the chain-ladder technique. This is related to other statistical approaches to the chain-ladder technique which have been presented previously. The statistical model is cast in the form of a generalised linear model, and a quasi-likelihood approach is used. It is shown that this enables the method to process negative incremental claims. It is suggested that the chain-ladder technique represents a very narrow view of the possible range of models.},
author = {Renshaw, A.E. and Verrall, R.J.},
doi = {10.1017/s1357321700000222},
isbn = {2044-0456},
issn = {1357-3217},
journal = {British Actuarial Journal},
number = {4},
pages = {903--923},
title = {{A Stochastic Model Underlying the Chain-Ladder Technique}},
volume = {4},
year = {1998}
}
@inbook{inbook,
author = {B{\"{u}}hlmann, Hans and Lengwiler, Martin},
booktitle = {Managing risk in reinsurance: from city fires to global warming},
chapter = {5},
editor = {Haueter, NV and Jones, G},
pages = {118--144},
title = {{Calculating the unpredictable: History of actuarial theories and practices in reinsurance}},
year = {2016}
}
@article{Chen2019,
abstract = {We present a probabilistic forecasting framework based on convolutional neural network for multiple related time series forecasting. The framework can be applied to estimate probability density under both parametric and non-parametric settings. More specifically, stacked residual blocks based on dilated causal convolutional nets are constructed to capture the temporal dependencies of the series. Combined with representation learning, our approach is able to learn complex patterns such as seasonality, holiday effects within and across series, and to leverage those patterns for more accurate forecasts, especially when historical data is sparse or unavailable. Extensive empirical studies are performed on several real-world datasets, including datasets from JD.com, China's largest online retailer. The results show that our framework outperforms other state-of-the-art methods in both accuracy and efficiency.},
archivePrefix = {arXiv},
arxivId = {1906.04397},
author = {Chen, Yitian and Kang, Yanfei and Chen, Yixiong and Wang, Zizhuo},
eprint = {1906.04397},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2019 - Probabilistic Forecasting with Temporal Convolutional Neural Network.pdf:pdf},
keywords = {dilated causal convolution,neural network,probabilistic forecasting},
title = {{Probabilistic Forecasting with Temporal Convolutional Neural Network}},
url = {http://arxiv.org/abs/1906.04397},
year = {2019}
}
@article{schelldorfer2019nesting,
author = {Schelldorfer, J{\"{u}}rg and W{\"{u}}thrich, Mario V.},
doi = {10.2139/ssrn.3320525},
journal = {SSRN Electronic Journal},
title = {{Nesting Classical Actuarial Models into Neural Networks}},
year = {2019}
}
@article{Pullum1991,
author = {Pullum, Thomas W},
isbn = {1097-0258},
journal = {Statistics in medicine},
number = {2},
pages = {191--200},
title = {{Statistical methods to adjust for date and age misreporting to improve estimates of vital rates in Pakistan}},
volume = {10},
year = {1991}
}
@inproceedings{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {1706.03762v5},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.03762v5},
issn = {10495258},
pages = {5999--6009},
title = {{Attention is all you need}},
volume = {2017-Decem},
year = {2017}
}
@misc{crd2006directive,
author = {{European Union}},
booktitle = {Official Journal of the European Union},
number = {176},
publisher = {EC},
title = {{Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the activity of credit institutions and the prudential supervision of credit institutions and investment firms, amending Directive 2002/87/EC and repealing Dir}},
volume = {L},
year = {2013}
}
@article{Hess2002,
author = {Hess, Klaus Th and Schmidt, Klaus D},
isbn = {0167-6687},
journal = {Insurance: Mathematics and Economics},
number = {3},
pages = {351--364},
title = {{A comparison of models for the chain–ladder method}},
volume = {31},
year = {2002}
}
@article{Richards2009,
author = {Richards, S J},
isbn = {1357-3217},
journal = {British Actuarial Journal},
number = {63},
pages = {267},
title = {{Selected issues in modelling mortality by cause and in small populations}},
volume = {15},
year = {2009}
}
@inproceedings{Beard1961,
author = {Beard, R.E.},
booktitle = {Proceedings of the International Population Conference1},
pages = {611--625},
title = {{A theory of mortality based on actuarial, biological, and medical considerations.}},
volume = {1},
year = {1963}
}
@article{Gage1984,
author = {Gage, T B and Dyke, B and Riviere, P G},
isbn = {0018-7143},
journal = {Human biology},
pages = {489--501},
title = {{Estimating mortality from two censuses: an application to the Trio of Surinam}},
year = {1984}
}
@article{Horiuchi1982,
author = {Horiuchi, Shiro and Coale, Ansley J},
isbn = {0032-4728},
journal = {Population Studies},
number = {2},
pages = {317--326},
title = {{A simple equation for estimating the expectation of life at old ages}},
volume = {36},
year = {1982}
}
@article{Banister2004,
abstract = {This paper uses data from censuses and surveys to re-estimate mortality levels and trends in China from the 1960s to 2000. We use the General Growth Balance method to evaluate the completeness of death reporting above the youngest ages in three censuses of the People's Republic of China from 1982 to 2000, concluding that reporting quality is quite high, and revisit the completeness of death recording in the 1973-75 Cancer Epidemiology Survey. Estimates of child mortality from a variety of direct and indirect sources are reviewed, and best estimates arrived at. Our estimates show a spectacular improvement in life expectancy in China: from about 60 years in the period 1964-82 to nearly 70 years in the period 1990-2000, with a further improvement to over 71 years by 2000. We discuss why survival rates continue improving in China despite reduced government involvement in and increasing privatization of health services, with little insurance coverage. {\textcopyright} 2004 Population Investigation Committee.},
annote = {Banister, Judith
Hill, Kenneth
eng
1-PO1-AG17625/AG/NIA NIH HHS/
Research Support, Non-U.S. Gov't
Research Support, U.S. Gov't, P.H.S.
England
2004/06/19 05:00
Popul Stud (Camb). 2004;58(1):55-75.},
author = {Banister, Judith and Hill, Kenneth},
doi = {10.1080/0032472032000183753},
isbn = {0032-4728 (Print)
0032-4728 (Linking)},
issn = {00324728},
journal = {Population Studies},
keywords = {Adjusted life tables,Adult mortality,Child mortality,China,Completeness of death reporting,General Growth Balance method,Life expectancy,Mortality trends,PRC censuses,Survival rates},
number = {1},
pages = {55--75},
pmid = {15204262},
title = {{Mortality in China 1964-2000}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15204262},
volume = {58},
year = {2004}
}
@article{Plat2011,
abstract = {Upcoming new regulation on regulatory required solvency capital for insurers will be predominantly based on a one-year Value-at-Risk measure. This measure aims at covering the risk of the variation in the projection year as well as the risk of changes in the best estimate projection for future years. This paper addresses the issue how to determine this Value-at-Risk for longevity and mortality risk. Naturally, this requires stochastic mortality rates. In the past decennium, a vast literature on stochastic mortality models has been developed. However, very few of them are suitable for determining the one-year Value-at-Risk. This requires a model for mortality trends instead of mortality rates. Therefore, we will introduce a stochastic mortality trend model that fits this purpose. The model is transparent, easy to interpret and based on well known concepts in stochastic mortality modeling. Additionally, we introduce an approximation method based on duration and convexity concepts to apply the stochastic mortality rates to specific insurance portfolios. {\textcopyright} 2011 Elsevier B.V.},
author = {Plat, Richard},
doi = {10.1016/j.insmatheco.2011.07.002},
file = {:C$\backslash$:/Users/user-pc/Downloads/1-s2.0-S0167668711000795-main.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {One-year Value-at-Risk,Solvency 2,Stochastic mortality trend model},
number = {3},
pages = {462--470},
publisher = {Elsevier B.V.},
title = {{One-year Value-at-Risk for longevity and mortality}},
url = {http://dx.doi.org/10.1016/j.insmatheco.2011.07.002},
volume = {49},
year = {2011}
}
@article{Ayuso2016a,
abstract = {Pay-as-you-drive (PAYD), or usage-based automobile insurance (UBI), is a policy agreement tied to vehicle usage. In this paper we analyze the effect of the distance traveled on the risk of accidents among young drivers with a PAYD policy. We use regression models for survival data to estimate how long it takes them to have their first accident at fault during the coverage period. Our empirical application with real data is presented and shows that gender differences are mainly attributable to the intensity of use. Indeed, although gender has a significant effect in explaining the time to the first crash, this effect is no longer significant when the average distance traveled per day is introduced in the model. This suggests that gender differences in the risk of accidents are, to a large extent, attributable to the fact that men drive more often than women. Estimates of the time to the first accident for different driver risk types are presented. We conclude that no gender discrimination is necessary if telematics provides enough information on driving habits.},
author = {Ayuso, Mercedes and Guillen, Montserrat and P{\'{e}}rez-Mar{\'{i}}n, Ana},
doi = {10.3390/risks4020010},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ayuso, Guillen, P{\'{e}}rez-Mar{\'{i}}n - 2016 - Telematics and Gender Discrimination Some Usage-Based Evidence on Whether Men's Risk of Accidents D.pdf:pdf},
issn = {2227-9091},
journal = {Risks},
month = {apr},
number = {2},
pages = {10},
publisher = {MDPI AG},
title = {{Telematics and Gender Discrimination: Some Usage-Based Evidence on Whether Men's Risk of Accidents Differs from Women's}},
volume = {4},
year = {2016}
}
@book{Mack2002,
author = {Mack, T},
publisher = {Schriftenreihe Angewandte Versicherungsmathematik, DGVM},
title = {{Schadenversicherungsmathematik 2. Auflage}},
year = {2002}
}
@article{Preston1996,
author = {Preston, Samuel H and Elo, Irma T and Rosenwaike, Ira and Hill, Mark},
isbn = {0070-3370},
journal = {Demography},
number = {2},
pages = {193--209},
title = {{African-American mortality at older ages: Results of a matching study}},
volume = {33},
year = {1996}
}
@article{Vincent1951,
author = {Vincent, Paul},
isbn = {0032-4663},
journal = {Population (french edition)},
pages = {181--204},
title = {{La mortalit{\'{e}} des vieillards}},
year = {1951}
}
@incollection{Boleslawski2001,
author = {Boleslawski, Lech and Tabeau, Ewa},
booktitle = {Forecasting Mortality in Developed Countries},
doi = {10.1007/0-306-47562-6_6},
isbn = {0792368339},
pages = {127--155},
publisher = {Springer},
title = {{Comparing Theoretical Age Patterns of Mortality Beyond the Age of 80}},
year = {2001}
}
@article{Financial2016,
author = {Financial, Directorate F O R and Affairs, Enterprise and Committee, Competition and Discrimination, Price},
file = {:C$\backslash$:/R/refas/DAF-COMP(2016)15.en.pdf:pdf},
title = {{OECD (2016), Price Discrimination}},
year = {2016}
}
@article{DBLP:journals/corr/HuangLW16a,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L2+1) direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {2261--2269},
title = {{Densely connected convolutional networks}},
url = {http://arxiv.org/abs/1608.06993},
volume = {2017-Janua},
year = {2017}
}
@misc{Dorrington2013d,
address = {Paris},
author = {Dorrington, R E},
booktitle = {Moultrie,T, Dorrington, R, Hill, A, Hill, K, Tim{\ae}us, I, Zaba, B. (Eds), Tools for Demographic Estimation. Paris: International Union for the Scientific Study of Population},
number = {2016/01/31},
publisher = {International Union for the Scientific Study of Population},
title = {{Synthetic extinct generations methods}},
url = {http://demographicestimation.iussp.org/content/synthetic-extinct-generations-methods},
volume = {2016},
year = {2013}
}
@article{Lindbergson2001,
author = {Lindbergson, Maria},
isbn = {0346-1238},
journal = {Scandinavian Actuarial Journal},
number = {1},
pages = {79--94},
title = {{Mortality among the elderly in Sweden 1988–1997}},
volume = {2001},
year = {2001}
}
@book{Dorrington2004,
author = {Dorrington, Rob and Moultrie, Tom A and Tim{\ae}us, Ian},
isbn = {079922281X},
publisher = {Centre for Actuarial Research, University of Cape Town},
title = {{Estimation of mortality using the South African Census 2001 data}},
year = {2004}
}
@misc{Moultrie2015,
address = {Johannesburg},
author = {Moultrie, T A and Dorrington, R},
booktitle = {Seventh African Population Conference},
title = {{Where do they all come from? Where do they all belong? Searching for Eleanor Rigby in the 2011 Census data on migration.}},
year = {2015}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. {\textcopyright} 1989.},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Breiman2001a,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * *, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
issn = {08856125},
journal = {Machine Learning},
keywords = {Classification,Ensemble,Regression},
month = {oct},
number = {1},
pages = {5--32},
publisher = {Springer},
title = {{Random forests}},
volume = {45},
year = {2001}
}
@article{Rossouw2019,
author = {Rossouw, Louis and Richman, Ronald},
doi = {10.2139/ssrn.3465424},
file = {:C$\backslash$:/Users/user-pc/Desktop/2019-RossouwRichman-FIN.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
title = {{Using Machine Learning to Model Claims Experience and Reporting Delays for Pricing and Reserving}},
url = {https://www.ssrn.com/abstract=3465424 https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=3465424},
year = {2019}
}
@book{Mitchell1997b,
author = {Mitchell, T M},
publisher = {McGraw-Hill Boston, MA},
title = {{Machine learning}},
year = {1997}
}
@book{VanTonder1975,
author = {{Van Tonder}, Jan Louis and {Van Eeden}, I J},
isbn = {0869652052},
publisher = {Institute for Sociological, Demographic and Criminological Research, Human Sciences Research Council},
title = {{Abridged Life Tables for All the Population Groups in the Republic of South Africa (1921-70)}},
year = {1975}
}
@article{li2005coherent,
abstract = {Mortality patterns and trajectories in closely related populations are likely to be similar in some respects, and differences are unlikely to increase in the long run. It should therefore be possible to improve the mortality forecasts for individual countries by taking into account the patterns in a larger group. Using the Human Mortality Database, we apply the Lee-Carter model to a group of populations, allowing each its own age pattern and level of mortality but imposing shared rates of change by age. Our forecasts also allow divergent patterns to continue for a while before tapering off. We forecast greater longevity gains for the United States and lesser ones for Japan relative to separate forecasts.},
author = {Li, Nan and Lee, Ronald},
doi = {10.1353/dem.2005.0021},
issn = {00703370},
journal = {Demography},
number = {3},
pages = {575--594},
pmid = {16235614},
publisher = {Springer},
title = {{Coherent mortality forecasts for a group of populations: An extension of the Lee-Carter method}},
volume = {42},
year = {2005}
}
@inproceedings{deng2009imagenet,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called {\&}{\#}x201C;ImageNet{\&}{\#}x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {{Jia Deng} and {Wei Dong} and Socher, R. and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
booktitle = {2009 IEEE conference on computer vision and pattern recognition},
doi = {10.1109/cvprw.2009.5206848},
organization = {Ieee},
pages = {248--255},
title = {{ImageNet: A large-scale hierarchical image database}},
year = {2009}
}
@article{Hill2010,
abstract = {Death distribution methods proposed for death registration coverage by comparison with census age distributions assume no net migration. This assumption makes it problematic to apply these methods to sub-national and national populations affected by substantial net migration. In this paper, we propose and explore a two-step process in which the Growth Balance Equation is first used to estimate net migration rates, using a model of age-specific migration, and then it is used to compare the observed death rates over successive ages against residual estimates made up by the entry rate plus the net migration rate minus the growth rate. This methodology is predicated on the observation that migration rates have a very different age pattern from death rates; it is only when this condition is true that net migration and deaths can be distinguished. The procedure proposed here works reasonably well in populations that generally have good data and rather high net migration rates. There is no reason to prefer the revised migration model over the original Rogers-Castro formulation.},
author = {Hill, Kenneth and Queiroz, Bernardo},
doi = {10.1590/s0102-30982010000100002},
isbn = {0102-3098},
issn = {0102-3098},
journal = {Revista Brasileira de Estudos de Popula{\c{c}}{\~{a}}o},
number = {1},
pages = {7--20},
title = {{Adjusting the general growth balance method for migration}},
volume = {27},
year = {2010}
}
@article{DAmour2019,
abstract = {Unobserved confounding is a central barrier to drawing causal inferences from observational data. Several authors have recently proposed that this barrier can be overcome in the case where one attempts to infer the effects of several variables simultaneously. In this paper, we present two simple, analytical counterexamples that challenge the general claims that are central to these approaches. We discuss some reasons for these failures and suggest directions for obtaining sufficient conditions for causal identifiaciton. Despite these negative results, we show that a simple modification to the multi-cause setting, incorporating a proxy or negative control variable, solves many of the problems highlighted by the examples, and suggest a way forward for causal inference with high-dimensional action spaces.},
author = {D'Amour, Alexander},
file = {:C$\backslash$:/R/refas/d-amour19a.pdf:pdf},
journal = {Proceedings of Machine Learning Research},
pages = {3478--3486},
title = {{On Multi-Cause Approaches to Causal Inference with Unobserved Counfounding: Two Cautionary Failure Cases and A Promising Alternative}},
url = {http://proceedings.mlr.press/v89/d-amour19a.html},
volume = {89},
year = {2019}
}
@article{Dorrington2001,
author = {Dorrington, Rob and Bourne, David and Bradshaw, Debbie and Laubscher, Ria and Tim{\ae}us, Ian M},
title = {{The impact of HIV/AIDS on adult mortality in South Africa}},
year = {2001}
}
@article{Diebold2006,
abstract = {Despite powerful advances in yield curve modeling in the last 20 years, comparatively little attention has been paid to the key practical problem of forecasting the yield curve. In this paper we do so. We use neither the no-arbitrage approach nor the equilibrium approach. Instead, we use variations on the Nelson-Siegel exponential components framework to model the entire yield curve, period-by-period, as a three-dimensional parameter evolving dynamically. We show that the three time-varying parameters may be interpreted as factors corresponding to level, slope and curvature, and that they may be estimated with high efficiency. We propose and estimate autoregressive models for the factors, and we show that our models are consistent with a variety of stylized facts regarding the yield curve. We use our models to produce term-structure forecasts at both short and long horizons, with encouraging results. In particular, our forecasts appear much more accurate at long horizons than various standard benchmark forecasts.},
author = {Diebold, Francis X. and Li, Canlin},
doi = {10.1016/j.jeconom.2005.03.005},
file = {:C$\backslash$:/Users/user-pc/Desktop/1-s2.0-S0304407605000795-main.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Factor model,Nelson-Siegel curve,Term structure,Yield curve,file:///C:/Users/user-pc/Desktop/1-s2.0-S030440760},
mendeley-tags = {file:///C:/Users/user-pc/Desktop/1-s2.0-S030440760},
month = {feb},
number = {2},
pages = {337--364},
publisher = {North-Holland},
title = {{Forecasting the term structure of government bond yields}},
volume = {130},
year = {2006}
}
@article{Guelman2014,
abstract = {Understanding the precise nature of price sensitivities at the individual policyholder level is extremely valuable for insurers. A rate increase has a direct impact on the premium customers are paying, but there is also the indirect impact as a result of the "causal" effect of the rate change on the customer's decision to renew the policy term. A rate increase may impair its intended impact on the overall profitability of the portfolio if it causes a large number of policyholders to lapse their policy and switch to an alternative insurer. The difficulty in measuring price elasticity from most insurance databases is that historical rate changes are reflective of a risk-based pricing exercise. As a result, the specific rate change at which a customer is exposed is a deterministic function of her observed covariates. The nature of the data is thus observational, rather than experimental. In this context, measuring the causal effect of a rate change on the policyholder's lapse outcome requires special modeling considerations. Conventional modeling approaches aimed to directly fit the lapse outcome as a function of the rate change and background covariates are likely to be inappropriate for the problem at hand. In this paper, we propose a causal inference framework to measure price elasticity in the context of Auto Insurance. One of the strengths of our approach is the transparency about the extent to which the database can support causal effects from rate changes. The model also allows us to more reliably estimate price-elasticity functions at the individual policyholder level. As the causal effect of a rate change varies across individuals, making an accurate rate change choice at the individual subject level is essential. The rate at which each subject is exposed could be optimized on the basis of the individual characteristics, for the purpose of maximizing the overall expected profitability of the portfolio. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Guelman, Leo and Guill{\'{e}}n, Montserrat},
doi = {10.1016/j.eswa.2013.07.059},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Causal inference,Insurance,Price elasticity,Price optimization},
number = {2},
pages = {387--396},
title = {{A causal inference approach to measure price elasticity in Automobile Insurance}},
volume = {41},
year = {2014}
}
@article{Bongaarts2006,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. One of the most notable achievements of modern societies is a large rise in human longevity. Since 1800 life expectancy at birth has doubled from about 40 years to nearly 80 years. Recent mortality trends are well established, but there is considerable disagreement among demographers and biologists about what lies ahead. Pessimists believe we are approaching limits to life expectancy, while optimists expect continued rapid improvements with no limits. Much is at stake: improvements in longevity are a key cause of sky rocketing costs of pensions and healthcare for the elderly. After a brief review of the controversy about future trends, this study},
author = {Bongaarts, John},
doi = {10.1111/j.1728-4457.2006.00144.x},
isbn = {1728-4457},
issn = {00987921},
journal = {Population and Development Review},
number = {4},
pages = {605--628},
pmid = {3398948},
title = {{How long will we live?}},
volume = {32},
year = {2006}
}
@misc{Natarajan2019,
abstract = {A Wall Street regulator is opening a probe into Goldman Sachs Group Inc.'s credit card practices after a viral tweet from a tech entrepreneur alleged gender discrimination in the new Apple Card's algorithms when determining credit limits.},
author = {Natarajan, Sridhar and Nasiripour, Shahien},
booktitle = {Bloomberg.com},
keywords = {A Keith Wall,APPLE INC,California,GOLDMAN SACHS GROUP INC,Gender,Law,New York,TWITTER INC,Wall Street,Women,business,technology},
language = {en},
mendeley-tags = {A Keith Wall,APPLE INC,California,GOLDMAN SACHS GROUP INC,Gender,Law,New York,TWITTER INC,Wall Street,Women,business,technology},
month = {nov},
title = {{Viral Tweet About Apple Card Leads to Goldman Sachs Probe}},
url = {https://www.bloomberg.com/news/articles/2019-11-09/viral-tweet-about-apple-card-leads-to-probe-into-goldman-sachs},
year = {2019}
}
@article{Kleinberg2015,
author = {Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Obermeyer, Ziad},
doi = {10.1257/aer.p20151023},
issn = {00028282},
journal = {American Economic Review},
month = {may},
number = {5},
pages = {491--495},
publisher = {American Economic Association},
title = {{Prediction policy problems}},
volume = {105},
year = {2015}
}
@techreport{Vincent2008,
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite. Copyright 2008 by the author(s)/owner(s).},
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre Antoine},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
doi = {10.1145/1390156.1390294},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vincent et al. - 2008 - Extracting and composing robust features with denoising autoencoders.pdf:pdf},
isbn = {9781605582054},
pages = {1096--1103},
title = {{Extracting and composing robust features with denoising autoencoders}},
year = {2008}
}
@book{Efron2016,
abstract = {The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. ‘Big data', ‘data science', and ‘machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.},
author = {Efron, Bradley and Hastie, Trevor},
booktitle = {Computer Age Statistical Inference: Algorithms, Evidence, and Data Science},
doi = {10.1017/CBO9781316576533},
isbn = {9781316576533},
pages = {1--475},
publisher = {Cambridge University Press},
title = {{Computer age statistical inference: Algorithms, evidence, and data science}},
volume = {5},
year = {2016}
}
@article{Deng2017a,
abstract = {Machine learning, one of today's most rapidly growing interdisciplinary fields, promises an unprecedented perspective for solving intricate quantum many-body problems. Understanding the physical aspects of the representative artificial neural-network states has recently become highly desirable in the applications of machine-learning techniques to quantum many-body physics. In this paper, we explore the data structures that encode the physical features in the network states by studying the quantum entanglement properties, with a focus on the restricted-Boltzmann-machine (RBM) architecture. We prove that the entanglement entropy of all short-range RBM states satisfies an area law for arbitrary dimensions and bipartition geometry. For long-range RBM states, we show by using an exact construction that such states could exhibit volume-law entanglement, implying a notable capability of RBM in representing quantum states with massive entanglement. Strikingly, the neural-network representation for these states is remarkably efficient, in the sense that the number of nonzero parameters scales only linearly with the system size. We further examine the entanglement properties of generic RBM states by randomly sampling the weight parameters of the RBM. We find that their averaged entanglement entropy obeys volume-law scaling, and the meantime strongly deviates from the Page entropy of the completely random pure states.We show that their entanglement spectrum has no universal part associated with random matrix theory and bears a Poisson-type level statistics. Using reinforcement learning, we demonstrate that RBM is capable of finding the ground state (with power-law entanglement) of a model Hamiltonian with a longrange interaction. In addition, we show, through a concrete example of the one-dimensional symmetryprotected topological cluster states, that the RBM representation may also be used as a tool to analytically compute the entanglement spectrum. Our results uncover the unparalleled power of artificial neural networks in representing quantum many-body states regardless of how much entanglement they possess, which paves a novel way to bridge computer-science-based machine-learning techniques to outstanding quantum condensed-matter physics problems.},
archivePrefix = {arXiv},
arxivId = {1701.04844},
author = {Deng, Dong Ling and Li, Xiaopeng and {Das Sarma}, S.},
doi = {10.1103/PhysRevX.7.021021},
eprint = {1701.04844},
file = {:C$\backslash$:/R/refas/1701.04844.pdf:pdf},
issn = {21603308},
journal = {Physical Review X},
keywords = {Computational physics,Condensed matter physics,Quantum physics},
number = {2},
pages = {1--17},
title = {{Quantum entanglement in neural network states}},
volume = {7},
year = {2017}
}
@article{Luo2018,
abstract = {Batch Normalization (BN) improves both convergence and generalization in training neural networks. This work understands these phenomena theoretically. We analyze BN by using a basic block of neural networks, consisting of a kernel layer, a BN layer, and a nonlinear activation function. This basic network helps us understand the impacts of BN in three aspects. First, by viewing BN as an implicit regularizer, BN can be decomposed into population normalization (PN) and gamma decay as an explicit regularization. Second, learning dynamics of BN and the regularization show that training converged with large maximum and effective learning rate. Third, generalization of BN is explored by using statistical mechanics. Experiments demonstrate that BN in convolutional neural networks share the same traits of regularization as the above analyses.},
archivePrefix = {arXiv},
arxivId = {1809.00846},
author = {Luo, Ping and Wang, Xinjiang and Shao, Wenqi and Peng, Zhanglin},
eprint = {1809.00846},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo et al. - 2018 - Towards Understanding Regularization in Batch Normalization.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
month = {sep},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Towards Understanding Regularization in Batch Normalization}},
url = {http://arxiv.org/abs/1809.00846},
year = {2018}
}
@article{Peters2016,
abstract = {What is the difference of a prediction that is made with a causal model and a non-causal model? Suppose we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (for example various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.},
author = {Peters, Jonas and Meinshausen, Nicolai and B{\"{u}}hlmann, Peter},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peters, Meinshausen, B{\"{u}}hlmann - 2016 - Causal inference using invariant prediction identification and confidence intervals (with discuss.pdf:pdf},
journal = {Jrss(B)},
keywords = {causal discovery,causal inference,confidence intervals,invariant prediction},
pages = {(to appear)},
title = {{Causal inference using invariant prediction: identification and confidence intervals (with discussion)}},
year = {2016}
}
@article{Shea1984,
abstract = {This article analyzes the flaws in the estimation and interpretation of spline-smoothed data, particularly when constants are not accounted for. The authors focus on the relationship between this type of data and interest rate term structure, which is the array of discount rates on a collection of discount bonds. Smoothing of interest-rate term-structure data with spline functions has been undertaken but remains controversial. Effective experimentation with term-structure approximations will require flexible systems for specifying constrained spline models.},
author = {Shea, Gary S.},
doi = {10.2307/2331089},
issn = {00221090},
journal = {The Journal of Financial and Quantitative Analysis},
month = {sep},
number = {3},
pages = {253},
publisher = {JSTOR},
title = {{Pitfalls in Smoothing Interest Rate Term Structure Data: Equilibrium Models and Spline Approximations}},
volume = {19},
year = {1984}
}
@article{Hejazi2017,
abstract = {As part of the new regulatory framework of Solvency II, introduced by the European Union, insurance companies are required to monitor their solvency by computing a key risk metric called the Solvency Capital Requirement (SCR). The official description of the SCR is not rigorous and has lead researchers to develop their own mathematical frameworks for calculation of the SCR. These frameworks are complex and are difficult to implement. Recently, Bauer et al. suggested a nested Monte Carlo (MC) simulation framework to calculate the SCR. But the proposed MC framework is computationally expensive even for a simple insurance product. In this paper, we propose incorporating a neural network approach into the nested simulation framework to significantly reduce the computational complexity in the calculation. We study the performance of our neural network approach in estimating the SCR for a large portfolio of an important class of insurance products called Variable Annuities (VAs). Our experiments show that the proposed neural network approach is both efficient and accurate.},
archivePrefix = {arXiv},
arxivId = {1610.01946},
author = {Hejazi, Seyed Amir and Jackson, Kenneth R.},
doi = {10.1016/j.cam.2016.10.005},
eprint = {1610.01946},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hejazi-2017-Efficient valuation of SCR via a n.pdf:pdf},
isbn = {0377-0427},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
keywords = {Neural network,Portfolio valuation,Solvency Capital Requirement (SCR),Spatial interpolation,Variable annuity},
pages = {427--439},
shorttitle = {Efficient valuation of SCR via a neural network ap},
title = {{Efficient valuation of SCR via a neural network approach}},
volume = {313},
year = {2017}
}
@article{Srivastava2018a,
author = {Srivastava, Rupesh Kumar},
file = {:C$\backslash$:/Users/user-pc/Desktop/2018INFO006.pdf:pdf},
number = {February},
title = {{New Architectures for Very Deep Learning Rupesh Kumar Srivastava}},
year = {2018}
}
@article{Bengio2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the stateof-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks. {\textcopyright} 2009 Y. Bengio.},
author = {Bengio, Yoshua},
doi = {10.1561/2200000006},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Bengio-2009-Learning deep architectures for AI.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2009 - Learning deep architectures for AI.pdf:pdf},
isbn = {1935-8237},
issn = {19358237},
journal = {Foundations and Trends in Machine Learning},
number = {1},
pages = {1--27},
shorttitle = {Learning deep architectures for AI},
title = {{Learning deep architectures for AI}},
volume = {2},
year = {2009}
}
@inproceedings{Monti2017,
abstract = {Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph-and 3D shape analysis and show that it consistently outperforms previous approaches.},
archivePrefix = {arXiv},
arxivId = {1611.08402},
author = {Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodol{\`{a}}, Emanuele and Svoboda, Jan and Bronstein, Michael M.},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.576},
eprint = {1611.08402},
isbn = {9781538604571},
month = {nov},
pages = {5425--5434},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Geometric deep learning on graphs and manifolds using mixture model CNNs}},
volume = {2017-Janua},
year = {2017}
}
@book{Wood2017,
abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to be introductory in nature with a wealth of practical examples and software implementation. It is self-contained, providing the necessary background in linear models, linear mixed models, and generalized linear models (GLMs), before presenting a balanced treatment of the theory and applications of GAMs and related models. The author bases his approach on a framework of penalized regression splines, and while firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. Use of R software helps explain the theory and illustrates the practical application of the methodology. Each chapter contains an extensive set of exercises, with solutions in an appendix or in the book's R data package gamair, to enable use as a course text or for self-study.},
author = {Wood, Simon N.},
booktitle = {Generalized Additive Models: An Introduction with R, Second Edition},
doi = {10.1201/9781315370279},
isbn = {9781498728348},
issn = {1548-7660},
pages = {1--476},
publisher = {Chapman and Hall/CRC},
title = {{Generalized additive models: An introduction with R, second edition}},
year = {2017}
}
@misc{Schmidt2017,
author = {Kotze, Klaus},
booktitle = {African Yearbook of Rhetoric},
issn = {2220-2188},
number = {1},
pages = {99--114},
title = {{A bibliography on surveillance}},
url = {https://www.math.tu-dresden.de/sto/schmidt/dsvm/reserve.pdf},
volume = {3},
year = {2012}
}
@article{Smith2016,
author = {Smith, ML and Beyers, FJC and {De Villiers}, JP},
doi = {10.4314/saaj.v16i1.2},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Smith-2016-A method of parameterising a feed f.pdf:pdf},
isbn = {1680-2179},
issn = {1680-2179},
journal = {South African Actuarial Journal},
number = {1},
pages = {36},
shorttitle = {A method of parameterising a feed forward multi-la},
title = {{A method of parameterising a feed forward multi-layered perceptron artificial neural network, with reference to South African financial markets}},
volume = {16},
year = {2016}
}
@article{Smith2016,
author = {Smith, ML and Beyers, FJC and {De Villiers}, JP},
doi = {10.4314/saaj.v16i1.2},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Smith-2016-A method of parameterising a feed f.pdf:pdf},
isbn = {1680-2179},
issn = {1680-2179},
journal = {South African Actuarial Journal},
number = {1},
pages = {36},
title = {{A method of parameterising a feed forward multi-layered perceptron artificial neural network, with reference to South African financial markets}},
volume = {16},
year = {2016}
}
@misc{Weisstein2003,
author = {Weisstein, E},
number = {24 June},
shorttitle = {Convolution},
title = {{Convolution}},
url = {http://mathworld.wolfram.com/Convolution.html},
volume = {2018},
year = {2003}
}
@techreport{Redfern2014,
author = {Redfern, D and McLean, D},
institution = {Moody's Analytics Research},
title = {{Principal Component Analysis for Yield Curve Modelling}},
url = {https://www.moodysanalytics.com/-/media/whitepaper/2014/2014-29-08-PCA-for-Yield-Curve-Modelling.pdf},
year = {2014}
}
@misc{Lecun2015a,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {1807.07987},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
booktitle = {Nature},
doi = {10.1038/nature14539},
eprint = {1807.07987},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lecun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
issn = {14764687},
month = {may},
number = {7553},
pages = {436--444},
pmid = {26017442},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@book{Ohlsson2010,
abstract = {As described in the previous section, the goal of a tariff analysis is to determine how one or more key ratios Y vary with a number of rating factors. This is reminiscent of analyzing how the dependent variable Y varies with the covariates (explanatory variables) x in a multiple linear regression. Linear regression, or the slightly larger general linear model, is not fully suitable for non-life insurance pricing, though, since: (i) it assumes normally distributed random errors, while the number of insur- ance claims follows a discrete probability distribution on the non-negative integers, and claim costs are non-negative and often skewed to the right; (ii) in linear mod- els, the mean is a linear function of the covariates, while multiplicative models are usually more reasonable for pricing, cf. Sect. 1.3. Generalized},
author = {Ohlsson, Esbj{\"{o}}rn and Johansson, Bj{\"{o}}rn},
booktitle = {EAA Lecture Notes},
doi = {10.1007/978-3-642-10791-7},
isbn = {978-3-642-10790-0},
issn = {1869-6929},
keywords = {Quantitative Finance,Statistics},
pages = {71--99},
pmid = {16107623},
publisher = {Springer},
title = {{Non-Life Insurance Pricing with Generalized Linear Models}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-10791-7{\%}5Cnhttp://books.google.com/books?hl=en{\&}lr={\&}id=l4rjeflJ{\_}bIC{\&}oi=fnd{\&}pg=PA1{\&}dq=Non-Life+Insurance+Pricing+with+Generalized+Linear+Models{\&}ots=eykZ-{\_}R{\_}yk{\&}sig={\_}w9pA{\_}XXzEiFJS59-A9N{\_}ioxNE4},
volume = {2},
year = {2010}
}
@article{Booth2008,
abstract = {Continuing increases in life expectancy beyond previously-held limits have brought to the fore the critical importance of mortality forecasting. Significant developments in mortality forecasting since 1980 are reviewed under three broad approaches: expectation, extrapolation and explanation. Expectation is not generally a good basis for mortality forecasting, as it is subjective; expert expectations are invariably conservative. Explanation is restricted to certain causes of death with known determinants. Decomposition by cause of death poses problems associated with the lack of independence among causes and data difficulties. Most developments have been in extrapolative forecasting, and make use of statistical methods rather than models developed primarily for age-specific graduation. Methods using two-factor models (age-period or age-cohort) have been most successful. The two-factor Lee–Carter method, and, in particular, its variants, have been successful in terms of accuracy, while recent advances have improved the estimation of forecast uncertainty. Regression-based (GLM) methods have been less successful, due to nonlinearities in time. Three-factor methods are more recent; the Lee–Carter age-period-cohort model appears promising. Specialised software has been developed and made available. Research needs include further comparative evaluations of methods in terms of the accuracy of the point forecast and its uncertainty, encompassing a wide range of mortality situations.},
author = {Booth, H. and Tickle, L.},
doi = {10.1017/s1748499500000440},
isbn = {1748-5002},
issn = {1748-4995},
journal = {Annals of Actuarial Science},
number = {1-2},
pages = {3--43},
title = {{Mortality Modelling and Forecasting: a Review of Methods}},
volume = {3},
year = {2008}
}
@article{Kuo2020,
abstract = {We introduce an individual claims forecasting framework utilizing Bayesian mixture density networks that can be used for claims analytics tasks such as case reserving and triaging. The proposed approach enables incorporating claims information from both structured and unstructured data sources, producing multi-period cash flow forecasts, and generating different scenarios of future payment patterns. We implement and evaluate the modeling framework using publicly available data.},
archivePrefix = {arXiv},
arxivId = {2003.02453},
author = {Kuo, Kevin},
eprint = {2003.02453},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuo - 2020 - Individual Claims Forecasting with Bayesian Mixture Density Networks.pdf:pdf},
month = {mar},
title = {{Individual Claims Forecasting with Bayesian Mixture Density Networks}},
url = {http://arxiv.org/abs/2003.02453},
year = {2020}
}
@article{Siegel1976,
author = {Siegel, Jacob S and Passel, Jeffrey S},
isbn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {355},
pages = {559--566},
title = {{New estimates of the number of centenarians in the United States}},
volume = {71},
year = {1976}
}
@article{Bzdok2019,
abstract = {The traditional goal of quantitative analytics is to find simple, transparent models that generate explainable insights. In recent years, large-scale data acquisition enabled, for instance, by brain scanning and genomic profiling with microarray-type techniques, has prompted a wave of statistical inventions and innovative applications. Here we review some of the main trends in learning from ‘big data' and provide examples from imaging neuroscience. Some main messages we find are that modern analysis approaches (1) tame complex data with parameter regularization and dimensionality-reduction strategies, (2) are increasingly backed up by empirical model validations rather than justified by mathematical proofs, (3) will compare against and build on open data and consortium repositories, as well as (4) often embrace more elaborate, less interpretable models to maximize prediction accuracy.},
author = {Bzdok, Danilo and Nichols, Thomas E. and Smith, Stephen M.},
doi = {10.1038/s42256-019-0069-5},
file = {:C$\backslash$:/Users/user-pc/Desktop/10.1038@s42256-019-0069-5.pdf:pdf},
isbn = {4225601900},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
number = {7},
pages = {296--306},
publisher = {Springer US},
title = {{Towards algorithmic analytics for large-scale datasets}},
url = {http://dx.doi.org/10.1038/s42256-019-0069-5},
volume = {1},
year = {2019}
}
@inproceedings{Szegedy,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
isbn = {9781467369640},
issn = {10636919},
pages = {1--9},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@phdthesis{Dorrington1998,
author = {Dorrington, Rob},
publisher = {University of Cape Town},
title = {{Estimates of the level and shape of mortality rates in South Africa around 1985 and 1990 derived by applying indirect demographic techniques to reported deaths}},
year = {1998}
}
@article{EuropeanCommission2012,
abstract = {Article 5 of Council Directive 2004/113/EC of 13 December 2004 implementing the principle of equal treatment between men and women in the access to and supply of goods and services1 (hereinafter ‘the Directive') regulates the use of actuarial factors related to sex in the provision of insurance and other related financial services. Article 5(1) provides that, for new contracts concluded after 21 December 2007, the use of sex as an actuarial factor in the calculation of premiums and benefits must not result in differences in individuals' premiums and benefits (hereinafter ‘the unisex rule'). Article 5(2) provides for derogation from this rule by allowing Member States to maintain proportionate differences in individuals' premiums and benefits where the use of sex is a determining factor in the assessment of risk based on relevant and accurate actuarial and statistical data.},
author = {{European Commission}},
file = {:C$\backslash$:/R/refas/guidelines2004{\_}113{\_}EC.pdf:pdf},
journal = {Official Journal of the European Union},
number = {March 2011},
pages = {1--11},
title = {{Guidelines on the Application of Council Directive 2004/113/EC to Insurance, in the Light of the Court of Justice of the European Union in Case C-236/09 (Test-Achats)}},
volume = {C11},
year = {2012}
}
@inproceedings{Buhlmann1983,
author = {B{\"{u}}hlmann, H and Straub, E},
booktitle = {International Summer School},
title = {{Estimation of IBNR reserves by the methods chain ladder, Cape Cod and complementary loss ratio}},
volume = {1983},
year = {1983}
}
@article{Winlaw2019,
abstract = {Usage-based insurance schemes provide new opportunities for insurers to accurately price and manage risk. These schemes have the potential to better identify risky drivers which not only allows insurance companies to better price their products but it allows drivers to modify their behaviour to make roads safer and driving more efficient. However, for Usage-based insurance products, we need to better understand how driver behaviours influence the risk of a crash or an insurance claim. In this article, we present our analysis of automotive telematics data from over 28 million trips. We use a case control methodology to study the relationship between crash drivers and crash-free drivers and introduce an innovative method for determining control (crash-free) drivers. We fit a logistic regression model to our data and found that speeding was the most important driver behaviour linking driver behaviour to crash risk.},
author = {Winlaw, Manda and Steiner, Stefan H. and MacKay, R. Jock and Hilal, Allaa R.},
doi = {10.1016/j.aap.2019.06.003},
issn = {00014575},
journal = {Accident Analysis and Prevention},
keywords = {Case-control study,Crash risk,Driving behaviour,Logistic regression,Pay-how-you-drive},
month = {oct},
pages = {131--136},
publisher = {Elsevier Ltd},
title = {{Using telematics data to find risky driver behaviour}},
volume = {131},
year = {2019}
}
@article{kleinow2015common,
abstract = {We introduce a model for the mortality rates of multiple populations. To build the proposed model we investigate to what extent a common age effect can be found among the mortality experiences of several countries and use a common principal component analysis to estimate a common age effect in an age-period model for multiple populations. The fit of the proposed model is then compared to age-period models fitted to each country individually, and to the fit of the model proposed by Li and Lee (2005). Although we do not consider stochastic mortality projections in this paper, we argue that the proposed common age effect model can be extended to a stochastic mortality model for multiple populations, which allows to generate mortality scenarios simultaneously for all considered populations. This is particularly relevant when mortality derivatives are used to hedge the longevity risk in an annuity portfolio as this often means that the underlying population for the derivatives is not the same as the population in the annuity portfolio.},
author = {Kleinow, Torsten},
doi = {10.1016/j.insmatheco.2015.03.023},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Basis risk,Common age effect,Longevity,Mortality of multiple populations,Stochastic mortality model},
pages = {147--152},
publisher = {Elsevier},
title = {{A common age effect model for the mortality of multiple populations}},
volume = {63},
year = {2015}
}
@article{Lauderdale2002,
author = {Lauderdale, Diane S and Kestenbaum, Bert},
isbn = {0070-3370},
journal = {Demography},
pages = {529--540},
title = {{Mortality Rates of Elderly Asian American Populations Based on Medicare and Social Security Data}},
year = {2002}
}
@article{Cohen2006,
author = {Cohen, Barney and Menken, Jane and Hosegood, Victoria and Timaeus, Ian M},
title = {{HIV/AIDS and Older People in South Africa}},
year = {2006}
}
@inproceedings{Condran1991,
author = {Condran, Gretchen A and Himes, Christine and Preston, Samuel H},
booktitle = {Population Association of America Annual Meeting Baltimore Maryland March 30-April 1 1989.},
publisher = {[Unpublished] 1989. },
title = {{Old age mortality patterns in low-mortality countries: an evaluation of population and death data at advanced ages 1950 to the present}},
year = {1991}
}
@article{enchev2017multi,
abstract = {We review a number of multi-population mortality models: variations of the Li {\&} Lee model, and the common-age-effect (CAE) model of Kleinow. Model parameters are estimated using maximum likelihood. Although this introduces some challenging identifiability problems and complicates the estimation process it allows a fair comparison of the different models. We propose to solve these identifiability problems by applying two-dimensional constraints over the parameters. Using data from six countries, we compare and rank, both visually and numerically, the models' fitting qualities and develop forecasting models that produce non-diverging, joint mortality rate scenarios. It is found that the CAE model fits best. But we also find that the Li and Lee model potentially suffers from robustness problems when calibrated using maximum likelihood.},
author = {Enchev, Vasil and Kleinow, Torsten and Cairns, Andrew J G},
doi = {10.1080/03461238.2015.1133450},
issn = {16512030},
journal = {Scandinavian Actuarial Journal},
keywords = {Li and Lee model,Stochastic mortality model,common age effect model,multi-population},
number = {4},
pages = {319--342},
publisher = {Taylor {\&} Francis},
title = {{Multi-population mortality models: fitting, forecasting and comparisons}},
volume = {2017},
year = {2017}
}
@article{Schmertmann2002,
author = {Schmertmann, Carl P},
isbn = {0070-3370},
journal = {Demography},
number = {2},
pages = {287--310},
title = {{A simple method for estimating age-specific rates from sequential cross sections}},
volume = {39},
year = {2002}
}
@article{Pollard1949,
author = {Pollard, A H},
isbn = {0020-2681},
journal = {Journal of the Institute of Actuaries},
pages = {151--182},
title = {{Methods of forecasting mortality using Australian data}},
year = {1949}
}
@book{Charpentier2014,
abstract = {This book series reflcts the recent rapid growth in the development and application of R, the programming language and software environment for statistical computing and graphics. R is now widely used in academic research, education, and industry. It is constantly growing, with new versions of the core software released regularly and more than 5,000 packages available. It is diffiult for the documentation to keep pace with the expansion of the software, and this vital book series provides a forum for the publication of books covering many aspects of the development and application of R. The scope of the series is wide, covering three main threads: • Applications of R to specifi disciplines such as biology, epidemiology, genetics, engineering, fiance, and the social sciences. • Using R for the study of topics of statistical methodology, such as linear and mixed modeling, time series, Bayesian methods, and missing data. • The development of R, including programming, building packages, and graphics. The books will appeal to programmers and developers of R software, as well as applied statisticians and data analysts in many filds. The books will feature detailed worked examples and R code fully integrated into the text, ensuring their usefulness to researchers, practitioners and students.},
author = {Durante, Fabrizio},
booktitle = {International Statistical Review},
doi = {10.1111/insr.12119},
isbn = {1466592605},
issn = {0306-7734},
number = {3},
pages = {511--511},
publisher = {CRC Press},
title = {{Computational Actuarial Science with R}},
volume = {83},
year = {2015}
}
@article{Ellers2007,
abstract = {Certain data sets with distributions or counts can be interpreted as indirect observations of latent distributions or (time) series of counts. The structure of such data matches elegantly with the composite link model (CLM). The parameters can be estimated with iteratively re-weighted linear regression. Unfortunately, the estimating equations generally are singular or severely ill-conditioned. An effective solution is to impose smoothness on the solution, by penalizing the likelihood with a roughness measure. The optimal smoothing parameter is found efficiently by minimizing Akaike's Information Criterion (AIC). Several applications are presented. {\textcopyright} 2007 SAGE Publications.},
author = {Ellers, Paul H.C.},
doi = {10.1177/1471082X0700700302},
issn = {1471082X},
journal = {Statistical Modelling},
keywords = {Back-calculation,Mixtures,Negative binomial distribution,Over-dispersion},
month = {oct},
number = {3},
pages = {239--254},
title = {{Ill-posed problems with counts, the composite link model and penalized likelihood}},
volume = {7},
year = {2007}
}
@article{Lee1985,
author = {Lee, Ronald D},
isbn = {0032-4728},
journal = {Population Studies},
number = {2},
pages = {233--248},
title = {{Inverse projection and back projection: A critical appraisal, and comparative results for England, 1539 to 1871}},
volume = {39},
year = {1985}
}
@article{srivastava2014dropout,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets. {\textcopyright} 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Model combination,Neural networks,Regularization},
number = {1},
pages = {1929--1958},
publisher = {JMLR. org},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
volume = {15},
year = {2014}
}
@article{England1999a,
abstract = {We consider an appropriate residual definition for use in a bootstrap exercise to provide a computationally simple method of obtaining reserve prediction errors for a generalised linear model which reproduces the reserve estimates of the chain ladder technique (under certain restrictions which are specified in the paper). We show how the bootstrap prediction errors can be computed easily in a spreadsheet, without the need for statistical software packages. The bootstrap prediction errors are compared with their analytic equivalent from other stochastic reserving models, and also compared with other methods commonly used, including Mack's distribution free approach (Mack, 1993. ASTIN Bulletin 23 (2), 213-225) and methods based on log-linear models. {\textcopyright} 1999 Elsevier Science B.V.},
author = {England, Peter and Verrall, Richard},
doi = {10.1016/S0167-6687(99)00016-5},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Bootstrapping,Chain ladder technique,Claims reserving,Generalised linear models,Prediction errors},
month = {dec},
number = {3},
pages = {281--293},
publisher = {Elsevier},
title = {{Analytic and bootstrap estimates of prediction errors in claims reserving}},
volume = {25},
year = {1999}
}
@article{Shisana2014,
author = {Shisana, O and Rehle, T and Simbayi, L C and Zuma, K and Jooste, S and Zungu, N and Labadarios, D and Onoya, D and Davids, A and Ramlagan, S},
journal = {Cape Town},
title = {{South African national HIV prevalence, incidence and behaviour survey, 2012}},
year = {2014}
}
@article{England2019,
abstract = {This paper brings together analytic and simulation-based approaches to reserve risk in general (P{\&}C) insurance, applied to the traditional actuarial view of risk over the lifetime of the liabilities and to the one-year view of Solvency II. It also connects the lifetime and one-year views of risk. The framework of the model in Mack (1993) is used throughout, although the results have wider applicability. The advantages of a simulation-based approach are highlighted, giving a full predictive distribution, which is used to estimate risk margins under Solvency II and risk adjustments under IFRS 17. We discuss methods for obtaining capital requirements in a cost-of-capital risk margin, and methods for estimating risk adjustments using risk measures applied to a simulated distribution of the outstanding liabilities over their lifetime.},
author = {England, Peter and Verrall, R. J. and W{\"{u}}thrich, M. V.},
doi = {10.1016/j.insmatheco.2018.12.002},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/England, Verrall, W{\"{u}}thrich - 2019 - On the lifetime and one-year views of reserve risk, with application to IFRS 17 and Solvency II risk.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Bootstrap,Coherent risk measure,Cost-of-capital,IFRS 17 risk adjustment,Solvency II risk margin,Stochastic reserving},
pages = {74--88},
publisher = {Elsevier B.V.},
title = {{On the lifetime and one-year views of reserve risk, with application to IFRS 17 and Solvency II risk margins}},
url = {https://doi.org/10.1016/j.insmatheco.2018.12.002},
volume = {85},
year = {2019}
}
@techreport{Lakshminarayanan,
abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1612.01474},
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1612.01474},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakshminarayanan, Pritzel, Deepmind - Unknown - Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles.pdf:pdf},
issn = {10495258},
pages = {6403--6414},
title = {{Simple and scalable predictive uncertainty estimation using deep ensembles}},
volume = {2017-Decem},
year = {2017}
}
@article{Peterson1990,
author = {Peterson, Jane W},
journal = {The cultural context of aging},
pages = {213--228},
title = {{Age of wisdom: Elderly Black women in family and church}},
year = {1990}
}
@article{Dorrington2014a,
author = {Dorrington, R E and Bradshaw, D and Laubscher, R},
journal = {Cape Town, South Africa: South African Medical Research Council},
title = {{Rapid mortality surveillance report 2012}},
year = {2014}
}
@article{Bengio,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning. {\textcopyright} 1979-2012 IEEE.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Bengio-Unsupervised feature learning and deep.pdf:pdf},
isbn = {0162-8828 VO - 35},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
number = {8},
pages = {1798--1828},
pmid = {23787338},
shorttitle = {Unsupervised feature learning and deep learning: A},
title = {{Representation learning: A review and new perspectives}},
url = {https://pdfs.semanticscholar.org/f8c8/619ea7d68e604e40b814b40c72888a755e95.pdf{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/23459267},
volume = {35},
year = {2013}
}
@article{Ananthakrishnan2016,
abstract = {A new polymeric dyad of oligo-anthracene-block-poly(3-hexylthiophene) (Oligo-ANT-b-P3HT) has been synthesized as a donor–donor dyad building block for organic photovoltaics. The polymer dyad and oligomer of anthracene-9,10-diyl (Oligo-ANT) are prepared by Grignard Metathesis. The higher order of crystallinity and molecular chains ordering at solid phase reveal the intrinsic optical and electrical properties of polymeric dyad resulting in relatively higher light harvesting ability compared to the oligo(anthracene-9,10-diyl). The UV-visible spectrum of (Oligo-ANT-b-P3HT) in solution shows broad absorption with two sets of absorption from both anthracene and thiophene core units, covering a wide range of the visible spectrum. The test devices of the blends of polymeric dyad with fullerene C61 (PCBM) show improved photovoltaic performance with a power conversion efficiency of 3.26{\%} upon subjecting to pre-fabrication thermal treatments. With optimized morphology of the interpenetrating network and the shorter fluorescence lifetime of the annealed dyad/PCBM blends, the effective charge transfer from the donor dyad to PCBM has evidenced. Thus, these studies will allow further synthetic advances to make potential high crystalline polymeric dyads with significantly improved light harvesting capability. {\textcopyright} 2016 Wiley Periodicals, Inc. J. Polym. Sci., Part A: Polym. Chem. 2016, 54, 3032–3045.},
author = {Ananthakrishnan, Soundaram Jeevarathinam and Strain, Jacob and {Neerudu Sreeramulu}, Niharika and Mitul, Abu and McNamara, Louis E. and Iefanova, Anastasiia and Hammer, Nathan I. and Qiao, Qiquan and Rathnayake, Hemali},
doi = {10.1002/pola.28189},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ananthakrishnan et al. - 2016 - A novel donor–donor polymeric dyad of Poly(3-hexylthiophene-block-oligo(anthracene-9,10-diyl) Synthesis,.pdf:pdf},
issn = {10990518},
journal = {Journal of Polymer Science, Part A: Polymer Chemistry},
keywords = {Diblock copolymers,Fullerenes,copolymerization,oligomers},
number = {18},
pages = {3032--3045},
title = {{A novel donor–donor polymeric dyad of Poly(3-hexylthiophene-block-oligo(anthracene-9,10-diyl): Synthesis, solid-state packing, and electronic properties}},
volume = {54},
year = {2016}
}
@article{Richman2019e,
abstract = {Deep Learning models are currently being introduced into business processes to support decision-making in insurance companies. At the same time model risk is recognized as an increasingly relevant field within the management of operational risk that tries to mitigate the risk of poor business decisions because of flawed models or inappropriate model use. In this paper we try to determine how Deep Learning models are different from established actuarial models currently in use in insurance companies and how these differences might necessitate changes in the model risk management framework. We analyse operational risk in the development and implementation of Deep Learning models using examples from pricing and mortality forecasting to illustrate specific model risks and controls to mitigate those risks. We discuss changes in model governance and the role that model risk managers could play in providing assurance on the appropriate use of Deep Learning models.},
author = {Richman, Ronald and von Rummell, Nicolai and Wuthrich, Mario V.},
doi = {10.2139/ssrn.3444833},
file = {:C$\backslash$:/Users/user-pc/Desktop/2019 RichmanVRummell{\&}Wuthrich 2D Authors.pdf:pdf},
journal = {SSRN Electronic Journal},
keywords = {Deep learning,Insurance Modelling,Model Risk,Mortality Forecasting,Pricing},
month = {sep},
publisher = {Elsevier BV},
title = {{Believing the Bot - Model Risk in the Era of Deep Learning}},
year = {2019}
}
@article{Li2009a,
author = {Li, Qiang and Reuser, Mieke and Kraus, Cornelia and Alho, Juha},
isbn = {1443-2447},
journal = {Journal of Population Research},
number = {1},
pages = {21--50},
title = {{Ageing of a giant: a stochastic population forecast for China, 2006–2060}},
volume = {26},
year = {2009}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144},
journal = {arXiv},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
volume = { arXiv:160},
year = {2016}
}
@article{Bourbeau2000,
author = {Bourbeau, Robert and Lebel, Andr{\'{e}}},
doi = {10.4054/demres.2000.2.2},
journal = {Demographic Research},
number = {2},
pages = {36},
title = {{Mortality statistics for the oldest-old}},
volume = {2},
year = {2000}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Schmidhuber-2015-Deep learning in neural netwo.pdf:pdf},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
pages = {85--117},
pmid = {25462637},
shorttitle = {Deep learning in neural networks: An overview},
title = {{Deep Learning in neural networks: An overview}},
volume = {61},
year = {2015}
}
@misc{de2008generalized,
abstract = {This is the only book actuaries need to understand generalized linear models (GLMs) for insurance applications. GLMs are used in the insurance industry to support critical decisions. Until now, no text has introduced GLMs in this context or addressed the problems specific to insurance data. Using insurance data sets, this practical, rigorous book treats GLMs, covers all standard exponential family distributions, extends the methodology to correlated data structures, and discusses recent developments which go beyond the GLM. The issues in the book are specific to insurance data, such as model selection in the presence of large data sets and the handling of varying exposure times. Exercises and data-based practicals help readers to consolidate their skills, with solutions and data sets given on the companion website. Although the book is package-independent, SAS code and output examples feature in an appendix and on the website. In addition, R code and output for all the examples are provided on the website.},
author = {{De Jong}, Piet and Heller, Gillian Z.},
booktitle = {Generalized Linear Models for Insurance Data},
doi = {10.1017/CBO9780511755408},
institution = {Cambridge University Press},
isbn = {9780511755408},
issn = {0266-4763},
pages = {1--196},
title = {{Generalized linear models for insurance data}},
year = {2008}
}
@article{Cleveland1990,
abstract = {STL is a filtering procedure for decomposing a time series into trend, seasonal, and remainder components. STL has a simple design that consists of a sequence of applications of the loess smoother; the simplicity allows analysis of the properties of the procedure and allows fast computation, even for very long time series and large amounts of trend and seasonal smoothing. Other features of STL are specification of amounts of seasonal and trend smoothing that range, in a nearly continuous way, from a very small amount of smoothing to a very large amount; robust estimates of the trend and seasonal components that are not distorted by aberrant behavior in the data; specification of the period of the seasonal component to any integer multiple of the time sampling interval greater than one; and the ability to decompose time series with missing values.},
author = {Cleveland, Robert B and Cleveland, William S and McRae, Jean E and Terpenning, Irma},
doi = {citeulike-article-id:1435502},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cleveland, Cleveland, Terpenning - 1990 - STL A Seasonal-Trend Decomposition Procedure Based on Loess.pdf:pdf},
isbn = {0282-423X},
issn = {0282-423X},
journal = {Journal of Official Statistics},
number = {1},
pages = {3--73},
title = {{STL: A seasonal-trend decomposition procedure based on loess}},
volume = {6},
year = {1990}
}
@article{Redington1969,
abstract = { Throughout this century mortality has been declining and if an exponential curve in the Gompertz form $\mu$ x =  bc x  is fitted to recent English Life Tables b has been decreasing as we would expect. Between the 1911 Table and the 1951 Table, 10 5 b has fallen from 18 to 6 for males and from 9 to 2 for females. But c has been increasing—from 1{\textperiodcentered}087 to 1{\textperiodcentered}103 for males, and from 1{\textperiodcentered}094 to 1{\textperiodcentered}112 for females. What feature of mortality can it be that has been deteriorating in this scientific age? Has c any counterpart in reality? Or is it just a mathematical by-product of our system of description? },
author = {Redington, F. M.},
doi = {10.1017/s0020268100016127},
isbn = {0020-2681},
issn = {0020-2681},
journal = {Journal of the Institute of Actuaries},
number = {2},
pages = {243--317},
title = {{An Exploration into Patterns of Mortality}},
volume = {95},
year = {1969}
}
@article{treasury2004orange,
abstract = {risk; risk management; Orange; principle; appetite; model; framework; communication; learning},
author = {{HM Treasury}},
isbn = {1845320441},
journal = {London: HM Treasury},
number = {October},
pages = {1--52},
title = {{The Orange Book Management of Risk: Principles and Concepts}},
year = {2004}
}
@misc{DmitriJdanov,
author = {Jdanov, Dmitri and Shkolnikov, Vladimir and Andreev, Evgueni and Alustiza, Ainhoa and Kubisch, Karolin and Gellers-Barkmann, Sigrid and Barbieri, Magali and Boe, Carl and Vallin, Jacques and Mesl{\'{e}}, France},
title = {{Human Life Table Database}},
url = {https://www.lifetable.de/cgi-bin/index.php},
urldate = {2020-07-02}
}
@article{Boonen2017a,
abstract = {This paper examines the consequences for a life annuity insurance company if the solvency II solvency capital requirements (SCR) are calibrated based on expected shortfall (ES) instead of value-at-risk (VaR). We focus on the risk modules of the SCRs for the three risk classes equity risk, interest rate risk and longevity risk. The stress scenarios are determined using the calibration method proposed by EIOPA in 2014. We apply the stress-scenarios for these three risk classes to a fictitious life annuity insurance company. We find that for EIOPA's current quantile 99.5{\%} of the VaR, the stress scenarios of the various risk classes based on ES are close to the stress scenarios based on VaR. Might EIOPA choose to calibrate the stress scenarios on a smaller quantile, the longevity SCR is relatively larger and the equity SCR is relatively smaller if ES is used instead of VaR. We derive the same conclusion if stress scenarios are determined with empirical stress scenarios.},
author = {Boonen, Tim J.},
doi = {10.1007/s13385-017-0160-4},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Boonen-2017-Solvency II solvency capital requi.pdf:pdf},
isbn = {2190-9733},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Expected shortfall,Solvency II,Solvency capital requirement,Value-at-risk},
number = {2},
pages = {405--434},
title = {{Solvency II solvency capital requirement for life insurance companies based on expected shortfall}},
volume = {7},
year = {2017}
}
@book{Bradshaw2002,
author = {Bradshaw, Debbie and Schneider, Michelle and Laubscher, Ria and Nojilana, Beatrice},
title = {{Cause of death profile, South Africa 1996}},
year = {2002}
}
@misc{Schreiber2017,
address = {DIA Munich 2017},
author = {Friendsurance},
number = {17 June},
title = {{The future of insurance | Friendsurance}},
url = {http://www.friendsurance.com/},
volume = {2018},
year = {2015}
}
@article{Ezzini2018,
abstract = {In the last decade, significant advances have been made in sensing and communication technologies. Such progress led to a considerable growth in the development and use of intelligent transportation systems. Characterizing driving styles of drivers using in-vehicle sensor data is an interesting research problem and an essential real-world requirement for automotive industries. A good representation of driving features can be extremely valuable for anti-theft, auto insurance, autonomous driving, and many other application scenarios. This paper addresses the problem of driver identification using real driving datasets consisting of measurements taken from in-vehicle sensors. The paper investigates the minimum learning and classification times that are required to achieve a desired identification performance. Further, feature selection is carried out to extract the most relevant features for driver identification. Finally, in addition to driving pattern related features, driver related features (e.g., heart-rate) are shown to further improve the identification performance.},
author = {Ezzini, Saad and Berrada, Ismail and Ghogho, Mounir},
doi = {10.1186/s40537-018-0118-7},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Ezzini-2018-Who is behind the wheel{\_} Driver id.pdf:pdf},
isbn = {2196-1115},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {Driver fingerprinting,Driver identification,Driver verification,Machine learning},
number = {1},
pages = {9},
shorttitle = {Who is behind the wheel? Driver identification and},
title = {{Who is behind the wheel? Driver identification and fingerprinting}},
volume = {5},
year = {2018}
}
@article{Kuroki2014,
abstract = {This paper highlights several areas where graphical techniques can be harnessed to address the problem of measurement errors in causal inference. In particular, it discusses the control of unmeasured confounders in parametric and nonparametric models and the computational problem of obtaining bias-free effect estimates in such models. We derive new conditions under which causal effects can be restored by observing proxy variables of unmeasured confounders with/without external studies. {\textcopyright} 2014 Biometrika Trust.},
author = {Kuroki, Manabu and Pearl, Judea},
doi = {10.1093/biomet/ast066},
issn = {14643510},
journal = {Biometrika},
keywords = {Causal diagram,Confounder,Instrumental variable method,Proxy variable,Regression coefficient,Total effect},
number = {2},
pages = {423--437},
title = {{Measurement bias and effect restoration in causal inference}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/ast066},
volume = {101},
year = {2014}
}
@article{Gan2015a,
abstract = {A variable annuity (VA) is equity-linked annuity product that has rapidly grown in popularity around the world in recent years. Research up to date on VA largely focuses on the valuation of guarantees embedded in a single VA contract. However, methods developed for individual VA contracts based on option pricing theory cannot be extended to large VA portfolios. Insurance companies currently use nested simulation to valuate guarantees for VA portfolios but efficient valuation under nested simulation for a large VA portfolio has been a real challenge. The computation in nested simulation is highly intensive and often prohibitive. In this paper, we propose a novel approach that combines a clustering technique with a functional data analysis technique to address the issue. We create a highly non-homogeneous synthetic VA portfolio of 100,000 contracts and use it to estimate the dollar Delta of the portfolio at each time step of outer loop scenarios under the nested simulation framework over a period of 25 years. Our test results show that the proposed approach performs well in terms of accuracy and efficiency.},
author = {Gan, Guojun and Lin, X. Sheldon},
doi = {10.1016/j.insmatheco.2015.02.007},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Gan-2015-Valuation of large variable annuity p.pdf:pdf},
isbn = {0167-6687},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Clustering,Dollar delta,Functional data analysis,Minimum guarantee,Nested simulation,Stochastic-on-stochastic,VA portfolio,Variable annuity},
pages = {138--150},
title = {{Valuation of large variable annuity portfolios under nested simulation: A functional data approach}},
volume = {62},
year = {2015}
}
@book{Datta2017,
abstract = {This book presents direct and concise explanations and examples to many $\backslash$LaTeX{\{}{\}} syntax and structures, allowing students and researchers to quickly understand the basics that are required for writing and preparing book manuscripts, journal articles, reports, presentation slides and academic theses and dissertations for publication. Unlike much of the literature currently available on $\backslash$LaTeX{\{}{\}}, which takes a more technical stance, focusing on the details of the software itself, this book presents a user-focused guide that is concerned with its application to everyday tasks and scenarios. It is packed with exercises and looks at topics like formatting text, drawing and inserting tables and figures, bibliographies and indexes, equations, slides, and provides valuable explanations to error and warning messages so you can get work done with the least time and effort needed. This means $\backslash$booktitle{\{}$\backslash$LaTeX{\{}{\}} in 24 Hours{\}} can be used by students and researchers with little or no previous experience with $\backslash$LaTeX{\{}{\}} to gain quick and noticeable results, as well as being used as a quick reference guide for those more experienced who want to refresh their knowledge on the subject.},
author = {Datta, Dilip},
booktitle = {LaTeX in 24 Hours},
doi = {10.1007/978-3-319-47831-9},
file = {:C$\backslash$:/Users/user-pc/Desktop/2017{\_}Book{\_}LaTeXIn24Hours.pdf:pdf},
isbn = {9783319478302},
title = {{LaTeX in 24 Hours}},
year = {2017}
}
@inproceedings{DasGupta1991,
author = {{Das Gupta}, P},
booktitle = {American Statistical Association Proceedings of the Social Statistics Section},
pages = {154--159},
title = {{Reconstruction of the Age Distribution of the Extreme Aged in the 1980 Census by the Method of Extinct Generations}},
year = {1991}
}
@techreport{UnitedNations1955,
author = {{United Nations}, Department of Social and Economic Affairs},
title = {{Methods of Appraisal of Quality of Basic Data for Population Estimates}},
year = {1955}
}
@article{Bongaarts2005,
abstract = {In the study reported here, I had two objectives: (1) to test a new version of the logistic model for the pattern of change over time in age-specific adult mortality rates and (2) to develop a new method for projecting future trends in adult mortality. A test of the goodness of fit of the logistic model for the force of mortality indicated that its slope parameter is nearly constant over time. This finding suggests a variant of the model that is called the shifting logistic model. A new projection method, based on the shifting mortality model, is proposed and compared with the widely used Lee-Carter procedure.},
author = {Bongaarts, John},
doi = {10.1353/dem.2005.0003},
isbn = {0070-3370},
issn = {00703370},
journal = {Demography},
number = {1},
pages = {23--49},
pmid = {15782894},
title = {{Long-range trends in adult mortality: Models and projection methods}},
volume = {42},
year = {2005}
}
@article{Beard1971,
author = {Beard, R.E},
title = {{Some Aspects of the Theories of Mortality, Cause of Death Analysis, Forecasting and Stochastic Process}},
year = {1971}
}
@misc{BankofEngland,
author = {{Bank of England}},
booktitle = {Yield curves},
title = {{Yield curves}},
url = {https://www.bankofengland.co.uk/statistics/yield-curves},
urldate = {2020-03-09},
year = {2020}
}
@article{DESA2013,
author = {DESA, U N},
journal = {New York: Department for Economic and Social Affairs},
title = {{World Population Prospects, The 2012 Revision}},
year = {2013}
}
@article{Thatcher1992,
author = {Thatcher, A Roger},
isbn = {0032-4728},
journal = {Population Studies},
number = {3},
pages = {411--426},
title = {{Trends in numbers and mortality at high ages in England and Wales}},
volume = {46},
year = {1992}
}
@article{Wijnands2018,
abstract = {Globally, motor vehicle crashes account for over 1.2 million fatalities per year and are the leading cause of death for people aged 15–29 years. The majority of road crashes are caused by human error, with risk heightened among young and novice drivers learning to negotiate the complexities of the road environment. Direct feedback has been shown to have a positive impact on driving behaviour. Methods that could detect behavioural changes and therefore, positively reinforce safer driving during the early stages of driver licensing could have considerable road safety benefit. A new methodology is presented combining in-vehicle telematics technology, providing measurements forming a personalised driver profile, with neural networks to identify changes in driving behaviour. Using Long Short-Term Memory (LSTM) recurrent neural networks, individual drivers are identified based on their pattern of acceleration, deceleration and exceeding the speed limit. After model calibration, new, real-time data of the driver is supplied to the LSTM and, by monitoring prediction performance, one can assess whether a (positive or negative) change in driving behaviour is occurring over time. The paper highlights that the approach is robust to different neural network structures, data selections, calibration settings, and methodologies to select benchmarks for safe and unsafe driving. Presented case studies show additional model applications for investigating changes in driving behaviour among individuals following or during specific events (e.g., receipt of insurance renewal letters) and time periods (e.g., driving during holiday periods). The application of the presented methodology shows potential to form the basis of timely provision of direct feedback to drivers by telematics-based insurers. Such feedback may prevent internalisation of new, risky driving habits contributing to crash risk, potentially reducing deaths and injuries among young drivers as a result.},
author = {Wijnands, Jasper S. and Thompson, Jason and Aschwanden, Gideon D.P.A. and Stevenson, Mark},
doi = {10.1016/j.trf.2017.12.006},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Wijnands-2018-Identifying behavioural change a.pdf:pdf},
isbn = {1369-8478},
issn = {13698478},
journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
keywords = {Behavior,Driver,Feedback,Long short-term memory,Neural network,Transportation},
pages = {34--49},
shorttitle = {Identifying behavioural change among drivers using},
title = {{Identifying behavioural change among drivers using Long Short-Term Memory recurrent neural networks}},
volume = {53},
year = {2018}
}
@article{Lindholm2020,
author = {Lindholm, Mathias and Richman, Ronald and Tsanakas, Andreas and Wuthrich, Mario V.},
doi = {10.2139/ssrn.3520676},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {causal inference,complex algorithmic models,confounding,differentiation,direct discrimination,discrimination,discriminatory covariates,indirect discrimination,individual policy char- acteristics,insurance pricing,neural networks},
month = {jan},
title = {{Discrimination-Free Insurance Pricing}},
url = {https://www.ssrn.com/abstract=3520676},
year = {2020}
}
@inproceedings{Condran1991a,
author = {Condran, Gretchen A and Himes, Christine and Preston, Samuel H},
publisher = {[Unpublished] 1989. Paper presented at the Population Association of America Annual Meeting Baltimore Maryland March 30-April 1 1989.},
title = {{Old age mortality patterns in low-mortality countries: an evaluation of population and death data at advanced ages 1950 to the present}},
year = {1991}
}
@book{Parodi2014,
abstract = {Based on the syllabus of the actuarial industry course on general insurance pricing — with additional material inspired by the author's own experience as a practitioner and lecturer — Pricing in General Insurance presents pricing as a formalised process that starts with collecting information about a particular policyholder or risk and ends with a commercially informed rate. The main strength of this approach is that it imposes a reasonably linear narrative on the material and allows the reader to see pricing as a story and go back to the big picture at any time, putting things into context. Written with both the student and the practicing actuary in mind, this pragmatic textbook and professional reference: Complements the standard pricing methods with a description of techniques devised for pricing specific products (e.g., non-proportional reinsurance and property insurance) Discusses methods applied in personal lines when there is a large amount of data and policyholders can be charged depending on many rating factors Addresses related topics such as how to measure uncertainty, incorporate external information, model dependency, and optimize the insurance structure Provides case studies, worked-out examples, exercises inspired by past exam questions, and step-by-step methods for dealing concretely with specific situations Pricing in General Insurance delivers a practical introduction to all aspects of general insurance pricing, covering data preparation, frequency analysis, severity analysis, Monte Carlo simulation for the calculation of aggregate losses, burning cost analysis, and more.},
author = {Parodi, Pietro},
booktitle = {Pricing in General Insurance},
doi = {10.1201/b17525},
isbn = {1466581441},
publisher = {CRC Press},
title = {{Pricing in General Insurance}},
year = {2014}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {1503.02531},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Vinyals, Dean - 2015 - Distilling the Knowledge in a Neural Network.pdf:pdf},
pages = {1--9},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@book{Skansi2018,
abstract = {阐述了液力变矩器叶栅系统参数化设计方法,在对几种典型叶型设计方法分析比较的基础上,基于实用的最小参数原则选定了样条拟合曲线方法实现叶片设计,并结合变宽度循环圆设计方法,提出了叶栅系统参数化设计方法,基于UG/OPEN API编制了相应程序并给出了设计实例。在上述研究基础上,总结并提出了液力变矩器叶栅系统参数化体系,为后续叶栅系统三维优化设计及制造提供了灵活便捷的设计模型。},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Skansi, Sandro},
booktitle = {Chemical Engineering Progress},
doi = {10.1007/978-3-319-73004-2},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/user-pc/Desktop/2018{\_}Book{\_}IntroductionToDeepLearning.pdf:pdf},
isbn = {978-3-319-73003-5},
issn = {03607275},
number = {6},
pages = {196},
pmid = {26840611},
title = {{Introduction to deep learning: From Logical Calculus to Artificial Intelligence}},
volume = {114},
year = {2018}
}
@article{Shmueli2010,
abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process. {\textcopyright} Institute of Mathematical Statistics, 2010.},
archivePrefix = {arXiv},
arxivId = {1101.0891},
author = {Shmueli, Galit},
doi = {10.1214/10-STS330},
eprint = {1101.0891},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Shmueli-2010-To explain or to predict{\_}.pdf:pdf},
isbn = {0883-4237},
issn = {08834237},
journal = {Statistical Science},
keywords = {Causality,Data mining,Explanatory modeling,Predictive modeling,Predictive power,Scientific research,Statistical strategy},
month = {aug},
number = {3},
pages = {289--310},
title = {{To explain or to predict?}},
volume = {25},
year = {2010}
}
@article{Makeham1867,
author = {Makeham, William Matthew},
isbn = {2046-1666},
journal = {Journal of the Institute of Actuaries (1866)},
pages = {325--358},
title = {{On the law of mortality}},
year = {1867}
}
@article{Gabrielli2019b,
abstract = {The chain-ladder method is one of the most popular claims reserving techniques. The aim of this study is to back-test the chain-ladder method. For this purpose, we use a stochastic scenario generator that allows us to simulate arbitrarily many upper claims reserving triangles of similar characteristics for which we also know the corresponding lower triangles. Based on these simulated triangles, we analyse the performance of the chain-ladder claims reserving method.},
author = {Gabrielli, Andrea and W{\"{u}}thrich, Mario V.},
doi = {10.1017/S1748499518000325},
issn = {17485002},
journal = {Annals of Actuarial Science},
keywords = {Back-testing,Chain-ladder,Claims reserving,Individual claims history simulation machine,Reserve uncertainty},
month = {sep},
number = {2},
pages = {334--359},
publisher = {Cambridge University Press},
title = {{Back-testing the chain-ladder method}},
volume = {13},
year = {2019}
}
@article{Case2012,
author = {Case, T H E Test-achats},
file = {:C$\backslash$:/R/refas/the-test-achats-case.pdf:pdf},
keywords = {The Test Achats Case, gender, premiums and benefit},
number = {December},
title = {{Can Gender Differentiated Actuarial Factors Still Be Used When Determining Premiums and Benefits Under Insurance Contracts ?}},
volume = {5},
year = {2012}
}
@article{Richards2006,
author = {Richards, Stephen J and Kirkby, J G and Currie, Iain D},
isbn = {2044-0456},
journal = {British Actuarial Journal},
number = {01},
pages = {5--38},
title = {{The importance of year of birth in two-dimensional mortality data}},
volume = {12},
year = {2006}
}
@misc{TowersPerrin2009,
author = {{Towers Perrin}},
doi = {http://s3.amazonaws.com/zanran_storage/www.cea.eu/ContentPages/19326750.pdf},
title = {{Longevity Risk Investigation}},
year = {2009}
}
@article{Gabrielli2019,
abstract = {The main idea of this paper is to embed a classical actuarial regression model into a neural network architecture. This nesting allows us to learn model structure beyond the classical actuarial regression model if we use as starting point of the neural network calibration exactly the classical actuarial model. Such models can be fitted efficiently which allows us to explore bootstrap methods for prediction uncertainty. As an explicit example, we consider the cross-classified over-dispersed Poisson model for general insurance claims reserving. We demonstrate how this model can be improved by neural network features.},
author = {Gabrielli, Andrea and Richman, Ronald and W{\"{u}}thrich, Mario V.},
doi = {10.1080/03461238.2019.1633394},
issn = {16512030},
journal = {Scandinavian Actuarial Journal},
keywords = {Cross-classified over-dispersed Poisson model,chain-ladder reserves,claims reserving in insurance,learning across portfolios,mean square error of prediction,model blending,nested models,neural network},
publisher = {Taylor and Francis Ltd.},
title = {{Neural network embedding of the over-dispersed Poisson reserving model}},
year = {2019}
}
@book{Goldberg2017,
abstract = {Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries. The second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning.},
archivePrefix = {arXiv},
arxivId = {1706.08502},
author = {Goldberg, Yoav},
booktitle = {Synthesis Lectures on Human Language Technologies},
doi = {10.2200/S00762ED1V01Y201703HLT037},
eprint = {1706.08502},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Goldberg-2017-Neural network methods for natur.pdf:pdf},
isbn = {1947-4040},
issn = {19474040},
keywords = {deep learning,machine learning,natural language processing,neural networks,recurrent neural networks,sequence to sequence models,supervised learning,word embeddings},
number = {1},
pages = {1--309},
pmid = {299497},
publisher = {Morgan {\&} Claypool Publishers},
series = {Synthesis Lectures on Human Language Technologies},
shorttitle = {Neural network methods for natural language proces},
title = {{Neural network methods for natural language processing}},
volume = {10},
year = {2017}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Schmidhuber-2015-Deep learning in neural netwo.pdf:pdf},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
pages = {85--117},
pmid = {25462637},
title = {{Deep Learning in neural networks: An overview}},
volume = {61},
year = {2015}
}
@misc{FederalEmergencyManagementAgency2019,
author = {{Federal Emergency Management Agency}},
isbn = {2019100000016},
title = {{FIMA NFIP Redacted Claims Data Set}},
url = {https://www.fema.gov/media-library/assets/documents/180376 https://www.fema.gov/media-library/assets/documents/180374},
year = {2019}
}
@article{Wuthrich2019,
author = {W{\"{u}}thrich, Mario V.},
doi = {10.2139/ssrn.3491790},
journal = {SSRN Electronic Journal},
keywords = {GLM,LASSO,balance property,canonical link,claims frequency modeling,deviance loss,exponen- tial dispersion family,generalized linear model,neural network,regression modeling,regularization,representation learning},
publisher = {Elsevier BV},
title = {{From Generalized Linear Models to Neural Networks, and Back}},
year = {2019}
}
@article{Wilmoth2007,
author = {Wilmoth, John R and Andreev, K and Jdanov, D and Glei, Dana A and Boe, C and Bubenheim, M and Philipov, D and Shkolnikov, V and Vachon, P},
journal = {University of California, Berkeley, and Max Planck Institute for Demographic Research, Rostock. URL: http://mortality.org [version 31/05/2007]},
title = {{Methods protocol for the human mortality database}},
year = {2007}
}
@article{Buettner2005,
author = {Buettner, T and Zlotnik, H},
isbn = {0016-6987},
journal = {Genus},
number = {1},
pages = {213--233},
title = {{Prospects for increasing longevity as assessed by the United Nations}},
volume = {LXI},
year = {2005}
}
@book{McGrayne2011,
abstract = {Bayes' rule appears to be a straightforward, one-line theorem: by updating our initial beliefs with objective new information, we get a new and improved belief. To its adherents, it is an elegant statement about learning from experience. To its opponents, it is subjectivity run amok. In the first-ever account of Bayes' rule for general readers, Sharon Bertsch McGrayne explores this controversial theorem and the human obsessions surrounding it. She traces its discovery by an amateur mathematician in the 1740s through its development into roughly its modern form by French scientist Pierre Simon Laplace. She reveals why respected statisticians rendered it professionally taboo for 150 years-at the same time that practitioners relied on it to solve crises involving great uncertainty and scanty information (Alan Turing's role in breaking Germany's Enigma code during World War II), and explains how the advent of off-the-shelf computer technology in the 1980s proved to be a game-changer. Today, Bayes' rule is used everywhere from DNA de-coding to Homeland Security. Drawing on primary source material and interviews with statisticians and other scientists, The Theory That Would Not Die is the riveting account of how a seemingly simple theorem ignited one of the greatest controversies of all time. {\textcopyright} 2011 by Sharon Bertsch McGrayne. All rights reserved.},
author = {McGrayne, Sharon Bertsch},
booktitle = {The Theory That Would Not Die: How Bayes' Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy},
doi = {10.1080/10848770.2013.817786},
isbn = {9780300169690},
issn = {1084-8770},
pages = {1--320},
publisher = {Yale University Press},
title = {{The theory that would not die: How bayes' rule cracked the enigma code, hunted down russian submarines, and emerged triumphant from two centuries of controversy}},
year = {2011}
}
@inproceedings{Gesmann,
author = {Gesmann, Markus},
booktitle = {Proc. of the R User Conference, August},
pages = {12--14},
title = {{The “ChainLadder” package-Insurance claims reserving in R}}
}
@article{Sadie1988,
author = {Sadie, Johannes L},
title = {{A reconstruction and projection of demographic movements in the RSA and TBVC countries}},
year = {1988}
}
@article{Milevsky2019,
author = {Milevsky, Moshe A},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Milevsky - 2019 - Calibrating Gompertz in Reverse Mortality-adjusted ( Biological ) Ages around the World.pdf:pdf},
number = {November},
title = {{Calibrating Gompertz in Reverse : Mortality-adjusted ( Biological ) Ages around the World}},
year = {2019}
}
@article{Mitchell1997,
author = {Mitchell, Tom M},
journal = {Burr Ridge, IL: McGraw Hill},
number = {37},
pages = {870--877},
title = {{Machine learning. 1997}},
volume = {45},
year = {1997}
}
@article{Mazess1982,
author = {Mazess, Richard B and Mathisen, Ralph W},
isbn = {0018-7143},
journal = {Human biology},
pages = {517--524},
title = {{Lack of unusual longevity in Vilcabamba, Ecuador}},
year = {1982}
}
@misc{DalMoro2016,
author = {{Dal Moro}, E and Cuypers, F and Miehe, P},
publisher = {ASTIN},
title = {{Non-life Reserving Practices}},
year = {2016}
}
@misc{DepartmentofDemography2020,
author = {Payeur, Fr{\'{e}}d{\'{e}}ric F. and Chouinard, Philippe and Bourbeau, Robert and Ouellette, Nadine},
title = {{CHMD Canadian Human Mortality Database}},
url = {http://www.bdlc.umontreal.ca/CHMD/},
urldate = {2020-06-15},
year = {2020}
}
@article{Dreksler2015,
abstract = {This paper brings together the work of the GI Solvency II Technical Provisions working party. The working party was formed in 2009 for the primary purpose of raising awareness of Solvency II and the impact it would have on the work that reserving actuaries do. Over the years, the working party's focus has shifted to exploring and promoting discussion of the many practical issues raised by the requirements and to promoting best practice. To this end, we have developed, presented and discussed many of the ideas contained in this paper at events and forums. However, the size of the subject means that at no one event have we managed to cover all of the areas that the reserving actuary needs to be aware of. This paper brings together our thinking in one place for the first time. We hope experienced practitioners will find it thought provoking, and a useful reference tool. For new practitioners, we hope it helps to get you up-to-speed quickly. Good luck!},
author = {Dreksler, S. and Allen, C. and Akoh-Arrey, A. and Courchene, J. A. and Junaid, B. and Kirk, J. and Lowe, W. and O'Dea, S. and Piper, J. and Shah, M. and Shaw, G. and Storman, D. and Thaper, S. and Thomas, L. and Wheatley, M. and Wilson, M.},
doi = {10.1017/s1357321714000099},
issn = {1357-3217},
journal = {British Actuarial Journal},
keywords = {Balance sheet,Binary events,Claims provision,Events not in data,Premium provision,Reserving,Risk margin,Segmentation,Solvency II,Technical provisions,Validation},
month = {mar},
number = {1},
pages = {7--129},
publisher = {Cambridge University Press (CUP)},
title = {{Solvency II Technical Provisions for General Insurers}},
volume = {20},
year = {2015}
}
@article{Merz2008,
abstract = {{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_} Abstract We assume that the claims liability process satisfies the distribution-free chain-ladder model assumptions. For claims reserving at time I we predict the total ultimate claim with the information available at time I and, similarly, at time 1 + I we predict the same total ultimate claim with the (updated) information available at time 1 + I . The claims development result at time 1 + I for accounting year (] 1 , + I I is then defined to be the difference between these two successive predictions for the total ultimate claim. In [6, 10] we have analyzed this claims development result and we have quantified its prediction uncertainty. Here, we simplify, modify and illustrate the results obtained in [6, 10]. We emphasize that these results have direct consequences for solvency considerations and were (under the new risk-adjusted solvency regulation) already implemented in industry.},
author = {Merz, Michael and W{\"{u}}thrich, Mario V},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Merz, W{\"{u}}thrich - 2008 - Modelling the claims development result for solvency purposes.pdf:pdf},
institution = {Casualty Actuary Society},
journal = {CAS E-Forum},
keywords = {chain-ladder method,claims development result,incurred losses prior accident,loss experience,mean square error of,predic-,solvency,stochastic claims reserving,years},
pages = {542--568},
title = {{Modelling the claims development result for solvency purposes}},
year = {2008}
}
@article{Preston1980a,
author = {Preston, Samuel and Hill, Kenneth},
isbn = {0032-4728},
journal = {Population studies},
number = {2},
pages = {349--366},
title = {{Estimating the completeness of death registration}},
volume = {34},
year = {1980}
}
@inproceedings{Brass1979,
author = {Brass, W.},
booktitle = {Asian and Pacific census forum / East-West Population Institute},
issn = {07320515},
number = {2},
pages = {5--7},
title = {{A procedure for comparing mortality measures calculated from intercensal survival with the corresponding estimates from registered deaths.}},
volume = {6},
year = {1979}
}
@misc{Dorrington2013b,
author = {Dorrington, Rob},
publisher = {Centre for Actuarial Research. Available: http://www.commerce.uct.ac.za/Research{\_}Units/CARE/Monographs/Monographs/Mono13.pdf. Accessed 2015/01/25},
title = {{Alternative South African mid-year estimates, 2013}},
year = {2013}
}
@article{Richards2014a,
abstract = {Longevity risk faced by annuity portfolios and defined-benefit pension schemes is typically long-term, i.e. the risk is of an adverse trend which unfolds over a long period of time. However, there are circumstances when it is useful to know by how much expectations of future mortality rates might change over a single year. Such an approach lies at the heart of the one-year, value-at-risk view of reserves, and also for the pending Solvency II regime for insurers in the European Union. This paper describes a framework for determining how much a longevity liability might change based on new information over the course of one year. It is a general framework and can accommodate a wide choice of stochastic projection models, thus allowing the user to explore the importance of model risk. A further benefit of the framework is that it also provides a robustness test for projection models, which is useful in selecting an internal model for management purposes.},
author = {Richards, S. J. and Currie, I. D. and Ritchie, G. P.},
doi = {10.1017/s1357321712000451},
issn = {1357-3217},
journal = {British Actuarial Journal},
month = {mar},
number = {1},
pages = {116--139},
publisher = {Cambridge University Press (CUP)},
title = {{A Value-at-Risk framework for longevity trend risk}},
volume = {19},
year = {2014}
}
@article{Moultrie2012,
author = {Moultrie, Tom A and Dorrington, Rob E},
isbn = {0141-9870},
journal = {Ethnic and Racial Studies},
number = {8},
pages = {1447--1465},
title = {{Used for ill; used for good: a century of collecting data on race in South Africa}},
volume = {35},
year = {2012}
}
@article{Garson1991,
author = {Garson, Lea Keil},
isbn = {0032-4728},
journal = {Population Studies},
number = {2},
pages = {265--278},
title = {{The centenarian question: old-age mortality in the Soviet Union, 1897 to 1970}},
volume = {45},
year = {1991}
}
@article{Paszke2017,
abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd, and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Paszke, Adam and Chanan, Gregory and Lin, Zeming and Gross, Sam and Yang, Edward and Antiga, Luca and Devito, Zachary},
doi = {10.1017/CBO9781107707221.009},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {31st Conference on Neural Information Processing Systems},
number = {Nips},
pages = {1--4},
pmid = {25246403},
title = {{Automatic differentiation in PyTorch}},
year = {2017}
}
@techreport{CSI2012,
author = {CSI},
file = {:C$\backslash$:/Users/user-pc/Zotero/storage/AJUL9ATF/Lewis et al. - ACTUARIAL SOCIETY OF SOUTH AFRICA ANNUITANT MORTAL.pdf:pdf},
publisher = {Continuous Statistical Investigation Committee,  Actuarial Society of South Africa},
title = {{Annuitant Mortality 2001-2004}},
year = {2012}
}
@inproceedings{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1310.4546},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Mikolov-2013-Distributed representations of wo.pdf:pdf},
issn = {10495258},
pages = {1--31},
shorttitle = {Distributed representations of words and phrases a},
title = {{Distributed representations ofwords and phrases and their compositionality}},
year = {2013}
}
@article{Machemedze2011,
author = {Machemedze, Takwanisa and Dorrington, Rob},
isbn = {0850-5780},
journal = {African Population Studies},
number = {s1},
pages = {63--76},
title = {{Levels of mortality of the South African aged population using the method of extinct generations}},
volume = {25},
year = {2011}
}
@article{Taylor1983,
abstract = {We begin by considering the lack of methodology for obtaining second moments of outstanding claims in non-life insurance (sections 1 and 2), and give some arguments as to why this lack is to be deprecated (section 3). We suggest that part of the reason is the lack of generality of the models and methods currently in use in claims analysis, and we suggest further that some unification might be achieved through regression analysis (section 4). The claims analysis problem is formulated and solved in terms of regression methods (sections 5 to 8), estimates of both first and second moments of outstanding claims being obtained. Two specific examples of the recommended regression procedures are presented algebraically (section 9). One of these is extended to a numerical example (section 10). The paper concludes with some general observations (section 11). {\textcopyright} 1983.},
author = {Taylor, G. C. and Ashe, F. R.},
doi = {10.1016/0304-4076(83)90074-X},
issn = {03044076},
journal = {Journal of Econometrics},
month = {sep},
number = {1},
pages = {37--61},
publisher = {North-Holland},
title = {{Second moments of estimates of outstanding claims}},
volume = {23},
year = {1983}
}
@article{Kuo2018,
abstract = {We propose a novel approach for loss reserving based on deep neural networks. The approach allows for joint modeling of paid losses and claims outstanding, and incorporation of heterogeneous inputs. We validate the models on loss reserving data across lines of business, and show that they improve on the predictive accuracy of existing stochastic methods. The models require minimal feature engineering and expert input, and can be automated to produce forecasts more frequently than manual workflows.},
archivePrefix = {arXiv},
arxivId = {1804.09253},
author = {Kuo, Kevin},
doi = {10.3390/risks7030097},
eprint = {1804.09253},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Kuo-2018-DeepTriangle{\_} A Deep Learning Approac.pdf:pdf},
issn = {2227-9091},
journal = {Risks},
number = {3},
pages = {97},
shorttitle = {DeepTriangle: A Deep Learning Approach to Loss Res},
title = {{DeepTriangle: A Deep Learning Approach to Loss Reserving}},
volume = {7},
year = {2019}
}
@article{Blake2006,
abstract = {This paper addresses the problem of longevity risk — the risk of uncertain aggregate mortality — and discusses the ways in which life assurers, annuity providers and pension plans can manage their exposure to this risk. In particular, it focuses on how they can use mortality-linked securities and over-the-counter contracts — some existing and others still hypothetical — to manage their longevity risk exposures. It provides a detailed analysis of two such securities — the Swiss Re mortality bond issued in December 2003 and the EIB/BNP longevity bond announced in November 2004. It then looks at the universe of hypothetical mortality-linked securities — other forms of longevity bonds, swaps, futures and options — and investigates their potential uses. It also addresses implementation issues, and draws lessons from the experiences of other derivative contracts. Particular attention is paid to the issues involved with the construction and use of mortality indices, the management of the associated credit risks, and possible barriers to the development of markets for these securities. It suggests that these implementation difficulties are essentially teething problems that will be resolved over time, and so leave the way open to the development of flourishing markets in a brand new class of securities.},
author = {Blake, D. and Cairns, A. J. G. and Dowd, K.},
doi = {10.1017/s1357321700004736},
isbn = {2044-0456},
issn = {1357-3217},
journal = {British Actuarial Journal},
number = {1},
pages = {153--197},
title = {{Living with Mortality: Longevity Bonds and Other Mortality-Linked Securities}},
volume = {12},
year = {2006}
}
@article{aggarwal2016model,
abstract = {With the increasing use of complex quantitative models in applications throughout the financial world, model risk has become a major concern. Such risk is generated by the potential inaccuracy and inappropriate use of models in business applications, which can lead to substantial financial losses and reputational damage. In this paper, we deal with the management and measurement of model risk. First, a model risk framework is developed, adapting concepts such as risk appetite, monitoring, and mitigation to the particular case of model risk. The usefulness of such a framework for preventing losses associated with model risk is demonstrated through case studies. Second, we investigate the ways in which different ways of using and perceiving models within an organisation both lead to different model risks. We identify four distinct model cultures and argue that in conditions of deep model uncertainty, each of those cultures makes a valuable contribution to model risk governance. Thus, the space of legitimate challenges to models is expanded, such that, in addition to a technical critique, operational and commercial concerns are also addressed. Third, we discuss through the examples of proxy modelling, longevity risk, and investment advice, common methods and challenges for quantifying model risk. Difficulties arise in mapping model errors to actual financial impact. In the case of irreducible model uncertainty, it is necessary to employ a variety of measurement approaches, based on statistical inference, fitting multiple models, and stress and scenario analysis.},
author = {Aggarwal, A. and Beck, M. B. and Cann, M. and Ford, T. and Georgescu, D. and Morjaria, N. and Smith, A. and Taylor, Y. and Tsanakas, A. and Witts, L. and Ye, I.},
doi = {10.1017/s1357321715000276},
issn = {1357-3217},
journal = {British Actuarial Journal},
number = {2},
pages = {229--296},
publisher = {Cambridge University Press},
title = {{Model risk – daring to open up the black box}},
volume = {21},
year = {2016}
}
@book{Hastie1990,
author = {Hastie, Trevor J and Tibshirani, Robert J},
isbn = {0412343908},
publisher = {CRC Press},
title = {{Generalized additive models}},
volume = {43},
year = {1990}
}
@article{DalMoro2014,
author = {{Dal Moro}, Eric and Lo, Joseph},
isbn = {1783-1350},
journal = {Astin Bulletin},
number = {03},
pages = {495--499},
title = {{An Industry Question: The Ultimate and One-Year Reserving Uncertainty for Different Non-Life Reserving Methodologies}},
volume = {44},
year = {2014}
}
@book{Axler1996,
abstract = {As humans, we perceive the three-dimensional structure of the world around us with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem, and what is the current state of the art?Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging and fun consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos.More than just a source of "recipes", this text/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting this process to produce the best possible descriptions of a scene. Exercises are presented throughout the book, with a heavy emphasis on testing algorithms.Suitable for either an undergraduate or a graduate-level course in computer vision, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries.Dr. Richard Szeliski has over twenty years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft.},
author = {Axler, Sheldon},
booktitle = {Choice Reviews Online},
doi = {10.5860/choice.33-6354},
file = {:C$\backslash$:/Users/user-pc/Desktop/2015{\_}Book{\_}LinearAlgebraDoneRight.pdf:pdf},
isbn = {9783319110790},
issn = {0009-4978},
number = {11},
pages = {33--6354--33--6354},
title = {{Linear algebra done right}},
volume = {33},
year = {1996}
}
@article{Tibshirani1996,
abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
author = {Tibshirani, Robert},
doi = {10.1111/j.2517-6161.1996.tb02080.x},
isbn = {0035-9246},
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
number = {1},
pages = {267--288},
title = {{Regression Shrinkage and Selection Via the Lasso}},
volume = {58},
year = {1996}
}
@article{Wuthrich2017,
abstract = {Car insurance companies have started to collect high-frequency GPS location data of their car drivers. This data provides detailed information about the driving habits and driving styles of individual car drivers. We illustrate how this data can be analyzed using techniques from pattern recognition and machine learning. In particular, we describe how driving styles can be categorized so that they can be used for a regression analysis in car insurance pricing.},
author = {W{\"{u}}thrich, Mario V.},
doi = {10.1007/s13385-017-0149-z},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/W{\"{u}}thrich-2017-Covariate selection from telemat.pdf:pdf},
isbn = {2190-9733},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Categorical classes,Clustering,Driving habits,Driving styles,K-means clustering,Machine learning,Pattern recognition,Regression,Telematics data,Unsupervised learning},
number = {1},
pages = {89--108},
shorttitle = {Covariate selection from telematics car driving da},
title = {{Covariate selection from telematics car driving data}},
volume = {7},
year = {2017}
}
@article{Thatcher1990,
abstract = {The ‘law of mortality' proposed by Heligman and Pollard is compared with the law of Gompertz and with English Life Table No. 14. Some new mathematical results are derived, including specific equations for the curve of deaths. Some numerical illustrations are given.},
author = {Thatcher, A. R.},
doi = {10.1017/s0020268100043043},
issn = {0020-2681},
journal = {Journal of the Institute of Actuaries},
number = {1},
pages = {135--149},
title = {{Some results on the Gompertz and Heligman and Pollard laws of mortality}},
volume = {117},
year = {1990}
}
@inproceedings{Kusner2017,
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
archivePrefix = {arXiv},
arxivId = {1703.06856},
author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1703.06856},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kusner et al. - 2017 - Counterfactual Fairness.pdf:pdf},
issn = {10495258},
pages = {4067--4077},
title = {{Counterfactual fairness}},
url = {http://arxiv.org/abs/1703.06856},
year = {2017}
}
@article{Buhlmann2009,
abstract = {In recent Solvency II considerations much effort has been put into the development of appropriate models for the study of the one-year loss reserving uncertainty in non-life insurance. In this article we derive formulas for the conditional mean square error of prediction of the one-year claims development result in the context of the Bayes chain ladder model studied in Gisler-W{\"{u}}thrich. The key to these formulas is a recursive representation for the results obtained in Gisler-W{\"{u}}thrich.},
author = {B{\"{u}}hlmann, Hans and {De Felice}, Massimo and Gisler, Alois and Moriconi, Franco and W{\"{u}}thrich, Mario V.},
doi = {10.2143/ast.39.1.2038065},
isbn = {0515-0361},
issn = {0515-0361},
journal = {ASTIN Bulletin},
number = {1},
pages = {275--306},
title = {{Recursive Credibility Formula for Chain Ladder Factors and the Claims Development Result}},
volume = {39},
year = {2009}
}
@article{StatsSA2009,
author = {{Stats SA}},
journal = {Statistical release P},
title = {{Mortality and causes of death in South Africa, 2007: Findings from death notification}},
volume = {309},
year = {2009}
}
@article{Dong2017,
abstract = {In this paper, we study learning generalized driving style representations from automobile GPS trip data. We propose a novel Autoencoder Regularized deep neural Network (ARNet) and a trip encoding framework trip2vec to learn drivers' driving styles directly from GPS records, by combining supervised and unsupervised feature learning in a unified architecture. Experiments on a challenging driver number estimation problem and the driver identification problem show that ARNet can learn a good generalized driving style representation: It significantly outperforms existing methods and alternative architectures by reaching the least estimation error on average (0.68, less than one driver) and the highest identification accuracy (by at least 3{\%} improvement) compared with traditional supervised learning methods.},
archivePrefix = {arXiv},
arxivId = {1701.01272},
author = {Dong, Weishan and Yuan, Ting and Yang, Kai and Li, Changsheng and Zhang, Shilei},
doi = {10.24963/ijcai.2017/222},
eprint = {1701.01272},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Dong-2017-Autoencoder regularized network for.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {1603--1609},
shorttitle = {Autoencoder regularized network for driving style },
title = {{Autoencoder regularized network for driving style representation learning}},
volume = {arXiv:1701},
year = {2017}
}
@book{Wong2019,
abstract = {This textbook offers a comprehensive overview of applied demography by presenting both basic concepts and methodological techniques. It allows students from the social and human sciences, demographers, consultants and anyone interested in applied demography to gain an understanding of a wide range of practical applications of demographic concepts, methods and techniques to real- world problems. Featured sidebars highlight relevant terms and concepts and case studies and exercises throughout the book offer first-hand exposure to demographic applications. Charts and graphs supplement the presentation of demographic concepts and a glossary provides an inventory of relevant terms. The first section reviews basic components of applied demography as a context for understanding and addressing societal issues. It details the methods, techniques and data sources applied by demographers in a variety of areas. Coverage includes cohort analysis, data standardization, population estimation, and the use of geographic in- formation systems (GIS). The second section focuses on the substantive areas in which demography is currently applied. The topics covered include business demography, health demography, political demography, educational demography, and applications to urban and regional planning. The book illustrates the many ways in which demographers contribute to the formulation of public policy and the resolution of societal issues.},
author = {Wong, David},
booktitle = {Spatial Demography},
doi = {10.1007/s40980-019-00047-1},
file = {:C$\backslash$:/Users/user-pc/Desktop/2018{\_}Book{\_}ConceptsMethodsAndPracticalApp.pdf:pdf},
isbn = {9783319654386},
issn = {2364-2289},
number = {1},
pages = {103--104},
title = {{Thomas, Richard K.: Concepts, Methods and Practical Applications in Applied Demography: An Introductory Text}},
volume = {7},
year = {2019}
}
@misc{FrenchInstituteforDemographicStudiesFranceandMaxPlanckInstituteforDemographicResearchGermany,
author = {{French Institute for Demographic Studies (France) and Max Planck Institute for Demographic Research (Germany)}},
title = {{Human CoD Database}},
url = {https://www.causesofdeath.org/cgi-bin/main.php},
urldate = {2020-03-08}
}
@article{VanderWeele2013,
abstract = {The causal inference literature has provided a clear formal definition of confounding expressed in terms of counterfactual independence. The literature has not, however, come to any consensus on a formal definition of a confounder, as it has given priority to the concept of confounding over that of a confounder. We consider a number of candidate definitions arising from various more informal statements made in the literature. We consider the properties satisfied by each candidate definition, principally focusing on (i) whether under the candidate definition control for all "confounders" suffices to control for "confounding" and (ii) whether each confounder in some context helps eliminate or reduce confounding bias. Several of the candidate definitions do not have these two properties. Only one candidate definition of those considered satisfies both properties. We propose that a "confounder" be defined as a pre-exposure covariate C for which there exists a set of other covariates X such that effect of the exposure on the outcome is unconfounded conditional on (X,C) but such that for no proper subset of (X,C) is the effect of the exposure on the outcome unconfounded given the subset. We also provide a conditional analogue of the above definition; and we propose a variable that helps reduce bias but not eliminate bias be referred to as a "surrogate confounder." These definitions are closely related to those given by Robins and Morgenstern [Comput. Math. Appl. 14 (1987) 869-916]. The implications that hold among the various candidate definitions are discussed. {\textcopyright} 2013 Institute of Mathematical Statistics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1304.0564v1},
author = {{Vander Weele}, Tyler J. and Shpitser, Ilya},
doi = {10.1214/12-AOS1058},
eprint = {arXiv:1304.0564v1},
file = {:C$\backslash$:/R/refas/tyler2013on.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Causal diagrams,Causal inference,Confounder,Counterfactual,Minimal sufficiency},
number = {1},
pages = {196--220},
pmid = {25544784},
title = {{On the definition of a confounder}},
volume = {41},
year = {2013}
}
@book{Khan2018,
abstract = {Computer vision has become increasingly important and effective in recent years due to its wide-ranging applications in areas as diverse as smart surveillance and monitoring, health and medicine, sports and recreation, robotics, drones, and self-driving cars. Visual recognition tasks, such as image classification, localization, and detection, are the core building blocks of many of these applications, and recent developments in Convolutional Neural Networks (CNNs) have led to outstanding performance in these state-of-the-art visual recognition tasks and systems. As a result, CNNs now form the crux of deep learning algorithms in computer vision. is self-contained guide will benefit those who seek to both understand the theory be- hind CNNs and to gain hands-on experience on the application of CNNs in computer vision. It provides a comprehensive introduction to CNNs starting with the essential concepts behind neural networks: training, regularization, and optimization of CNNs. e book also discusses a wide range of loss functions, network layers, and popular CNN architectures, reviews the differ- ent techniques for the evaluation of CNNs, and presents some popular CNN tools and libraries that are commonly used in computer vision. Further, this text describes and discusses case stud- ies that are related to the application of CNN in computer vision, including image classification, object detection, semantic segmentation, scene understanding, and image generation. is book is ideal for undergraduate and graduate students, as no prior background knowl- edge in the field is required to follow the material, as well as new researchers, developers, engi- neers, and practitioners who are interested in gaining a quick understanding of CNN models.},
author = {Khan, Salman and Rahmani, Hossein and Shah, Syed Afaq Ali and Bennamoun, Mohammed},
booktitle = {Synthesis Lectures on Computer Vision},
doi = {10.2200/s00822ed1v01y201712cov015},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Khan-2018-A Guide to Convolutional Neural Netw.pdf:pdf},
isbn = {1681730227},
issn = {2153-1056},
number = {1},
pages = {1--207},
publisher = {Morgan {\&} Claypool Publishers},
shorttitle = {A Guide to Convolutional Neural Networks for Compu},
title = {{A Guide to Convolutional Neural Networks for Computer Vision}},
volume = {8},
year = {2018}
}
@article{thomson2006typology,
abstract = {This paper proposes a categorisation of the models used in actuarial science. It illustrates the application of that categorisation by using it to classify numerous such models. It is suggested that this categorisation, together with the illustrative classification, may be used as a typology for the classification of other such models, and that this typology may be found useful as a candidate exemplar, or as a basis for further refinement, in the discourse of actuarial science.},
author = {Thomson, RJ},
doi = {10.4314/saaj.v6i1.24505},
issn = {1680-2179},
journal = {South African Actuarial Journal},
number = {1},
pages = {19--36},
publisher = {Actuarial Society of South Africa},
title = {{A typology of models used in actuarial science}},
volume = {6},
year = {2006}
}
@article{Olshansky1997,
author = {Olshansky, S Jay and Carnes, Bruce A},
isbn = {0070-3370},
journal = {Demography},
number = {1},
pages = {1--15},
title = {{Ever since gompertz}},
volume = {34},
year = {1997}
}
@article{Anderson1999,
abstract = {This paper presents some new estimates of the UK real and nominal yield curves. These estimates are derived using a spline-based technique put forward by Waggoner (1997), modified for the UK government bond markets. At the short end of the nominal yield curve, additional data are included from the GC repo market. Estimates of the real yield curve are derived from the prices of index-linked gilts within a modified version of the framework put forward by Evans (1998). It is found that the new yield curves outperform existing methods on a number of criteria that are designed to examine the suitability of estimates for the purpose of assessing monetary conditions. In particular, the estimates are found to be smooth across maturity while having sufficient flexibility to describe the shape of the curve at shorter maturities where expectations are relatively precise. The curves are also robust to small errors in the data.},
author = {Anderson, By Nicola and Sleath, John},
journal = {Bank of England Quarterly Bulletin},
pages = {384--392},
title = {{New estimates of the UK real and nominal yield curves}},
url = {https://www.bankofengland.co.uk/working-paper/2001/new-estimates-of-the-uk-real-and-nominal-yield-curves},
year = {1999}
}
@book{Shalabh2009,
abstract = {Statistical Learning from a Regression Perspective considers statistical learning applications when interest centers on the conditional distribution of the response variable, given a set of predictors, and when it is important to characterize how the predictors are related to the response. As a first approximation, this is can be seen as an extension of nonparametric regression. Among the statistical learning procedures examined are bagging, random forests, boosting, and support vector machines. Response variables may be quantitative or categorical.},
author = {Shalabh},
booktitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
doi = {10.1111/j.1467-985x.2009.00614_2.x},
file = {:C$\backslash$:/Users/user-pc/Desktop/2016{\_}Book{\_}StatisticalLearningFromARegres.pdf:pdf},
isbn = {9783319440477},
issn = {09641998},
number = {4},
pages = {935--935},
title = {{Statistical Learning from a Regression Perspective}},
volume = {172},
year = {2009}
}
@article{Janssen2005,
author = {Janssen, Fanny and Peeters, Anna and Mackenbach, Johan P and Kunst, Anton E},
isbn = {1470-2738},
journal = {Journal of Epidemiology and Community Health},
number = {9},
pages = {775--781},
title = {{Relation between trends in late middle age mortality and trends in old age mortality—is there evidence for mortality selection?}},
volume = {59},
year = {2005}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hochreiter-1997-Long short-term memory.pdf:pdf},
isbn = {0899-7667},
issn = {08997667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
pmid = {9377276},
publisher = {MIT Press Journals},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Wilmoth1995,
author = {Wilmoth, John R},
isbn = {0032-4728},
journal = {Population studies},
number = {2},
pages = {281--295},
title = {{Are mortality rates falling at extremely high ages? An investigation based on a model proposed by Coale and Kisker}},
volume = {49},
year = {1995}
}
@article{Boucher2017,
abstract = {In Pay-As-You-Drive (PAYD) automobile insurance, the premium is fixed based on the distance traveled, while in usage-based insurance (UBI) the driving patterns of the policyholder are also considered. In those schemes, drivers who drive more pay a higher premium compared to those with the same characteristics who drive only occasionally, because the former are more exposed to the risk of accident. In this paper, we analyze the simultaneous effect of the distance traveled and exposure time on the risk of accident by using Generalized Additive Models (GAM).We carry out an empirical application and show that the expected number of claims (1) stabilizes once a certain number of accumulated distance-driven is reached and (2) it is not proportional to the duration of the contract, which is in contradiction to insurance practice. Finally, we propose to use a rating system that takes into account simultaneously exposure time and distance traveled in the premium calculation. We think that this is the trend the automobile insurance market is going to follow with the eruption of telematics data},
author = {Boucher, Jean-Philippe and C{\^{o}}t{\'{e}}, Steven and Guillen, Montserrat},
doi = {10.3390/risks5040054},
isbn = {2227-9091},
issn = {2227-9091},
journal = {Risks},
number = {4},
pages = {54},
title = {{Exposure as Duration and Distance in Telematics Motor Insurance Using Generalized Additive Models}},
url = {http://www.mdpi.com/2227-9091/5/4/54},
volume = {5},
year = {2017}
}
@incollection{Miller2006,
author = {Miller, Robert B},
booktitle = {Encyclopedia of Actuarial Science},
doi = {10.1002/9780470012505.tag010},
isbn = {9780470012505},
keywords = {Bayes,Gompertz,Makeham,Whittaker,graduation,kernel-smoothing,splines,weighted moving averages},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Graduation}},
url = {http://dx.doi.org/10.1002/9780470012505.tag010},
year = {2006}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Jauvin, Christian},
doi = {10.1162/153244303322533223},
isbn = {0262122413},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Artificial neural networks,Curse of dimensionality,Distributed representation,Statistical language modeling},
number = {6},
pages = {1137--1155},
shorttitle = {A neural probabilistic language model},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@article{Bourbeau2002,
abstract = {The level and age trajectory of mortality at advanced ages in Canada are not readily and exactly obtained, because of problems with the reliability of data on deaths and on population counts beyond a certain point in the official statistics. There are two ways to ensure nonetheless the termination of the life tables. One consists of finding ways to validate a sufficient number of unbiased high ages at death to produce an accurate measure with the extinct, or almost extinct, generation method. This paper presents the results of a systematic verification of ages at death and a preliminary estimation of centenarian mortality based on observations, which seems to lend credence to a leveling off of mortality rates at the highest ages for females. Another is to establish convincing evidence as to the pattern of survival at the very highest ages; mathematical techniques can then be used to generate the rates as an extension of mortality at ages 70 to 90 or 100. Historical data were used here to give an insight on what this pattern of survival could be. Contrary to what might have been expected, the progression of mortality remains pretty much exponential until the unavoidable erratic values corresponding to the few extreme observations are reached. This entails that whatever the nature of the selections that would produce a slowing down of the rate of increase of the rates at the highest ages, they did not express themselves conclusively a few centuries ago. {\textcopyright} 2002 Taylor {\&} Francis Group, LLC.},
author = {Bourbeau, Robert and Desjardins, Bertrand},
doi = {10.1080/10920277.2002.10596052},
isbn = {1092-0277},
issn = {10920277},
journal = {North American Actuarial Journal},
number = {3},
pages = {1--13},
title = {{Dealing with Problems in Data Quality for the Measurement of Mortality at Advanced Ages in Canada}},
volume = {6},
year = {2002}
}
@article{Girshick2015,
author = {Girshick, R},
journal = {arXiv},
shorttitle = {Fast R-CNN},
title = {{Fast R-CNN}},
volume = {arXiv:1504},
year = {2015}
}
@misc{reserve2011sr,
abstract = {Banks rely heavily on quantitative analysis and models in most aspects of financial decision making. They routinely use models for a broad range of activities, including underwriting credits; valuing exposures, instruments, and positions; measuring risk; managing and safeguarding client assets; determining capital and reserve adequacy; and many other activities. In recent years, banks have applied models to more complex products and with more ambitious scope, such as enterprise-wide risk measurement, while the markets in which they are used have also broadened and changed. Changes in regulation have spurred some of the recent developments, particularly the U.S. regulatory capital rules for market, credit, and operational risk based on the framework developed by the Basel Committee on Banking Supervision. Even apart from these regulatory considerations, however, banks have been increasing the use of data-driven, quantitative decision-making tools for a number of years. The expanding use of models in all aspects of banking reflects the extent to which models can improve business decisions, but models also come with costs. There is the direct cost of devoting resources to develop and implement models properly. There are also the potential indirect costs of relying on models, such as the possible adverse consequences (including financial loss) of decisions based on models that are incorrect or misused. Those consequences should be addressed by active management of model risk. This guidance describes the key aspects of effective model risk management. Section II explains the purpose and scope of the guidance, and Section III gives an overview of model risk management. Section IV discusses robust model development, implementation, and use. Section V describes the components of an effective validation framework. Section VI explains the salient features of sound governance, policies, and controls over model development, implementation, use, and validation. Section VII concludes.},
author = {{Federal Reserve}},
booktitle = {Board of Governors of the Federal Reserve System},
pages = {1--21},
title = {{Supervisory guidance on model risk management}},
url = {http://www.occ.treas.gov/news-issuances/bulletins/2011/bulletin-2011-12.html},
volume = {2011},
year = {2011}
}
@techreport{Zhao,
abstract = {Starting from the observation that Friedman's partial dependence plot has exactly the same formula as Pearl's back-door adjustment, we explore the possibility of extracting causal information from black-box models trained by machine learning algorithms. There are three requirements to make causal interpretations: a model with good predictive performance, some domain knowledge in the form of a causal diagram and suitable visualization tools. We provide several illustrative examples and find some interesting causal relations in these datasets.},
author = {Zhao, Qingyuan and Hastie, Trevor},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Hastie - Unknown - CAUSAL INTERPRETATIONS OF BLACK-BOX MODELS.pdf:pdf},
title = {{CAUSAL INTERPRETATIONS OF BLACK-BOX MODELS}}
}
@article{Humphrey1970,
author = {Humphrey, G T},
isbn = {0020-2681},
journal = {Journal of the Institute of Actuaries},
pages = {105--119},
title = {{Mortality at the oldest ages}},
year = {1970}
}
@inproceedings{Cooper-Williams2012,
address = {Cape Town},
author = {Cooper-Williams, Jason and Albertyn, Lize-Mari and Lewis, Paul},
booktitle = {2012 Actuarial Convention, Actuarial Society of South Africa },
title = {{MORTALITY IMPROVEMENTS IN SOUTH AFRICA}},
year = {2012}
}
@book{Wickham2016,
abstract = {The genus Artemisia consists of about 500 species, occurring throughout the world. Some very important drug leads have been discovered from this genus, notably artemisinin, the well known anti-malarial drug isolated from the Chinese herb Artemisia annua. The genus is also known for its aromatic nature and hence research has been focussed on the chemical compositions of the volatile secondary metabolites obtained from various Artemisia species. In the southern African region, A. afra is one of the most popular and commonly used herbal medicines. It is used to treat various ailments ranging from coughs and colds to malaria and diabetes. Although it is one of the most popular local herbal medicines, only limited scientific research, mainly focussing on the volatile secondary metabolites content, has been conducted on this species. The aim of this review was therefore to collect all available scientific literature published on A. afra and combine it into this paper. In this review, a general overview will be given on the morphology, taxonomy and geographical distribution of A. afra. The major focus will however be on the secondary metabolites, mainly the volatile secondary metabolites, which have been identified from this species. In addition all of the reported biological activities of the extracts derived from this species have been included as well as the literature on the pharmacology and toxicology. We aim at bringing together most of the available scientific research conducted on this species, which is currently scattered across various publications, into this review paper. {\textcopyright} 2008 SAAB.},
author = {Ginestet, Cedric},
booktitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
doi = {10.1111/j.1467-985x.2010.00676_9.x},
isbn = {3319242776},
issn = {0964-1998},
number = {1},
pages = {245--246},
publisher = {Springer},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
volume = {174},
year = {2011}
}
@article{CAIRNS,
abstract = {The analysis of national mortality trends is critically dependent on the quality of the population, exposures and deaths data that underpin death rates. We develop a framework that allows us to assess data reliability and to identify anomalies, illustrated, by way of example, using England and Wales population data. First, we propose a set of graphical diagnostics that help to pinpoint anomalies. Second, we develop a simple Bayesian model that allows us to quantify objectively the size of any anomalies. Two-dimensional graphical diagnostics and modelling techniques are shown to improve significantly our ability to identify and quantify anomalies. An important conclusion is that significant anomalies in population data can often be linked to uneven patterns of births of people in cohorts born in the distant past. In the case of England and Wales, errors of more than 9{\%} in the estimated size of some birth cohorts can be attributed to an uneven pattern of births. We propose methods that can use births data to improve estimates of the underlying population exposures. Finally, we consider the effect of anomalies on mortality forecasts and annuity values, and we find significant effects for some cohorts. Our methodology has general applicability to other sources of population data, such as the Human Mortality Database.},
author = {Cairns, Andrew J.G. and Blake, David and Dowd, Kevin and Kessler, Amy R.},
doi = {10.1111/rssa.12159},
issn = {1467985X},
journal = {Journal of the Royal Statistical Society. Series A: Statistics in Society},
keywords = {Baby boom,Cohort–births–deaths exposures methodology,Convexity adjustment ratio,Deaths,Graphical diagnostics,Population data},
number = {4},
pages = {975--1005},
title = {{Phantoms never die: living with unreliable population data}},
volume = {179},
year = {2016}
}
@misc{Brosse2020,
abstract = {Uncertainty quantification for deep learning is a challenging open problem. Bayesian statistics offer a mathematically grounded framework to reason about uncertainties; however, approximate posteriors for modern neural networks still require prohibitive computational costs. We propose a family of algorithms which split the classification task into two stages: representation learning and uncertainty estimation. We compare four specific instances, where uncertainty estimation is performed via either an ensemble of Stochastic Gradient Descent or Stochastic Gradient Langevin Dynamics snapshots, an ensemble of bootstrapped logistic regressions, or via a number of Monte Carlo Dropout passes. We evaluate their performance in terms of $\backslash$emph{\{}selective{\}} classification (risk-coverage), and their ability to detect out-of-distribution samples. Our experiments suggest there is limited value in adding multiple uncertainty layers to deep classifiers, and we observe that these simple methods strongly outperform a vanilla point-estimate SGD in some complex benchmarks like ImageNet.},
archivePrefix = {arXiv},
arxivId = {2001.08049},
author = {Brosse, Nicolas and Riquelme, Carlos and Martin, Alice and Gelly, Sylvain and Moulines, {\'{E}}ric},
eprint = {2001.08049},
keywords = {Bootstrap,Deep Neural Networks,Last Layer,Monte Carlo Dropout,Stochastic Gradient Langevin Dynamics,Uncertainties},
month = {jan},
title = {{On Last-Layer Algorithms for Classification: Decoupling Representation from Uncertainty Estimation}},
url = {http://arxiv.org/abs/2001.08049},
year = {2020}
}
@article{Borovykh2017,
abstract = {Forecasting financial time series using past observations has been a significant topic of interest. While temporal relationships in the data exist, they are difficult to analyze and predict accurately due to the non-linear trends and noise present in the series. We propose to learn these dependencies by a convolutional neural network. In particular the focus is on multivariate time series forecasting. Effectively, we use multiple financial time series as input in the neural network, thus conditioning the forecast of a time series x(t) on both its own history as well as that of a second (or third) time series y(t). Training a model on multiple stock series allows the network to exploit the correlation structure between these series so that the network can learn the market dynamics in shorter sequences of data. We show that long-term temporal dependencies in and between financial time series can be learned by means of a deep convolutional neural network based on the WaveNet model [2]. The network makes use of dilated convolutions applied to multiple time series so that the receptive field of the network is wide enough to learn both short and long-term dependencies. The architecture includes batch normalization and uses a 1 × k convolution with parametrized skip connections from the input time series as well as the time series we condition on, in this way learning long-term interdependencies in an efficient manner [1]. This improves the forecast, while at the same time limiting the requirement for a long historical price series and reducing the noise. Knowing the strong performance of CNNs on classification problems we show that they can be applied successfully to forecasting financial time series, without the need of large samples of data. We compare the performance of the WaveNet model to a state-of-the-art fully convolutional network (FCN), and an autoregressive model popular in econometrics and show that our model is much better able to learn important dependencies in between financial time series resulting in a more robust and accurate forecast.},
archivePrefix = {arXiv},
arxivId = {1703.04691},
author = {Borovykh, Anastasia and Bohte, Sander and Oosterlee, Cornelis W.},
doi = {10.1007/978-3-319-68612-7},
eprint = {1703.04691},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Borovykh, Bohte, Oosterlee - 2017 - Conditional time series forecasting with convolutional neural networks.pdf:pdf},
isbn = {9783319686110},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural network,Financial time series},
pages = {729--730},
title = {{Conditional time series forecasting with convolutional neural networks}},
volume = {10614 LNCS},
year = {2017}
}
@incollection{McCulloch2017,
abstract = {Theoretical neurophysiology rests on certain cardinal assumptions. The nervous system is a net of neurons, each having a soma and an axon. Their adjunctions, or synapses, are always between the axon of one neuron and the soma of another. At any instant a neuron has some threshold, which excitation must exceed to initiate an impulse. This, except for the fact and the time of its occurrence, is determined by the neuron, not by the excitation. From the point of excitation the impulse is propagated to all parts of the neuron. The velocity along the axon varies directly with its diameter, from less than one meter per second in thin axons, which are usually short, to more than 150 meters per second in thick axons, which are usually long. The time for axonal conduction is consequently of little importance in determining the time of arrival of impulses at points unequally remote from the same source. Excitation across synapses occurs predominantly from axonal terminations to somata. It is still a moot point whether this depends upon irreciprocity of individual synapses or merely upon prevalent anatomical configurations. To suppose the latter requires no hypothesis ad hoc and explains known exceptions, but any assumption as to cause is compatible with the calculus to come. No case is known in which excitation through a single synapse has elicited a nervous impulse in any neuron, whereas any neuron may be excited by impulses arriving at a sufficient number of neighboring synapses within the period of latent addition, which lasts less than one quarter of a millisecond. Observed temporal summation of impulses at greater intervals is impossible for single neurons and empirically depends upon structural properties of the net. Between the arrival of impulses upon a neuron and its own propagated impulse there is a synaptic delay of more than half a millisecond. During the first part of the nervous impulse the neuron is absolutely refractory to any stimulation. Thereafter its excitability returns rapidly, in some cases reaching a value above normal from which it sinks again to a subnormal value, whence it returns slowly to normal. Frequent activity augments this subnormality. Such specificity as is possessed by nervous impulses depends solely upon their time and place and not on any other specificity of nervous energies. Of late only inhibition. has been seriously adduced to contravene this thesis. Inhibition is the termination or prevention of the activity of one group of neurons by concurrent or antecedent activity of a second group. Until recently this could be explained on the supposition that previous activity of neurons of the second group might so raise the thresholds of internuncial neurons that they could no longer be excited by neurons of the first group, whereas the impulses of the first group must sum with the impulses of these internuncials to excite the now inhibited neurons. Today, some inhibitions have been shown to consume less than one millisecond. This excludes internuncials and requires synapses 94through which impulses inhibit that neuron which is being stimulated by impulses through other synapses. As yet experiment has not shown whether the refractoriness is relative or absolute. We will assume the latter and demonstrate that the difference is immaterial to our argument. Either variety of refractoriness can be accounted for in either of two ways. The “inhibitory synapse�? may be of such a kind as to produce a substance which raises the threshold of the neuron, or it may be so placed that the local disturbance produced by its excitation opposes the alteration induced by the otherwise excitatory synapses. Inasmuch as position is already known to have such effects in the case of electrical stimulation, the first hypothesis is to be excluded unless and until it be substantiated, for the second involves no new hypothesis. We have, then, two explanations of inhibition based on the same general premises, differing only in the assumed nervous nets and, consequently, in the time required for inhibition. Hereafter we shall refer to such nervous nets as equivalent in the extended sense. Since we are concerned with properties of nets which are invariant under equivalence, we may make the physical assumptions which are most convenient for the calculus.},
author = {McCulloch, Warren S. and Pitts, Walter H.},
booktitle = {Systems Research for Behavioral Science: A Sourcebook},
doi = {10.1007/BF02478259},
isbn = {9781351487214},
issn = {00074985},
month = {jan},
pages = {93--96},
pmid = {2185863},
publisher = {Taylor and Francis},
title = {{A logical calculus of the ideas immanent in nervous activity}},
year = {2017}
}
@article{Sadie1951,
author = {Sadie, J L},
isbn = {1813-6982},
journal = {South African Journal of Economics},
number = {4},
pages = {361--369},
title = {{DIFFERENTIAL MORTALITY IN SOUTH AFRICA*}},
volume = {19},
year = {1951}
}
@book{Taylor2012,
author = {Taylor, Gregory},
isbn = {1461545838},
publisher = {Springer Science {\&} Business Media},
title = {{Loss reserving: an actuarial perspective}},
volume = {21},
year = {2012}
}
@article{Sjolander2018,
abstract = {Measures of causal effects play a central role in epidemiology. A wide range of measures exist, which are designed to give relevant answers to substantive epidemiological research questions. However, due to mathematical convenience and software limitations most studies only report odds ratios for binary outcomes and hazard ratios for time-to-event outcomes. In this paper we show how logistic regression models and Cox proportional hazards regression models can be used to estimate a wide range of causal effect measures, with the R-package stdReg. For illustration we focus on the attributable fraction, the number needed to treat and the relative excess risk due to interaction. We use two publicly available data sets, so that the reader can easily replicate and elaborate on the analyses. The first dataset includes information on 487 births among 188 women, and the second dataset includes information on 2982 women diagnosed with primary breast cancer.},
author = {Sj{\"{o}}lander, Arvid},
doi = {10.1007/s10654-018-0375-y},
file = {:C$\backslash$:/R/refas/sjolander2018estimation.pdf:pdf},
isbn = {0123456789},
issn = {15737284},
journal = {European Journal of Epidemiology},
keywords = {Attributable fraction,Causal effect,Cox proportional hazards regression,Logistic regression,Number needed to treat,Relative excess risk due to interaction},
number = {9},
pages = {847--858},
publisher = {Springer Netherlands},
title = {{Estimation of causal effect measures with the R-package stdReg}},
url = {https://doi.org/10.1007/s10654-018-0375-y},
volume = {33},
year = {2018}
}
@inproceedings{Loshchilov2019,
abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14{\%} and 16.21{\%}, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset.},
archivePrefix = {arXiv},
arxivId = {1608.03983},
author = {Loshchilov, Ilya and Hutter, Frank},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1608.03983},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Loshchilov, Hutter - Unknown - SGDR STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS.pdf:pdf},
title = {{SGDR: Stochastic gradient descent with warm restarts}},
url = {https://github.com/loshchil/SGDR},
year = {2019}
}
@book{Speyer2008,
author = {Speyer, Jason L. and Chung, Walter H.},
booktitle = {Stochastic Processes, Estimation, and Control},
doi = {10.1137/1.9780898718591.ch5},
file = {:C$\backslash$:/Users/user-pc/Desktop/2016{\_}Book{\_}StochasticProcessesAndCalculus.pdf:pdf},
isbn = {9783319234274},
pages = {153--196},
title = {{5. Stochastic Processes and Stochastic Calculus}},
year = {2008}
}
@article{H.L.Poon2019,
author = {{H. L. Poon}, Jacky and Poon, Jacky H.L.},
doi = {10.3390/risks7030095},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H. L. Poon - 2019 - Penalising Unexplainability in Neural Networks for Predicting Payments per Claim Incurred.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Poon - 2019 - Penalising unexplainability in neural networks for predicting payments per claim incurred.pdf:pdf},
journal = {Risks},
keywords = {Actuarial,Granular models,Loss reserving,Neural networks,Payments per claim incurred,Risk pricing},
month = {sep},
number = {3},
pages = {95},
publisher = {MDPI AG},
title = {{No Title}},
url = {https://www.mdpi.com/2227-9091/7/3/95},
volume = {7},
year = {2019}
}
@article{Keiding1990,
author = {Keiding, Niels},
isbn = {1364-503X},
journal = {Philosophical Transactions of the Royal Society of London. Series A: Physical and Engineering Sciences},
number = {1627},
pages = {487--509},
title = {{Statistical inference in the Lexis diagram}},
volume = {332},
year = {1990}
}
@article{Budd1991,
abstract = {The United Kingdom's Old Age Pensions Act of 1908 instituted means-tested, non-contributory pensions for men and women aged 70 or over. The pension and the lack of civil registration of births before 1864 caused many Irish to exaggerate their ages in the Census of 1911. In this paper a linked sample from the manuscript censuses of 1901 and 1911 is used to estimate the magnitude and determinants of this age misrepresentation. Our results show three types of age discrepancies: those associated with a significant reduction in age-heaping; those associated with efforts to obtain a pension before age 70; and some apparent age-exaggeration unconnected with the Old Age Pension. {\textcopyright} 1991 Taylor {\&} Francis Group, LLC.},
author = {Budd, John W. and Guinnane, Timothy},
doi = {10.1080/0032472031000145666},
isbn = {0032-4728},
issn = {14774747},
journal = {Population Studies},
number = {3},
pages = {497--518},
title = {{Intentional age-misreporting, age-heaping, and the 1908 old age pensions act in Ireland*}},
volume = {45},
year = {1991}
}
@article{Weidner2016,
abstract = {This paper presents pricing innovations to German car insurance. The purpose is to provide an effective approach to adapting actuarial pricing decision to incorporate telematic data, which differs substantially from established tariff criteria in complexity and volume. A vehicle mobility model and a real-world sample of driving profiles form the input into the analysis. We propose an allocation of the driving profiles based on velocity and acceleration parameters to specific driving styles for evaluating the driving behaviour to subsequently enable discounts or surcharges on the premiums to obtain usage-based insurance premiums. The result is highly relevant for actuaries, who calculate the tariffs, but also for managers, as they have to make a pricing decision.},
author = {Weidner, Wiltrud and Transchel, Fabian W.G. and Weidner, Robert},
doi = {10.1017/s1748499516000130},
edition = {09/13},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Weidner-2016-Telematic driving profile classif.pdf:pdf},
isbn = {1748-4995},
issn = {1748-4995},
journal = {Annals of Actuarial Science},
keywords = {Car insurance,Car insurance Pricing innovations Telematic Evalua,Driving styles,Evaluation of driving behaviour,Pricing innovations,Telematic},
number = {2},
pages = {213--236},
publisher = {Cambridge University Press},
shorttitle = {Telematic driving profile classification in car in},
title = {{Telematic driving profile classification in car insurance pricing}},
url = {https://www.cambridge.org/core/article/telematic-driving-profile-classification-in-car-insurance-pricing/E3EDF4CB87B1E4FC07B93E4E6251851D},
volume = {11},
year = {2017}
}
@article{Mesle2002,
author = {Mesl{\'{e}}, France and Vallin, Jacques},
isbn = {1634-2941},
journal = {Population (english edition)},
number = {4},
pages = {601--629},
title = {{Improving the Accuracy of Life Tables for the Oldest Old}},
volume = {57},
year = {2002}
}
@inproceedings{Riedmiller1993,
abstract = {A new learning algorithm for multilayer feedforward networks, RPROP, is proposed. To overcome the inherent disadvantages of pure gradient-descent, RPROP performs a local adaptation of the weight-updates according to the behavior of the error function. In substantial difference to other adaptive techniques, the effect of the RPROP adaptation process is not blurred by the unforeseeable influence of the size of the derivative but only dependent on the temporal behavior of its sign. This leads to an efficient and transparent adaptation process. The promising capabilities of RPROP are shown in comparison to other well-known adaptive techniques.},
author = {Riedmiller, Martin and Braun, Heinrich},
booktitle = {1993 IEEE International Conference on Neural Networks},
doi = {10.1109/icnn.1993.298623},
isbn = {0780312007},
pages = {586--591},
publisher = {IEEE},
title = {{Direct adaptive method for faster backpropagation learning: The RPROP algorithm}},
year = {1993}
}
@techreport{SolvencyAssessmentandManagement:Pillar1QuantitativeRequirementsSubCommittee,
author = {{Solvency Assessment and Management: Pillar 1 – Quantitative Requirements Sub Committee}},
language = {en},
pages = {52},
title = {{Position Paper 94 - Interest Rate Risk}},
url = {https://www.fsca.co.za/Regulated Entities/SAM DOCUMENTS/Position Paper 94 (v 4) FINAL.pdf},
volume = {4}
}
@article{Clevert,
abstract = {We introduce the “exponential linear unit” (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10{\%} classification error for a single crop, single model network.},
archivePrefix = {arXiv},
arxivId = {1511.07289},
author = {Clevert, Djork Arn{\'{e}} and Unterthiner, Thomas and Hochreiter, Sepp},
eprint = {1511.07289},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
title = {{Fast and accurate deep network learning by exponential linear units (ELUs)}},
year = {2016}
}
@book{Resnick1997,
author = {Resnick, Paul and Varian, Hal R.},
booktitle = {Communications of the ACM},
doi = {10.1145/245108.245121},
file = {:C$\backslash$:/Users/user-pc/Desktop/2016{\_}Book{\_}RecommenderSystems.pdf:pdf},
isbn = {9783319296579},
issn = {00010782},
number = {3},
pages = {56--58},
title = {{Recommender Systems}},
volume = {40},
year = {1997}
}
@incollection{Vaupel2000,
author = {Vaupel, James W and Romo, V Canudas},
booktitle = {Optimization, Dynamics, and Economic Analysis},
isbn = {3790812951},
pages = {345--352},
publisher = {Springer},
title = {{How mortality improvement increases population growth}},
year = {2000}
}
@article{Gabrielli2018a,
abstract = {The aim of this project is to develop a stochastic simulation machine that generates individual claims histories of non-life insurance claims. This simulation machine is based on neural networks to incorporate individual claims feature information. We provide a fully calibrated stochastic scenario generator that is based on real non-life insurance data. This stochastic simulation machine allows everyone to simulate their own synthetic insurance portfolio of individual claims histories and back-test thier preferred claims reserving method.},
author = {Gabrielli, Andrea and W{\"{u}}thrich, Mario V.},
doi = {10.3390/risks6020029},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gabrielli, W{\"{u}}thrich - 2018 - An individual claims history simulation machine.pdf:pdf},
issn = {22279091},
journal = {Risks},
keywords = {Chain-ladder,Claims cash flows,Claims reserving,Claims simulation,Individual claims,Individual claims covariates,Individual claims features,Loss reserving,Micro-level stochastic reserving,Neural network reserving},
month = {jun},
number = {2},
publisher = {MDPI AG},
title = {{An individual claims history simulation machine}},
volume = {6},
year = {2018}
}
@inproceedings{Viola2001,
abstract = {The IoT enabled with smart security system presents security to the home and also providing a facility to the user where one can continuously monitor the surrounding parameters inside the house (like temperature, smoke and light intensity) and can control them by collection and exchange of data between the things for example switching on/off devices (like fan and light based on these parameters). In recent days when ever house is locked the break-ins number has been increased enormously. So in order to provide security to the home this presented paper is helpful. When the intruders enters into the house, image of the intruder is captured by the system, even if intruder escapes Police need to caught the intruder to recover the stolen things which needs the picture of the intruder to the police. The planned system captures the picture of the intruder and sends it to the authorized mail through internet over Simple Mail Transfer Protocol (SMTP). So security to the home is provided more effectively in the smart way of communicating the things. Home appliances are smartly automated to reduce the human effort for intelligent decisions with the help of Internet Of Things (IoT). This compact and lightweight product is designed to provide security and to control home appliances in the house by the owner through IoT servers. Microcontroller used here is Raspberry Pi3 for all processing and controlling operations. Various sensors such as LM35 a temperature sensor, Light Dependent Resistor, PIR sensor with a magnetic door switch, smoke sensor are interfaced to pi General Purpose Input Output port pins through Analog to Digital Converter module along with a camera and LAN connection is interfaced to the pi board.},
author = {Chandra, M. L.Ravi and Kumar, B. Varun and Sureshbabu, B.},
booktitle = {2017 International Conference on Energy, Communication, Data Analytics and Soft Computing, ICECDS 2017},
doi = {10.1109/ICECDS.2017.8389630},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Viola-2001-Rapid object detection using a boos.pdf:pdf},
isbn = {9781538618868},
keywords = {IoT,Raspberry Pi3 Microcontroller,SMTP,Smart security sensors},
pages = {1193--1197},
publisher = {IEEE},
shorttitle = {Rapid object detection using a boosted cascade of },
title = {{IoT enabled home with smart security}},
volume = {1},
year = {2018}
}
@article{Wilmoth2006,
author = {Wilmoth, John R and Shkolnikov, Vladimir},
title = {{Human mortality database}},
year = {2006}
}
@article{Delong2020a,
author = {Delong, Lukasz and Lindholm, Mathias and Wuthrich, Mario V.},
doi = {10.2139/ssrn.3582398},
journal = {SSRN Electronic Journal},
keywords = {Collective Reserving using Individual Claims Data,IBNR claims,Lukasz Delong,Mario V. Wuthrich,Mathias Lindholm,RBNS claims,SSRN,chain-ladder method,claims reserving,general insurance,individual claims data,micro-level reserving,neural networks,over-dispersed Poisson model},
month = {may},
publisher = {Elsevier BV},
title = {{Collective Reserving using Individual Claims Data}},
url = {https://papers.ssrn.com/abstract=3582398},
year = {2020}
}
@article{Secretariat2007,
author = {Secretariat, U N},
journal = {PLACE: the Population Division of the Department of Economic and Social Affairs of the UN Secretariat [http://earthtrends. wri. org/text/population-health/variable-379. html]},
title = {{World population prospects: the 2006 revision}},
year = {2007}
}
@article{Thomson2006,
abstract = {This paper proposes a categorisation of the models used in actuarial science. It illustrates the application of that categorisation by using it to classify numerous such models. It is suggested that this categorisation, together with the illustrative classification, may be used as a typology for the classification of other such models, and that this typology may be found useful as a candidate exemplar, or as a basis for further refinement, in the discourse of actuarial science.},
author = {Thomson, RJ},
doi = {10.4314/saaj.v6i1.24505},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomson - 2006 - A TYPOLOGY OF MODELS USED IN ACTUARIAL SCIENCE.pdf:pdf},
issn = {1680-2179},
journal = {South African Actuarial Journal},
keywords = {Actuarial models,categorisation,classification,typology},
number = {1},
pages = {19--36},
title = {{A typology of models used in actuarial science}},
volume = {6},
year = {2006}
}
@article{Rougier2007,
abstract = {We consider inference based on ensembles of climate model evaluations, and contrast the Monte Carlo approach, in which the evaluations are selected at random from the model-input space, with a more overtly statistical approach using emulators and experimental design. {\textcopyright} 2007 The Royal Society.},
author = {Rougier, Jonathan and Sexton, David M.H.},
doi = {10.1098/rsta.2007.2071},
file = {:C$\backslash$:/R/refas/rougier2007inference.pdf:pdf},
issn = {1364503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Designed ensemble,Emulator,Importance sampling,Monte carlo ensemble,Screening,Uncertainty},
number = {1857},
pages = {2133--2143},
title = {{Inference in ensemble experiments}},
volume = {365},
year = {2007}
}
@article{LeCun2015a,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {1807.07987},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {1807.07987},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/LeCun-2015-Deep learning.pdf:pdf;:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lecun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
isbn = {1476-4687},
issn = {14764687},
journal = {Nature},
month = {may},
number = {7553},
pages = {436--444},
pmid = {26017442},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{Wang2013,
author = {Wang, Haidong and Dwyer-Lindgren, Laura and Lofgren, Katherine T and Rajaratnam, Julie Knoll and Marcus, Jacob R and Levin-Rector, Alison and Levitz, Carly E and Lopez, Alan D and Murray, Christopher J L},
isbn = {0140-6736},
journal = {The Lancet},
number = {9859},
pages = {2071--2094},
title = {{Age-specific and sex-specific mortality in 187 countries, 1970–2010: a systematic analysis for the Global Burden of Disease Study 2010}},
volume = {380},
year = {2013}
}
@article{Dorrington1999a,
author = {Dorrington, Rob and Bradshaw, Debbie and Wegner, Trevor},
journal = {Cape Town: South African Medical Research Council},
title = {{Estimates of the level and shape of mortality rates in South Africa around 1985 and 1990 derived by applying indirect demographic techniques to reported deaths}},
year = {1999}
}
@article{Kahn2011,
author = {Kahn, Kathleen},
isbn = {0197-5897},
journal = {Journal of public health policy},
pages = {S30--S36},
title = {{Population health in South Africa: dynamics over the past two decades}},
year = {2011}
}
@inproceedings{Hill1984,
author = {Hill, Kenneth},
publisher = {[Unpublished] 1981. Paper presented at Intrernational Union for the Scientific Study of Population Committee on Factors Affecting Mortality and the Length of Life Seminar on Methodology and Data Collection in Mortality Studies Dakar Senegal July 1981.},
title = {{An evaluation of indirect methods for estimating mortality}},
year = {1984}
}
@article{Andreev2004,
abstract = {In many countries population estimates are unreliable at higher ages. In this article a method for producing an independent estimate of population aged 90+ from data on deaths and population estimates at lower ages is developed. The method builds on an indirect mortality estimate from deaths only and on an estimate of rate of mortality change. Theoretical foundation and bias expected on application of this procedure to the real data are discussed as well. Testing of this method on accurate demographic data shows its superiority over available procedures. The method has been applied to the evaluation of size of population 90+ in the census 2000 of the United States. The results show a high degree of agreement between two estimates, but the possibility of slight overestimation of males in census data cannot be completely ruled out. To facilitate the application of this method, a computer program is provided as well. {\textcopyright} 2004 Max-Planck-Gesellschaft.},
author = {Andreev, Kirill F.},
doi = {10.4054/DemRes.2004.11.9},
issn = {14359871},
journal = {Demographic Research},
pages = {235--262},
title = {{A method for estimating size of population aged 90 and over with application to the 2000 U.S. census data}},
volume = {11},
year = {2004}
}
@article{Rosenwaike1983,
abstract = {This study attempts to verify age reporting on the death certificate for the "extreme aged" population and to evaluate the accuracy of recent death rates for this group in light of the findings. In addition, methods used and problems encountered in carrying out a record linkage study, particularly a low match rate, are identified. A sample of more than three thousand death records was selected from those filed for decedents age 85 and over in Pennsylvania and New Jersey in the 1968 to 1972 period. Death certificates of 53 percent of whites and 30 percent of nonwhites were linked to the 1900 U.S. Census. A comparison of age on the death certificate with the age reported for the same individual in the census record showed a high level of agreement for whites, except at ages 100 and over; for nonwhites, however, age agreement levels were substantially lower. Within racial groups, there was little difference by sex in agreement on age. These results, corroborating those of earlier studies, make it clear that nonwhite mortality at the oldest ages has been consistently understated in official statistics. {\textcopyright} 1983 Population Association of America.},
author = {Rosenwaike, Ira and Logue, Barbara},
doi = {10.2307/2061120},
isbn = {0070-3370},
issn = {00703370},
journal = {Demography},
number = {4},
pages = {569--585},
title = {{Accuracy of death certificate ages for the extreme aged}},
volume = {20},
year = {1983}
}
@book{Denuit2019,
author = {Denuit, Michel and Hainaut, Donatien and Trufin, Julien},
doi = {10.1007/978-3-030-25827-6},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Denuit, Hainaut, Trufin - 2019 - Effective Statistical Learning Methods for Actuaries I.pdf:pdf},
isbn = {978-3-030-25826-9},
title = {{Effective Statistical Learning Methods for Actuaries I}},
url = {http://link.springer.com/10.1007/978-3-030-25827-6},
year = {2019}
}
@article{Chen2020,
abstract = {Following the EU Gender Directive, that obliges insurance companies to charge the same premium to policyholders of different genders, we address the issue of calculating solvency capital requirements (SCRs) for pure endowments and annuities issued to mixed portfolios. The main theoretical result is that, if the unisex fairness principle is adopted for the unisex premium, the SCR at issuing time of the mixed portfolio calculated with unisex survival probabilities is greater than the sum of the SCRs of the gender-based subportfolios. Numerical results show that for pure endowments the gap between the two is negligible, but for lifetime annuities the gap can be as high as 3-4{\%}. We also analyze some conservative pricing procedures that deviate from the unisex fairness principle, and find that they lead to SCRs that are lower than the sum of the gender-based SCRs because the policyholders are overcharged at issuing time.},
author = {Chen, An and Guillen, Montserrat and Vigna, Elena},
doi = {10.1017/asb.2018.11},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Guillen, Vigna - 2020 - SOLVENCY REQUIREMENT IN A UNISEX MORTALITY MODEL.pdf:pdf},
issn = {17831350},
journal = {ASTIN Bulletin},
keywords = {Gender Directive,SCR,gender discrimination,life insurance pricing,life table,risk margin,unisex fairness principle,unisex tariff},
number = {3},
pages = {1219--1243},
title = {{Solvency requirement in a unisex mortality model}},
url = {https://doi.org/10.1017/asb.2018.11},
volume = {48},
year = {2018}
}
@article{Januschowski2020,
abstract = {Classifying forecasting methods as being either of a “machine learning” or “statistical” nature has become commonplace in parts of the forecasting literature and community, as exemplified by the M4 competition and the conclusion drawn by the organizers. We argue that this distinction does not stem from fundamental differences in the methods assigned to either class. Instead, this distinction is probably of a tribal nature, which limits the insights into the appropriateness and effectiveness of different forecasting methods. We provide alternative characteristics of forecasting methods which, in our view, allow to draw meaningful conclusions. Further, we discuss areas of forecasting which could benefit most from cross-pollination between the ML and the statistics communities.},
author = {Januschowski, Tim and Gasthaus, Jan and Wang, Yuyang and Salinas, David and Flunkert, Valentin and Bohlke-Schneider, Michael and Callot, Laurent},
doi = {10.1016/j.ijforecast.2019.05.008},
file = {:C$\backslash$:/Users/user-pc/Desktop/10.1016@j.ijforecast.2019.05.008.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
number = {1},
pages = {167--177},
publisher = {Elsevier B.V.},
title = {{Criteria for classifying forecasting methods}},
url = {https://doi.org/10.1016/j.ijforecast.2019.05.008},
volume = {36},
year = {2020}
}
@article{Thatcher1999,
author = {Thatcher, A Roger},
isbn = {1467-985X},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
number = {1},
pages = {5--43},
title = {{The long‐term pattern of adult mortality and the highest attained age}},
volume = {162},
year = {1999}
}
@misc{Cairns2019b,
abstract = {We introduce a new modelling framework to explain socio-economic differences in mortality in terms of an affluence index that combines information on individual wealth and income. The model is illustrated using data on older Danish males over the period 1985-2012 reported in the Statistics Denmark national register database. The model fits the historical mortality data well, captures their key features, generates smoothed death rates that allow us to work with a larger number of sub-groups than has previously been considered feasible, and has plausible projection properties.},
author = {Cairns, Andrew J.G. and Kallestrup-Lamb, Malene and Rosenskjold, Carsten and Blake, David and Dowd, Kevin},
booktitle = {ASTIN Bulletin},
doi = {10.1017/asb.2019.14},
issn = {17831350},
keywords = {Affluence index,CBD-X model,Danish mortality data,forward correlation term structure,gravity model,multipopulation mortality modelling},
month = {sep},
number = {3},
pages = {555--590},
publisher = {Cambridge University Press},
title = {{Modelling socio-economic differences in mortality using a new affluence index}},
volume = {49},
year = {2019}
}
@article{croforum_model,
author = {Forum, C R O},
title = {{Leading Practices in Model Management}},
year = {2017}
}
@article{Wilmoth2012,
abstract = {Mortality estimates for many populations are derived using model life tables, which describe typical age patterns of human mortality. We propose a new system of model life tables as a means of improving the quality and transparency of such estimates. A flexible two-dimensional model was fitted to a collection of life tables from the Human Mortality Database. The model can be used to estimate full life tables given one or two pieces of information: child mortality only, or child and adult mortality. Using life tables from a variety of sources, we have compared the performance of new and old methods. The new model outperforms the Coale-Demeny and UN model life tables. Estimation errors are similar to those produced by the modified Brass logit procedure. The proposed model is better suited to the practical needs of mortality estimation, since both input parameters are continuous yet the second one is optional.},
annote = {Wilmoth, John
Zureick, Sarah
Canudas-Romo, Vladimir
Inoue, Mie
Sawyer, Cheryl
eng
R01 AG011552/AG/NIA NIH HHS/
R24 HD042854/HD/NICHD NIH HHS/
England
2011/12/14 06:00
Popul Stud (Camb). 2012 Mar;66(1):1-28. doi: 10.1080/00324728.2011.611411. Epub 2011 Dec 13.},
author = {Wilmoth, J and Zureick, S and Canudas-Romo, V and Inoue, M and Sawyer, C},
doi = {10.1080/00324728.2011.611411},
isbn = {1477-4747 (Electronic)
0032-4728 (Linking)},
journal = {Popul Stud (Camb)},
keywords = {*Life Expectancy,*Life Tables,Adolescent,Adult,Age Factors,Aged,Aged, 80 and over,Child,Child, Preschool,Databases, Factual,Female,Humans,Infant,Infant, Newborn,Logistic Models,Male,Middle Aged,Models, Statistical,Mortality/*trends,Probability,Risk Assessment,Statistics as Topic/*methods,Young Adult},
number = {1},
pages = {1--28},
pmid = {22150635},
title = {{A flexible two-dimensional mortality model for use in indirect estimation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22150635},
volume = {66},
year = {2012}
}
@misc{Dorrington2015,
address = {University of Cape Town},
author = {Dorrington, R},
publisher = {Centre for Actuarial Research},
title = {{Migration data - South Africa}},
year = {2015}
}
@book{Gelman2007,
abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces and demonstrates a wide variety of models, at the same time instructing the reader in how to fit these models using freely available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen in the authors' own applied research, with pro-gramming code provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental vari-ables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout. University. He has published more than 150 articles in statistical theory, methods, and computation and in applications areas including decision analysis, survey sampling, polit-ical science, public health, and policy. His other books are Bayesian Data Analysis (1995, second edition 2003) and Teaching Statistics: A Bag of Tricks (2002).},
author = {Hilbe, Joseph M.},
booktitle = {Journal of Statistical Software},
doi = {10.18637/jss.v030.b03},
issn = {1548-7660},
number = {Book Review 3},
publisher = {Cambridge University Press New York, NY, USA},
title = {{Data Analysis Using Regression and Multilevel/Hierarchical Models}},
volume = {30},
year = {2009}
}
@article{jarvis2017ersatz,
abstract = {This paper describes how statistical methods can be tested on computer-generated data from known models. We explore bias and percentile tests in detail, illustrating these with examples based on insurance claims and financial time series.},
author = {Jarvis, S. and Sharpe, J. and Smith, A. D.},
doi = {10.1017/s1357321717000137},
issn = {1357-3217},
journal = {British Actuarial Journal},
number = {3},
pages = {490--521},
publisher = {Cambridge University Press},
title = {{Ersatz model tests}},
volume = {22},
year = {2017}
}
@misc{Wuthrich2018d,
author = {W{\"{u}}thrich, M},
edition = {Version 1},
number = {1 July},
title = {{v-a Heatmap Simulation Machine}},
url = {https://people.math.ethz.ch/{~}wueth/simulation.html},
volume = {2018},
year = {2018}
}
@article{Courbage1979,
author = {Courbage, Youssef and Fargues, Philippe},
isbn = {0032-4728},
journal = {Population Studies},
number = {1},
pages = {165--180},
title = {{A method for deriving mortality estimates from incomplete vital statistics}},
volume = {33},
year = {1979}
}
@article{Debon2006,
abstract = {The nonparametric graduation of mortality data aims to estimate death rates by carrying out a smoothing of the crude rates obtained directly from original data. The main difference with regard to parametric models is that the assumption of an age-dependent function is unnecessary, which is advantageous when the information behind the model is unknown, as one cause of error is often the choice of an inappropriate model. This paper reviews the various alternatives and presents their application to mortality data from the Valencia Region, Spain. The comparison leads us to the conclusion that the best model is a smoothing by means of Generalised Additive Models (GAM) with splines. The most interesting part of this paper is the development of a plan that can be applied to mortality data for a wide range of age groups in any geographical area, allowing the most appropriate table to be chosen for the data in hand. /// La graduation non param{\'{e}}trique des donn{\'{e}}es sur la mortalit{\'{e}} envisage d'estimer les differents mesures de mortalit{\'{e}}, en effectuant un lissage des mesures brutes directement obtenues {\`{a}} partir des donn{\'{e}}es originelles. La diff{\'{e}}rence fondamentale avec les mod{\`{e}}les param{\'{e}}triques est qu'il n'est pas n{\'{e}}cessaire de supposer une fonction d{\'{e}}pendant de l'{\^{a}}ge, ce qui repr{\'{e}}sente un avantage lorsque l'on n'a pas d'information sur le mod{\`{e}}le sous-jacent, puisqu'une source d'erreur en est souvent le choix inad{\'{e}}quat. Dans ce travail, nous en avons examin{\'{e}} les diff{\'{e}}rentes alternatives et nous y avons montr{\'{e}} leur application {\`{a}} des donn{\'{e}}es sur la mortalit{\'{e}} dans la Region de Valencia, Espagne. Nous concluons d'apr{\`{e}}s cette comparaison que le meilleur mod{\`{e}}le en est le rabotage par des mod{\`{e}}les additifs g{\'{e}}n{\'{e}}ralis{\'{e}}s (GAM) avec des "splines". L'int{\'{e}}r{\^{e}}t principal de notre travail est le d{\'{e}}veloppement d'un plan qui peut {\^{e}}tre appliqu{\'{e}} {\`{a}} des donn{\'{e}}es sur la mortalit{\'{e}} pour une large plage d'{\^{a}}ge dans n'importe quel domaine g{\'{e}}ographique, de sorte qu'il nous permet de choisir le tableau le plus ad{\'{e}}quat {\`{a}} l'exp{\'{e}}rience concern{\'{e}}e.},
author = {Deb{\'{o}}n, Ana and Montes, Francisco and Sala, Ram{\'{o}}n},
doi = {10.2307/25472704},
isbn = {03067734},
journal = {International Statistical Review / Revue Internationale de Statistique},
number = {2},
pages = {215--233},
publisher = {International Statistical Institute (ISI)},
title = {{A Comparison of Nonparametric Methods in the Graduation of Mortality: Application to Data from the Valencia Region (Spain)}},
url = {http://www.jstor.org/stable/25472704},
volume = {74},
year = {2006}
}
@article{Dorrington2007,
author = {Dorrington, R E and Tootla, S},
journal = {South African Actuarial Journal},
pages = {161--184},
title = {{South African annuitant standard mortality tables 1996–2000 (SAIML98 and SAIFL98)}},
volume = {7},
year = {2007}
}
@article{Mack1993,
abstract = {A distribution-free formula for the standard error of chain ladder reserve estimates is derived and compared to the results of some parametric methods using a numerical example.},
author = {Mack, Thomas},
doi = {10.2143/ast.23.2.2005092},
file = {:C$\backslash$:/Users/user-pc/Downloads/213.pdf:pdf},
isbn = {0515-0361},
issn = {0515-0361},
journal = {ASTIN Bulletin},
keywords = {chain ladder,claims reserving,standard error},
number = {2},
pages = {213--225},
title = {{Distribution-free Calculation of the Standard Error of Chain Ladder Reserve Estimates}},
volume = {23},
year = {1993}
}
@book{Chambers1991,
abstract = {Statistical Models in S extends the S language to fit and analyze a variety of statistical models, including analysis of variance, generalized linear models, additive models, local regression, and tree-based models. The contributions of the ten authors-most of whom work in the statistics research department at AT {\&} T Bell Laboratories-represent results of research in both the computational and statistical aspects of modeling data.},
author = {Chambers, John M. and Hastie, Trevor J.},
booktitle = {Statistical Models in S},
doi = {10.1201/9780203738535},
isbn = {9781351414234},
issn = {00401706},
pages = {1--608},
publisher = {CRC Press, Inc.},
title = {{Statistical models in S}},
year = {2017}
}
@book{Karl,
author = {Karl, Wolfgang and Cathy, H{\"{a}}rdle and Chen, Yi-Hsuan and Overbeck, Ludger},
file = {:C$\backslash$:/Users/user-pc/Desktop/2017{\_}Book{\_}AppliedQuantitativeFinance.pdf:pdf},
isbn = {9783662544853},
title = {{Statistics and Computing Third Edition}},
url = {http://www.springer.com/series/3022}
}
@article{Sweeting2011,
author = {Sweeting, P J},
isbn = {1748-5002},
journal = {Annals of Actuarial Science},
number = {02},
pages = {139--141},
title = {{The Usefulness of Stochastic Mortality Modelling}},
volume = {5},
year = {2011}
}
@article{Wen2017,
abstract = {We propose a framework for general probabilistic multi-step time series regression. Specifically, we exploit the expressiveness and temporal nature of Sequence-to-Sequence Neural Networks (e.g. recurrent and convolutional structures), the nonparametric nature of Quantile Regression and the efficiency of Direct Multi-Horizon Forecasting. A new training scheme, *forking-sequences*, is designed for sequential nets to boost stability and performance. We show that the approach accommodates both temporal and static covariates, learning across multiple related series, shifting seasonality, future planned event spikes and cold-starts in real life large-scale forecasting. The performance of the framework is demonstrated in an application to predict the future demand of items sold on Amazon.com, and in a public probabilistic forecasting competition to predict electricity price and load.},
archivePrefix = {arXiv},
arxivId = {1711.11053},
author = {Wen, Ruofeng and Torkkola, Kari and Narayanaswamy, Balakrishnan and Madeka, Dhruv},
eprint = {1711.11053},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wen et al. - 2017 - A Multi-Horizon Quantile Recurrent Forecaster.pdf:pdf},
number = {Nips 2017},
title = {{A Multi-Horizon Quantile Recurrent Forecaster}},
url = {http://arxiv.org/abs/1711.11053},
year = {2017}
}
@inproceedings{Coale1985,
author = {Coale, A J},
booktitle = {Asian and Pacific census forum/East-West Population Institute},
isbn = {0732-0515},
number = {1},
pages = {5},
title = {{An extension and simplification of a new synthesis of age structure and growth}},
volume = {12},
year = {1985}
}
@article{Hill2000,
author = {Hill, Mark E and Preston, Samuel H and Rosenwaike, Ira},
isbn = {0070-3370},
journal = {Demography},
number = {2},
pages = {175--186},
title = {{Age reporting among white Americans aged 85+: Results of a record linkage study}},
volume = {37},
year = {2000}
}
@misc{UnitedNations2013,
author = {{United Nations}},
publisher = {Department of Economic and Social Affairs, Population Division},
title = {{World Population Prospects: The 2012 Revision, DVD Edition.}},
year = {2013}
}
@article{Sweeting2011a,
author = {Sweeting, P J},
isbn = {1748-5002},
journal = {Annals of Actuarial Science},
number = {02},
pages = {143--162},
title = {{A trend-change extension of the Cairns-Blake-Dowd model}},
volume = {5},
year = {2011}
}
@article{Lee1974,
author = {Lee, Ronald},
isbn = {0032-4728},
journal = {Population Studies},
number = {3},
pages = {495--512},
title = {{Estimating series of vital rates and age structures from baptisms and burials: A new technique, with applications to pre-industrial England}},
volume = {28},
year = {1974}
}
@article{Joubert2013,
author = {Joubert, Jan{\'{e}} and Rao, Chalapati and Bradshaw, Debbie and Vos, Theo and Lopez, Alan D},
isbn = {1932-6203},
journal = {PloS one},
number = {5},
pages = {e64592},
title = {{Evaluating the Quality of National Mortality Statistics from Civil Registration in South Africa, 1997–2007}},
volume = {8},
year = {2013}
}
@article{Kuo2018a,
author = {Kuo, K},
journal = {arXiv},
shorttitle = {DeepTriangle: A Deep Learning Approach to Loss Res},
title = {{DeepTriangle: A Deep Learning Approach to Loss Reserving}},
volume = {arXiv:1804},
year = {2018}
}
@article{Yang1999,
abstract = {Risk bounds are derived for regression estimation based on model selection over an unrestricted number of models. While a large list of models provides more flexibility, significant selection bias may occur with model selection criteria like AIC. We incorporate a model complexity penalty term in AIC to handle selection bias. Resulting estimators are shown to achieve a trade-off among approximation error, estimation error and model complexity without prior knowledge about the true regression function. We demonstrate the adaptability of these estimators over full and sparse approximation function classes with different smoothness. For high-dimensional function estimation by tensor product splines we show that with number of knots and spline order adaptively selected, the least squares estimator converges at anticipated rates simultaneously for Sobolev classes with different interaction orders and smoothness parameters.},
author = {Yang, Yuhong},
file = {:C$\backslash$:/R/refas/yang1999model.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {Adaptive estimation,Model complexity,Model selection,Nonparametric regression,Rates of convergence},
number = {2},
pages = {475--499},
title = {{Model selection for nonparametric regression}},
volume = {9},
year = {1999}
}
@techreport{Rudin,
abstract = {Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward-it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.},
archivePrefix = {arXiv},
arxivId = {1811.10154v3},
author = {Rudin, Cynthia},
eprint = {1811.10154v3},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rudin - Unknown - Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.pdf:pdf},
title = {{Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead}}
}
@article{Coale1984,
author = {Coale, Ansley J},
isbn = {0032-4701},
journal = {Population Index},
pages = {193--213},
title = {{Life table construction on the basis of two enumerations of a closed population}},
year = {1984}
}
@article{Preston1998,
author = {Preston, Samuel H and Elo, Irma T and Foster, Andrew and Fu, Haishan},
isbn = {0070-3370},
journal = {Demography},
number = {1},
pages = {1--21},
title = {{Reconstructing the size of the African American population by age and sex, 1930–1990}},
volume = {35},
year = {1998}
}
@article{Buhlmann2018,
abstract = {We discuss recent work for causal inference and predictive robustness in a unifying way. The key idea relies on a notion of probabilistic invariance or stability: it opens up new insights for formulating causality as a certain risk minimization problem with a corresponding notion of robustness. The invariance itself can be estimated from general heterogeneous or perturbation data which frequently occur with nowadays data collection. The novel methodology is potentially useful in many applications, offering more robustness and better `causal-oriented' interpretation than machine learning or estimation in standard regression or classification frameworks.},
archivePrefix = {arXiv},
arxivId = {1812.08233},
author = {B{\"{u}}hlmann, Peter},
eprint = {1812.08233},
file = {:C$\backslash$:/R/refas/buhlmann2018invariance.pdf:pdf},
keywords = {anchor regression,causal regularization,distributional robustness,heterogeneous data,instru-,interventional data,mental variables regression,random forests,variable importance},
number = {786461},
pages = {1--36},
title = {{Invariance, Causality and Robustness}},
url = {http://arxiv.org/abs/1812.08233},
volume = {786461},
year = {2018}
}
@article{Carriere1992,
abstract = {This paper presents a general law of mortality that is equal to a mixture of Gompertz, Weibull, Inverse-Gompertz, and Inverse-Weibull survival functions. We demonstrate that a special case of our model fits the pattern of mortality of a U.S. life table up to age 90. We investigate several loss criteria for parameter estimation including some information theoretic criteria. We also present parsimonious special cases of our general law that fit the male and female 1980 CSO mortality tables to age 90 and the male and female 1983 Table a to age 100. Plots of our estimates of the valuation mortality rates are almost indistinguishable from the valuation rates themselves, and so we conclude that future valuation tables can be defined with reasonable math- ematical formulas. Finally, we demonstrate that modeling a law of mortality as a mixture of survival functions is equivalent to using a multiple decrement table.},
author = {Carriere, JACQUES F},
doi = {10.1016/0167-6687(94)90414-6},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
number = {1},
pages = {66},
title = {{Parametric models for life tables.}},
volume = {14},
year = {1994}
}
@article{Weidner2016,
abstract = {Using modern telematics technologies for car insurance, it is no particular challenge to produce an intractably large amount of kinematic and contextual information about driving profiles of motor vehicles. In order to evaluate this data with respect to both efficient and effective use in scoring and subsequent actuarial pricing, we propose a scale-sensitive approach that treats observations on semantically different levels. Furthermore we discuss the application of methods necessary to assess the information of different scale levels including signal processing, pattern recognition and Fourier analysis. In this way we show how maneuvers, trips and trip sections as well as the total insurance period can be analyzed to individually or collectively gain significantly scoreable insights into individual driving behaviour.},
author = {Weidner, W. and Transchel, F. W.G. and Weidner, R.},
doi = {10.1007/s13385-016-0127-x},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Actuarial pricing,Car insurance,Driving behaviour,Pattern recognition,Telematics technologies},
month = {jul},
number = {1},
pages = {3--24},
shorttitle = {Classification of scale-sensitive telematic observ},
title = {{Classification of scale-sensitive telematic observables for riskindividual pricing}},
url = {https://doi.org/10.1007/s13385-016-0127-x},
volume = {6},
year = {2016}
}
@article{Cheng1994,
abstract = {This paper informs a statistical readership about Artificial Neural Networks (ANNs), points out some of the links with statistical methodology and encourages cross-disciplinary research in the directions most likely to bear fruit. The areas of statistical interest are briefly outlined, and a series of examples indicates the flavor of ANN models. We then treat various topics in more depth. In each case, we describe the neural network architectures and training rules and provide a statistical commentary. The topics treated in this way are perceptrons (from singleunit to multilayer versions), Hopfleld-type recurrent networks (including probabilistic versions strongly related to statistical physics and Gibbs distributions) and associative memory networks trained by so-called unsuperviszd learning rules. Perceptrons are shown to have strong associations with discriminant analysis and regression, and unsupervized networks with cluster analysis. The paper concludes with some thoughts on the future of the interface between neural networks and statistics. {\textcopyright} 1994, Institute of Mathematical Statistics. All Rights Reserved.},
author = {Cheng, Bing and Titterington, D. M.},
doi = {10.1214/ss/1177010638},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Titterington - 1994 - Neural networks A review from a statistical perspective.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Artificial intelligence,Artificial neural networks,Cluster analysis,Discriminant analysis,Gibbs distributions,Incomplete data,Nonparametric regression,Statistical pattern recognition},
number = {1},
pages = {2--30},
title = {{Neural networks: A review from a statistical perspective}},
url = {https://projecteuclid.org/euclid.ss/1177010638},
volume = {9},
year = {1994}
}
@incollection{Hazelton2014,
abstract = {This book provides a nice overview of kernel-smoothing ideas, methods, and open questions with a smooth, intuitive style. The reader should not expect, however, a comprehensive treatment of all aspects of kernels. To the authors' credit, they admit up front that the choice of topics is "personal" and point those interested toward the plethora of other books covering this subject. This book is meant as an introduction, but it does require some comfort level with calculus and linear algebra. Following a short introductory chapter, the authors devote three chapters to density estimation, one to regression, and a closing chapter to whetting the appetite about other related topics. The book aims to provide the reader with sufficient understanding of the principles behind the mathematical machinery of kernel methods to make judgments about their proper use. Graphics liberally spread through the text illustrate the construction and properties of kernel estimates. In particular, the importance of curvature and boundaries is stressed in several places. In a sense, this book fills a void, maintaining mathematical rigor while providing a more heuristic approach to the various compromises that are involved in kernel methods. The trade-offs between a clean parametric form of a density or regression function must be balanced with letting the data suggest the best shape. Suspending belief about the "true" form of a curve, however, adds uncertainty in estimating the shape. These issues are made precise in plain language in the first chapter. Later chapters reinterpret the questions raised in more mathematical terms, with immediate interpretation in words and graphs as appropriate. Chapters 2-4 provide an up-to-date accounting of kernel density estimation, as would be expected from authors who have made important contributions in this area in recent years. Notation is introduced in a natural way as they build ideas. The chapter on bandwidth selection brings together many recent concepts that have not appeared in book form to date. Unfortunately, as Wand and Jones aptly point out, there is much we still do not know in this arena. Multivariate density estimation is briefly addressed, alluding to the "curse of dimensionality" while focusing largely on two dimensions. The chapter on regression (5) considers "local polynomial kernel estimators", which have become quite popular. This appears to be the first treatment of this important area in book form, showing its advantages over other more traditional kernel methods in terms of large-sample properties and boundary behavior. Recent theoretical work on mean squared error (MSE) and bandwidth selection are smoothly incorporated, showing the connection with related work on density estimation. The reader primarily interested in nonparametric regression, however, would want to supplement this book with others, such as those of Green and Silverman (1994) or Wahba (1990), as encouraged by the authors. The final chapter in a sense teases the reader, with only a few pages about each topic. In particular, censored data are lightly addressed even though there is considerable literature in this area. Space-time data, which could be viewed as a special type of multivariate data, seems to be ignored. There is a curious feature in the examples used throughout the book. Most of the data sets appear to contain a few hundred observations, which places them toward the lower threshold of what is possible with kernel methods. Although this is laudable in some sense, it raises the question of how kernel methods function when the data set is huge, with millions, billions, or trillions of measurements. Data on this scale are being collected in areas of engineering and environmental and biomedical sciences. These problems cannot usually be considered as having the same degree of curvature throughout, and boundaries between regions may or may not be distinct. How well does kernel smoothing work here? Although the authors do not address this area directly, their insights can help the reader build intuition and a mathematical framework to further investigate important issues in kernel smoothing.},
author = {Bowman, A. W. and Wand, M. P. and Jones, M. C.},
booktitle = {Biometrics},
doi = {10.2307/2534029},
issn = {0006341X},
number = {1},
pages = {393},
title = {{Kernel Smoothing.}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat06538},
volume = {54},
year = {1998}
}
@misc{Cotteril2020,
abstract = {Central bank unveils measures to provide liquidity as country readies for lockdown},
author = {Cotteril, Joseph and Johnson, Steve},
booktitle = {Financial Times},
language = {en-GB},
month = {mar},
title = {{South Africa launches bond purchases after coronavirus roils markets}},
url = {https://www.ft.com/content/497ff534-6ead-11ea-89df-41bea055720b},
urldate = {2020-04-02},
year = {2020}
}
@misc{Dowle2018,
address = {CRAN},
author = {Dowle, M and Srinivasan, A},
title = {data.table},
url = {https://cran.r-project.org/web/packages/data.table/index.html},
year = {2018}
}
@article{Moultrie2003,
author = {Moultrie, Tom A and Tim{\ae}us, Ian M},
isbn = {0032-4728},
journal = {Population studies},
number = {3},
pages = {265--283},
title = {{The South African fertility decline: Evidence from two censuses and a Demographic and Health Survey}},
volume = {57},
year = {2003}
}
@article{Preston2006,
abstract = {The 2003 official US life tables show exceptionally high survival probabilities from age 85 to 100 for black men and women. Because a large fraction of Americans now reach age 85, mortality estimates above that age affect summary measures of longevity, including life expectancy at birth and the probability that a newborn will reach age 100. This probability is greater for blacks than for whites. We discuss problems of data accuracy among older African Americans and the effect that they are expected to have on mortality estimates. We compare the 2003 black life table values to established age patterns of mortality in lowmortality countries and to mortality estimates based on Social Security data. We conclude that black mortality above age 85 in the 2003 official life tables is implausibly low.},
author = {Preston, Samuel H and Elo, Irma T},
doi = {10.2307/20058904},
isbn = {00987921},
journal = {Population and Development Review},
number = {3},
pages = {557--565},
publisher = {Population Council},
title = {{Black Mortality at Very Old Ages in Official Us Life Tables: A Skeptical Appraisal}},
url = {http://www.jstor.org/stable/20058904},
volume = {32},
year = {2006}
}
@misc{drive.ai2018a,
author = {Drive.ai},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/drive.ai-Drive.ai Announces On-Demand Self-Dri.pdf:pdf},
howpublished = {drive.ai},
number = {17 June},
publisher = {drive.ai},
shorttitle = {Drive.ai Announces On-Demand Self-Driving Car Serv},
title = {{Drive.ai Announces On-Demand Self-Driving Car Service on Public Roads in Texas}},
url = {https://s3.amazonaws.com/www-staging.drive.ai/content/uploads/2018/05/06164346/Press-Release{\_}Drive.ai-Texas-Deployment.pdf},
volume = {2018},
year = {2018}
}
@article{DasGupta1975,
author = {Gupta, Prithwis Das and {Das Gupta}, Prithwis},
isbn = {0070-3370},
journal = {Demography},
number = {2},
pages = {303--312},
title = {{A general method of correction for age misreporting in census populations}},
volume = {12},
year = {1975}
}
@techreport{SwissAssociationofActuariesSAA2018,
author = {(SAA), Swiss Association of Actuaries},
institution = {Swiss Association of Actuaries (SAA)},
number = {August},
title = {{Data science strategy}},
year = {2018}
}
@article{Gisler2008,
abstract = {We consider the chain ladder reserving method in a Bayesian set up, which allows for combining the information from a specific claims development triangle with the information from a collective. That is, for instance, to consider simultaneously own company specific data and industry-wide data to estimate the own company's claims reserves. We derive Bayesian estimators and credibility estimators within this Bayesian framework. We show that the credibility estimators are exact Bayesian in the case of the exponential dispersion family with its natural conjugate priors. Finally, we make the link to the classical chain ladder method and we show that using non-informative priors we arrive at the classical chain ladder forecasts. However, the estimates for the mean square error of prediction differ in our Bayesian set up from the ones found in the literature. Hence, the paper also throws a new light upon the estimator of the mean square error of prediction of the classical chain ladder forecasts and suggests a new estimator in the chain ladder method.},
author = {Gisler, Alois and W{\"{u}}thrich, Mario V.},
doi = {10.2143/ast.38.2.2033354},
isbn = {0515-0361},
issn = {0515-0361},
journal = {ASTIN Bulletin},
number = {02},
pages = {565--600},
title = {{Credibility for the Chain Ladder Reserving Method}},
volume = {38},
year = {2008}
}
@inproceedings{Feeney1979,
author = {Feeney, Griffith},
booktitle = {Asian and Pacific Census Forum},
number = {3},
pages = {12--14},
title = {{A technique for correcting age distributions for heaping on multiples of five}},
volume = {5},
year = {1979}
}
@article{Jowett1992a,
author = {Jowett, A John and Li, Yuan-Qing},
isbn = {0343-2521},
journal = {GeoJournal},
number = {4},
pages = {427--442},
title = {{Age—heaping: contrasting patterns from china}},
volume = {28},
year = {1992}
}
@article{Preston1984,
author = {Preston, Samuel H},
title = {{Use of direct and indirect techniques for estimating the completeness of death registration systems}},
year = {1984}
}
@article{Gabrielli2018,
abstract = {The aim of this project is to develop a stochastic simulation machine that generates individual claims histories of non-life insurance claims. This simulation machine is based on neural networks to incorporate individual claims feature information. We provide a fully calibrated stochastic scenario generator that is based on real non-life insurance data. This stochastic simulation machine allows everyone to simulate their own synthetic insurance portfolio of individual claims histories and back-test thier preferred claims reserving method.},
author = {Gabrielli, Andrea and W{\"{u}}thrich, Mario V.},
doi = {10.3390/risks6020029},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Gabrielli-2018-An Individual Claims History Si.pdf:pdf},
issn = {22279091},
journal = {Risks},
keywords = {Chain-ladder,Claims cash flows,Claims reserving,Claims simulation,Individual claims,Individual claims covariates,Individual claims features,Loss reserving,Micro-level stochastic reserving,Neural network reserving},
number = {2},
pages = {29},
shorttitle = {An Individual Claims History Simulation Machine},
title = {{An individual claims history simulation machine}},
volume = {6},
year = {2018}
}
@article{Gao2019,
abstract = {The aim of this project is to analyze high-frequency GPS location data (second per second) of individual car drivers (and trips). We extract feature information about speeds, acceleration, deceleration, and changes of direction from this high-frequency GPS location data. Time series of this feature information allow us to appropriately allocate individual car driving trips to selected drivers using convolutional neural networks.},
author = {Gao, Guangyuan and W{\"{u}}thrich, Mario V.},
doi = {10.3390/risks7010006},
issn = {22279091},
journal = {Risks},
keywords = {Convolutional neural networks,Driving styles,Image recognition,Pattern recognition,Telematics car driving data},
number = {1},
title = {{Convolutional neural network classification of telematics car driving data}},
volume = {7},
year = {2019}
}
@article{Werbos1988,
abstract = {Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research. This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place. {\textcopyright} 1988.},
author = {Werbos, Paul J.},
doi = {10.1016/0893-6080(88)90007-X},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Backpropagation,Cerebral cortex,Continuous time,Energy models,Modelling,Prediction,Recurrent,Reinforcement learning},
number = {4},
pages = {339--356},
title = {{Generalization of backpropagation with application to a recurrent gas market model}},
url = {http://www.sciencedirect.com/science/article/pii/089360808890007X},
volume = {1},
year = {1988}
}
@article{Jarner2011,
author = {Jarner, S{\o}ren Fiig and Kryger, Esben Masotti},
isbn = {1783-1350},
journal = {Astin Bulletin},
number = {02},
pages = {377--418},
title = {{Modelling adult mortality in small populations: The SAINT model}},
volume = {41},
year = {2011}
}
@misc{Meyers2011,
author = {{Casualty Actuarial Society}},
title = {{Loss Reserving Data Pulled From Naic Schedule P}},
url = {http://www.casact.org/research/index.cfm?fa=loss{\_}reserves{\_}data},
year = {2011}
}
@inproceedings{Luther1983,
author = {Luther, Norman Y},
booktitle = {Asian and Pacific Census Forum},
number = {3},
pages = {1--11},
title = {{Measuring changes in Census coverage in Asia}},
volume = {9},
year = {1983}
}
@article{DeBrebisson2015,
abstract = {We describe our first-place solution to the ECML/PKDD discovery challenge on taxi destination prediction. The task consisted in predicting the destination of a taxi based on the beginning of its trajectory, represented as a variable-length sequence of GPS points, and diverse associated meta-information, such as the departure time, the driver id and client information. Contrary to most published competitor approaches, we used an almost fully automated approach based on neural networks and we ranked first out of 381 teams. The architectures we tried use multi-layer perceptrons, bidirectional recurrent neural networks and models inspired from recently introduced memory networks. Our approach could easily be adapted to other applications in which the goal is to predict a fixed-length output from a variable-length sequence.},
archivePrefix = {arXiv},
arxivId = {1508.00021},
author = {{De Br{\'{e}}bisson}, Alexandre and Simon, {\'{E}}tienne and Auvolat, Alex and Vincent, Pascal and Bengio, Yoshua},
eprint = {1508.00021},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/De Br{\'{e}}bisson-2015-Artificial neural networks a(2).pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
shorttitle = {Artificial neural networks applied to taxi destina},
title = {{Artificial neural networks applied to taxi destination prediction}},
volume = {1526},
year = {2015}
}
@article{Kannisto1988,
author = {Kannisto, V{\"{a}}in{\"{o}}},
isbn = {0032-4728},
journal = {Population studies},
number = {3},
pages = {389--406},
title = {{On the survival of centenarians and the span of life}},
volume = {42},
year = {1988}
}
@article{Makeham1860,
author = {Makeham, William Matthew},
isbn = {2046-1658},
journal = {The Assurance Magazine, and Journal of the Institute of Actuaries},
pages = {301--310},
title = {{On the law of mortality and the construction of annuity tables}},
year = {1860}
}
@article{Dorrington2012,
author = {Dorrington, R E and Bradshaw, D and Laubscher, R},
journal = {Cape Town, South Africa: South African Medical Research Council},
title = {{Rapid mortality surveillance report 2012}},
year = {2012}
}
@incollection{Joubert2006,
address = {Cape Town},
author = {Joubert, Jan{\'{e}} and Bradshaw, Debbie},
booktitle = {Chronic Diseases of Lifestyle in South Africa: 1995 - 2005.},
chapter = {15},
editor = {Steyn, Krisela and Fourie, Jean and Temple, Norman},
pages = {204--267},
publisher = {South African Medical Research Council},
title = {{Population Ageing and Health Challenges}},
year = {2006}
}
@article{Xu2019,
abstract = {Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design TGAN, which uses a conditional generative adversarial network to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. TGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.},
archivePrefix = {arXiv},
arxivId = {1907.00503},
author = {Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
eprint = {1907.00503},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2019 - Modeling Tabular data using Conditional GAN.pdf:pdf},
month = {jun},
title = {{Modeling Tabular data using Conditional GAN}},
url = {http://arxiv.org/abs/1907.00503},
year = {2019}
}
@article{Ortega1986,
author = {Ortega, Antonio and Garcia, V},
title = {{Estudio experimental sobre la mortalidad y algunas caracter{\'{i}}sticas socioecon{\'{o}}micas de las personas de la tercera edad: informe de la investigaci{\'{o}}n efectuada en los cantones de Puriscal y Coronado, 3-20 junio 1985}},
year = {1986}
}
@article{Bhat1995,
author = {Bhat, P N},
journal = {Demography India},
number = {1},
pages = {59--80},
title = {{Age misreporting and its impact on adult mortality estimates in South Asia}},
volume = {24},
year = {1995}
}
@misc{Racine2018,
author = {Hayfield, T and Racine, J S},
publisher = {CRAN},
title = {{np: Nonparametric kernel smoothing methods for mixed datatypes}},
url = {https://cran.r-project.org/web/packages/np/index.html},
year = {2007}
}
@book{Wuthrich2008,
author = {W{\"{u}}thrich, Mario V and Merz, Michael},
isbn = {0470772727},
publisher = {John Wiley {\&} Sons},
title = {{Stochastic claims reserving methods in insurance}},
volume = {435},
year = {2008}
}
@article{Mack2000,
author = {Mack, Thomas},
isbn = {1783-1350},
journal = {Astin Bulletin},
number = {02},
pages = {333--347},
title = {{Credible claims reserves: the Benktander method}},
volume = {30},
year = {2000}
}
@article{Coale1989,
author = {Coale, Ansley and Guo, Guang},
isbn = {0032-4701},
journal = {Population index},
pages = {613--643},
title = {{Revised regional model life tables at very low levels of mortality}},
year = {1989}
}
@article{Bhat2002,
abstract = {This paper proposes a reformulation of the general growth balance method for estimating census and registration completeness so as to make it applicable even to populations that are affected by migration. It also discusses a new procedure of line fitting that could be useful in countries where the input data are severely affected by age misreporting. The method is applicable to countries where data on age distribution of the population are available for two points in time from either censuses or surveys. Following closely the original proposal of Brass, it involves adjusting the 'partial' birth rates for age-specific disturbances from growth and migration rates. Beyond correcting the death rates, the method is useful in inferring the relative completeness of the censuses, and in deriving a robust estimate of birth rate under certain conditions. The application of the method is illustrated using the example of the male population of the Indian state of Uttar Pradesh for the period 1981 to 1991.},
annote = {Bhat, P N Mari
eng
England
2002/01/01 00:00
Popul Stud (Camb). 2002 Jan;56(1):23-34. doi: 10.1080/00324720213798.},
author = {Bhat, P. N.Mari},
doi = {10.1080/00324720213798},
isbn = {0032-4728 (Print)
0032-4728 (Linking)},
issn = {00324728},
journal = {Population Studies},
number = {1},
pages = {23--34},
pmid = {22010843},
title = {{General growth balance method: A reformulation for populations open to migration}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22010843},
volume = {56},
year = {2002}
}
@article{Horiuchi1998,
author = {Horiuchi, Shiro and Wilmoth, John R},
isbn = {0070-3370},
journal = {Demography},
number = {4},
pages = {391--412},
title = {{Deceleration in the age pattern of mortality at olderages}},
volume = {35},
year = {1998}
}
@inproceedings{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
isbn = {9781467369640},
issn = {10636919},
pages = {1--9},
publisher = {IEEE},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@book{Aggarwal2018,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Aggarwal, Charu C.},
booktitle = {Neural Networks and Deep Learning},
doi = {10.1007/978-3-319-94463-0},
file = {:C$\backslash$:/Users/user-pc/Desktop/2018{\_}Book{\_}NeuralNetworksAndDeepLearning.pdf:pdf},
isbn = {9783319944623},
title = {{Neural Networks and Deep Learning}},
year = {2018}
}
@article{Kjærgaard2019,
abstract = {Cause-specific mortality forecasting is often based on predicting cause-specific death rates independently. Only a few methods have been suggested that incorporate dependence between causes. An attractive alternative is to model and forecast cause-specific death distributions, rather than mortality rates, as dependence between the causes can be incorporated directly. We follow this idea and propose two new models which extend the current research on mortality forecasting using death distributions. We find that adding age, time and cause-specific weights and decomposing both joint and individual variation between different causes of death increased the forecast accuracy of cancer deaths by using data for French and Dutch populations.},
author = {Kj{\ae}rgaard, S{\o}ren and Ergemen, Yunus Emre and Kallestrup-Lamb, Malene and Oeppen, Jim and Lindahl-Jacobsen, Rune},
doi = {10.1111/rssc.12357},
file = {:C$\backslash$:/Users/user-pc/Downloads/rssc.12357.pdf:pdf},
issn = {14679876},
journal = {Journal of the Royal Statistical Society. Series C: Applied Statistics},
keywords = {Cancer forecast,Cause-specific mortality,Compositional data analysis,Forecasting methods,Population health},
month = {nov},
number = {5},
pages = {1351--1370},
publisher = {Blackwell Publishing Ltd},
title = {{Forecasting causes of death by using compositional data analysis: the case of cancer deaths}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12357},
volume = {68},
year = {2019}
}
@article{Bengio,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5538v2},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {3C2DBCEE-8A96-493B-B88B-36B1F52ECA58},
eprint = {arXiv:1206.5538v2},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Bengio-Unsupervised feature learning and deep.pdf:pdf},
isbn = {0162-8828 VO - 35},
issn = {1939-3539},
journal = {Tpami},
keywords = {Boltzmann Machine,Index Terms—Deep learning,RBM,auto-encoder,feature learning,neural network,unsupervised learning},
number = {1993},
pages = {1--30},
pmid = {23459267},
title = {{Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives}},
url = {https://pdfs.semanticscholar.org/f8c8/619ea7d68e604e40b814b40c72888a755e95.pdf{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/23459267},
volume = {35},
year = {2013}
}
@article{LeCun1998,
abstract = {1. The grain size of beryllium is not a single-valued characteristic of the structure; the mechanical properties also depend on the substructure. 2. The reduction of the yield strength with increasing recrystallization temperatures is due to a reduction of the dislocation density. 3. The increase of the activation volume with increasing annealing temperatures is due to the lower concentration of impurities in the matrix. {\textcopyright} 1978 Plenum Publishing Corporation.},
author = {Khristenko, I. N. and Kornienko, L. A. and Nikolaenko, A. A.},
doi = {10.1007/BF00774006},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/LeCun-1998-Gradient-based learning applied to.pdf:pdf},
isbn = {0018-9219},
issn = {00260673},
journal = {Metal Science and Heat Treatment},
number = {6},
pages = {509--512},
shorttitle = {Gradient-based learning applied to document recogn},
title = {{Effect of recrystallization temperature on the mechanical properties of beryllium}},
volume = {20},
year = {1978}
}
@article{England2019a,
abstract = {This paper brings together analytic and simulation-based approaches to reserve risk in general (P{\&}C) insurance, applied to the traditional actuarial view of risk over the lifetime of the liabilities and to the one-year view of Solvency II. It also connects the lifetime and one-year views of risk. The framework of the model in Mack (1993) is used throughout, although the results have wider applicability. The advantages of a simulation-based approach are highlighted, giving a full predictive distribution, which is used to estimate risk margins under Solvency II and risk adjustments under IFRS 17. We discuss methods for obtaining capital requirements in a cost-of-capital risk margin, and methods for estimating risk adjustments using risk measures applied to a simulated distribution of the outstanding liabilities over their lifetime.},
author = {England, Peter and Verrall, R. J. and W{\"{u}}thrich, M. V.},
doi = {10.1016/j.insmatheco.2018.12.002},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/England, Verrall, W{\"{u}}thrich - 2019 - On the lifetime and one-year views of reserve risk, with application to IFRS 17 and Solvency II r(2).pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Bootstrap,Coherent risk measure,Cost-of-capital,IFRS 17 risk adjustment,Solvency II risk margin,Stochastic reserving},
month = {mar},
pages = {74--88},
publisher = {Elsevier B.V.},
title = {{On the lifetime and one-year views of reserve risk, with application to IFRS 17 and Solvency II risk margins}},
volume = {85},
year = {2019}
}
@techreport{jewell1980models,
author = {Jewell, William S},
institution = {CALIFORNIA UNIV BERKELEY OPERATIONS RESEARCH CENTER},
title = {{Models in Insurance: Paradigms, Puzzles, Communications and Revolutions}},
year = {1980}
}
@inproceedings{Hill1987,
abstract = {PIP:$\backslash$n$\backslash$nThis article presents a new method for estimating the relative completeness of 2 census enumerations and of intercensal registered deaths. The Growth Balance Equation was developed by Brass (1975) to estimate the completeness of death registration relative to the completeness of census enumeration. The method presented here can be seen either as an extension of Martin's formulation to allow explicitly for changes in census coverage or as a modification of Brass's method to use deaths by age group rather than deaths by cohort, preferable on the grounds that age group comparisons will be less distorted by age misreporting than cohort comparisons if the patterns of age misreporting are similar for 2 successive censuses. This simple method estimates simultaneously the relative coverage of the 2 censuses and the completeness of registration of intercensal deaths. The key assumptions of the method are that the population is closed to migration and that all the coverage factors involved are invariant with age, at least for the age range studied. Analysis of the sensitivity of the estimates to the assumptions and further work on extending the method to open populations would be useful.},
author = {Hill, K.},
booktitle = {Asian and Pacific population forum / East-West Population Institute, East-West Center},
isbn = {0891-2823},
issn = {08912823},
number = {3},
pages = {8--13, 23},
title = {{Estimating census and death registration completeness.}},
volume = {1},
year = {1987}
}
@article{Dorrington2008,
author = {Dorrington, R E and Tim{\ae}us, I M and Moultrie, T A},
journal = {Working paper},
title = {{Death distribution methods for estimating adult mortality: sensitivity analysis with simulated data errors, revisited}},
year = {2008}
}
@article{CMI2016,
author = {CMI},
journal = {The Institute and Faculty of Actuaries},
title = {{Continuous Mortality Investigation Working Paper 90}},
year = {2016}
}
@article{Currie2011a,
author = {Currie, Iain D},
isbn = {1783-1350},
journal = {Astin Bulletin},
number = {02},
pages = {419--427},
title = {{Modelling and forecasting the mortality of the very old}},
volume = {41},
year = {2011}
}
@techreport{Fey,
abstract = {We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence. Our source code is available on GitHub 1 .},
author = {Fey, Matthias and Lenssen, Jan Eric and Weichert, Frank and M{\"{u}}ller, Heinrich},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fey et al. - Unknown - SplineCNN Fast Geometric Deep Learning with Continuous B-Spline Kernels.pdf:pdf},
title = {{SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels}},
url = {https://github.com/rusty1s/pytorch{\_}geometric},
year = {2017}
}
@article{Hyndman2015,
author = {Hyndman, R J and Athanasopoulos, G and Razbash, S and Schmidt, D and Zhou, Z and Khan, Y and Bergmeir, C and Wang, E},
journal = {R package version},
number = {6},
pages = {7},
title = {{forecast: Forecasting functions for time series and linear models. R package version 5.2}},
volume = {6},
year = {2014}
}
@article{Gelman2010,
abstract = {Chapter 9 discussed situations in which it is dangerous to use a standard linear regression of outcome on predictors and an indicator variable for estimating causal effects: when there is imbalance or lack of complete overlap or when ignorability is in doubt. This chapter discusses these issues in more detail and provides potential solutions for each. 10.1 Imbalance and lack of complete overlap In a study comparing two treatments (which we typically label " treatment " and " control "), causal inferences are cleanest if the units receiving the treatment are comparable to those receiving the control. Until Section 10.5, we shall restrict our-selves to ignorable models, which means that we only need to consider observed pre-treatment predictors when considering comparability. For ignorable models, we consider two sorts of departures from comparability— imbalance and lack of complete overlap. Imbalance occurs if the distributions of relevant pre-treatment variables differ for the treatment and control groups. Lack of complete overlap occurs if there are regions in the space of relevant pre-treatment variables where there are treated units but no controls, or controls but no treated units. Imbalance and lack of complete overlap are issues for causal inference largely because they force us to rely more heavily on model specification and less on direct support from the data. When treatment and control groups are unbalanced, the simple comparison of group averages, ¯ y 1 − ¯ y 0 , is not, in general, a good estimate of the average treat-ment effect. Instead, some analysis must be performed to adjust for pre-treatment differences between the groups. When treatment and control groups do not completely overlap, the data are in-herently limited in what they can tell us about treatment effects in the regions of nonoverlap. No amount of adjustment can create direct treatment/control compar-isons, and one must either restrict inferences to the region of overlap, or rely on a model to extrapolate outside this region. Thus, lack of complete overlap is a more serious problem than imbalance. But similar statistical methods are used in both scenarios, so we discuss these problems together here. Imbalance and model sensitivity When attempting to make causal inferences by comparing two samples that differ in terms of the " treatment " or causing variable of interest (participation in a program, taking a drug, engaging in some activity) but that also differ in terms of confounding covariates (predictors related both to the treatment and outcome), we can be misled 199},
author = {Gelman, Andrew and Hill, Jennifer and Gelman, Andrew and Hill, Jennifer},
doi = {10.1017/cbo9780511790942.013},
file = {:C$\backslash$:/R/refas/chap10.pdf:pdf},
journal = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
pages = {199--234},
title = {{Causal inference using more advanced models}},
year = {2010}
}
@article{DeFelice2019,
abstract = {We present an approach to individual claims reserving and claim watching in general insurance based on classification and regression trees (CART). We propose a compound model consisting of a frequency section, for the prediction of events concerning reported claims, and a severity section, for the prediction of paid and reserved amounts. The formal structure of the model is based on a set of probabilistic assumptions which allow the provision of sound statistical meaning to the results provided by the CART algorithms. The multiperiod predictions required for claims reserving estimations are obtained by compounding one-period predictions through a simulation procedure. The resulting dynamic model allows the joint modeling of the case reserves, which usually yields useful predictive information. The model also allows predictions under a double-claim regime, i.e., when two different types of compensation can be required by the same claim. Several explicit numerical examples are provided using motor insurance data. For a large claims portfolio we derive an aggregate reserve estimate obtained as the sum of individual reserve estimates and we compare the result with the classical chain-ladder estimate. Backtesting exercises are also proposed concerning event predictions and claim-reserve estimates.},
author = {{De Felice}, Massimo and Moriconi, Franco},
doi = {10.3390/risks7040102},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Felice, Moriconi - 2019 - Claim Watching and Individual Claims Reserving Using Classification and Regression Trees.pdf:pdf},
issn = {22279091},
journal = {Risks},
keywords = {Claim watching,Classification and regression trees,Individual claims reserving,Machine learning},
month = {oct},
number = {4},
pages = {102},
publisher = {MDPI AG},
title = {{Claim watching and individual claims reserving using classification and regression trees}},
url = {https://www.mdpi.com/2227-9091/7/4/102},
volume = {7},
year = {2019}
}
@article{Palloni1984,
author = {Palloni, Alberto and Kominski, Robert},
isbn = {0032-4728},
journal = {Population studies},
number = {3},
pages = {479--493},
title = {{Estimation of adult mortality using forward and backward projections}},
volume = {38},
year = {1984}
}
@article{Taylor2019,
abstract = {The purpose of this paper is to survey recent developments in granular models and machine learning models for loss reserving, and to compare the two families with a view to assessment of their potential for future development. This is best understood against the context of the evolution of these models from their predecessors, and the early sections recount relevant archaeological vignettes from the history of loss reserving. However, the larger part of the paper is concerned with the granular models and machine learning models. Their relative merits are discussed, as are the factors governing the choice between them and the older, more primitive models. Concluding sections briefly consider the possible further development of these models in the future.},
author = {Taylor, Greg},
doi = {10.3390/risks7030082},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Taylor - 2019 - Loss Reserving Models Granular and Machine Learning Forms.pdf:pdf},
issn = {22279091},
journal = {Risks},
keywords = {Granular models,Loss reserving,Machine learning,Neural networks},
month = {jul},
number = {3},
pages = {82},
title = {{Loss reserving models: Granular and machine learning forms}},
url = {https://www.mdpi.com/2227-9091/7/3/82},
volume = {7},
year = {2019}
}
@book{Robinson2006,
abstract = {Summary: Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.},
author = {Robinson, Stephen M},
booktitle = {Numerical Optimization},
doi = {10.1007/978-0-387-40065-5},
file = {:C$\backslash$:/Users/user-pc/Desktop/2006{\_}Book{\_}NumericalOptimization.pdf:pdf},
isbn = {9780387303031},
title = {{Numerical Optimization}},
year = {2006}
}
@article{Friedman2009a,
address = {New York, NY},
author = {Friedman, J and Hastie, T and Tibshirani, R},
isbn = {9780387848587 0387848584},
language = {English},
publisher = {Springer-Verlag New York},
title = {{The Elements of Statistical Learning : Data Mining, Inference, and Prediction}},
url = {http://dx.doi.org/10.1007/978-0-387-84858-7},
year = {2009}
}
@article{Kannisto1994,
author = {Kannisto, Vaino and Lauritsen, Jens and Thatcher, A Roger and Vaupel, James W},
isbn = {0098-7921},
journal = {Population and development review},
pages = {793--810},
title = {{Reductions in mortality at advanced ages: several decades of evidence from 27 countries}},
year = {1994}
}
@techreport{Dorrington2014,
address = {Cape Town, South Africa},
author = {Dorrington, R E and Bradshaw, D and Laubscher, R and Nannan, N},
publisher = {South African Medical Research Council},
title = {{Rapid mortality surveillance report 2013}},
year = {2014}
}
@incollection{Bengio2006,
abstract = {A central goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on several methods to speed-up both training and probability computation, as well as comparative experiments to evaluate the improvements brought by these techniques. We finally describe the incorporation of this new language model into a state-of-the-art speech recognizer of conversational speech.},
author = {Bengio, Yoshua and Schwenk, Holger and Sen{\'{e}}cal, Jean-S{\'{e}}bastien and Morin, Fr{\'{e}}deric and Gauvain, Jean-Luc},
booktitle = {Innovations in Machine Learning},
doi = {10.1007/3-540-33486-6_6},
month = {may},
pages = {137--186},
publisher = {Springer-Verlag},
title = {{Neural Probabilistic Language Models}},
year = {2006}
}
@article{Preston1989,
author = {Preston, Samuel H and Himes, Christine and Eggers, Mitchell},
isbn = {0070-3370},
journal = {Demography},
number = {4},
pages = {691--704},
title = {{Demographic conditions responsible for population aging}},
volume = {26},
year = {1989}
}
@misc{South2020,
author = {{South African Reserve Bank}},
booktitle = {SARB Website},
month = {mar},
title = {{QA on Amendments to Money Market Liquidity Strategy of the SARB}},
url = {https://www.resbank.co.za/Lists/News and Publications/Attachments/9810/QA on Amendments to Money Market Liquidity Strategy of the SARB.pdf},
urldate = {2020-04-02},
year = {2020}
}
@article{Thatcher2002,
abstract = {Only a few countries have official population statistics which are sufficiently accurate to produce reliable estimates of death rates at high ages. For other countries, there are several methods which can be used to produce improved estimates. The choice is important for research on old age mortality. In 1999 the Max Planck Institute for Demographic Research undertook a research project to compare the performance of the three leading methods, using data for nine countries over 35 years. This paper describes the research and the results, which were unexpectedly simple. It also gives an authoritative account of the most successful method. {\textcopyright} 2002 Max-Planck-Gesellschaft.},
author = {Thatcher, Roger and Kannisto, Vaino and Andreev, Kirill},
doi = {10.4054/demres.2002.6.1},
issn = {14359871},
journal = {Demographic Research},
keywords = {Estimation techniques},
number = {1},
pages = {2--15},
title = {{The survivor ratio method for estimating numbers at high ages}},
volume = {6},
year = {2002}
}
@article{Chen2018,
abstract = {This paper introduces a gender specific model for the joint mortality projection of three countries (England and Wales combined, Scotland, and Northern Ireland) of the United Kingdom. The model, called 2-tier Augmented Common Factor model, extends the classical Lee and Carter [26] and Li and Lee [32] models, with a common time factor for the whole UK population, a sex specific period factor for males and females, and a specific time factor for each country within each gender. As death counts in each subpopulation are modelled directly, a Poisson framework is used. Our results show that the 2-tier ACF model improves the in-sample fitting compared to the use of independent LC models for each subpopulation or of independent Li and Lee models for each couple of genders within each country. Mortality projections also show that the 2-tier ACF model produces coherent forecasts for the two genders within each country and different countries within each gender, thus avoiding the divergence issues arising when independent projections are used. The 2-tier ACF is further extended to include a cohort term to take into account the faster improvements of the UK ‘golden generation'.},
author = {Chen, Ree Yongqing and Millossovich, Pietro},
doi = {10.1007/s13385-017-0164-0},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Coherent forecast,Cohort term,Common factor,Lee-Carter,Mortality projection},
month = {jun},
number = {1},
pages = {69--95},
title = {{Sex-specific mortality forecasting for UK countries: a coherent approach}},
url = {https://doi.org/10.1007/s13385-017-0164-0},
volume = {8},
year = {2018}
}
@article{McNeil2006,
abstract = {Credit risk models like Moody's {\{}KMV{\}} are now well established in the market and give bond managers reliable estimates of default probabilities for individual firms. Until now it has been hard to relate those probabilities to the actual credit spreads observed on the market for corporate bonds. Inspired by the existence of scaling laws in financial markets by Dacorogna et al. [2001. An Introduction to High Frequency Finance. Academic Press, San Diego, CA] and Di Matteo et al. [2005. Journal of Banking and Finance 29, 827–851] deviating from the Gaussian behavior, we develop a model that quantitatively links those default probabilities to credit spreads (market prices). The main input quantities to this study are merely industry yield data of different times to maturity and expected default frequencies (EDFs) of Moody's KMV. The empirical results of this paper clearly indicate that the model can be used to calculate approximate credit spreads (market prices) from EDFs, independent of the time to maturity and the industry sector under consideration. Moreover, the model is effective in an out-of-sample setting, it produces consistent results on the European bond market where data are scarce and can be adequately used to approximate credit spreads on the corporate level. },
author = {McNeil, Alexander J. and Denzler, Stefan M. and Dacorogna, Michel M. and M{\"{u}}ller, Ulrich a.},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McNeil et al. - 2006 - From default probabilities to credit spreads Credit risk models do explain market prices.pdf:pdf},
journal = {Finance Research Letters},
keywords = {actual default probability and,bond pricing,credit risk modeling,credit spread,default risk,expected default frequency,risk-neutral default probability},
number = {2},
pages = {79--95},
title = {{From default probabilities to credit spreads: Credit risk models do explain market prices}},
volume = {3},
year = {2006}
}
@book{Mita1981,
abstract = {The metabolic activation of a tryptophan pyrolysis product, 3-amino-1-methyl-5H-pyrido[4,3-b]indole(Trp-P-2), by rat liver nuclei was studied. Nuclei from the livers of rats treated with polychlorinated biphenyl (PCB) or 3-methylcholanthrene (MC) showed high mutagenic activity with Trp-P-2 in the Ames test, but activities with nuclei of untreated or phenobarbital (PB)-treated rat livers were quite low. The formation of N-hydroxy-Trp-P-2 by nuclei of PCB- or MC-treated rat livers was greater than that by nuclei of untreated or PB-treated rat livers. Similar results were observed with microsomes, which suggests that Trp-P-2 is metabolized by the same type of monoxygenase system in nuclei as in microsomes. {\textcopyright} 1981.},
author = {Mita, Shiro and Yamazoe, Yasushi and Kamataki, Tetsuya and Kato, Ryuichi},
booktitle = {Cancer Letters},
doi = {10.1016/0304-3835(81)90152-X},
file = {:C$\backslash$:/Users/user-pc/Desktop/2015{\_}Book{\_}DataMining.pdf:pdf},
isbn = {9783319141411},
issn = {03043835},
number = {3},
pages = {261--266},
title = {{Metabolic activation of a tryptophan pyrolysis product, 3-amino-1-methyl-5H-pyrido[4,3-b]indole(Trp-P-2) by isolated rat liver nuclei}},
volume = {14},
year = {1981}
}
@article{RDevelopmentCoreTeam2012,
author = {{R Development Core Team}},
title = {{R: A language and environment for statistical computing}},
year = {2012}
}
@techreport{StatsSA2012,
author = {{Stats SA}},
publisher = {Report 03-19-00},
title = {{Social profile of vulnerable groups in South Africa, 2002–2011}},
year = {2012}
}
@article{Heitjan1990,
abstract = {Multiple imputation is applied to a demographic data set with coarse age measurements for Tanzanian children. The heaped ages are multiply imputed with plausible true ages using (a) a simple naive model and (b) a new, relatively complex model that relates true age to the observed values of heaped age, sex, and anthropometric variables. The imputed true ages are used to create valid inferences under the models and compare inferences across models, thereby revealing sensitivity of inferences to prior specifications, from naive to complex. In addition, diagnostic analyses applied to the imputed data are used to suggest which models appear most appropriate. Because it is not clear just what set of heaping intervals should be used, the models are applied under various assumptions about the heaping: rounding (to the nearest year or half year) versus a combination of rounding and truncation as practiced in the United States, and medium versus wide heaping interval sizes. The most striking conclusions are the following: (a) inferences are very sensitive to the assumption of strict rounding versus rounding combined with truncation, yet judging from the diagnostics, the data cannot distinguish between such models; and (b) the diagnostics consistently favor the new, more complex model, which, although theoretically more satisfactory, can lead to inferences very similar to those obtained with the naive model. It is concluded that knowledge of the interval widths and heaping process sharpens valid inferences from data of this kind, and that given a specified process, simple and easily programmed multiple-imputation methods can lead to valid inferences.},
author = {Heitjan, Daniel F and Rubin, Donald B},
doi = {10.2307/2289765},
isbn = {01621459},
journal = {Journal of the American Statistical Association},
number = {410},
pages = {304--314},
publisher = {Taylor {\&} Francis, Ltd. on behalf of the American Statistical Association},
title = {{Inference from Coarse Data Via Multiple Imputation with Application to Age Heaping}},
url = {http://www.jstor.org/stable/2289765},
volume = {85},
year = {1990}
}
@article{wuthrich2019yes,
author = {W{\"{u}}thrich, Mario V and Merz, Michael},
journal = {ASTIN Bulletin: The Journal of the IAA},
number = {1},
pages = {1--3},
publisher = {Cambridge University Press},
title = {{Yes, we CANN!}},
volume = {49},
year = {2019}
}
@article{Bennett1984,
abstract = {Age-specific population growth rates were introduced to demographic analysis in earlier work by Bennett and Horiuchi (1981) and Preston and Coale (1982). In this paper, we derive a method which uses these growth rates to transform what may be a set of incompletely recorded deaths by age into a life table that accurately reflects the true mortality experience of the population under study. The method does not rely on the assumption of stability and, for example, in contrast to intercensal cohort survival techniques, is simple to implement when presented with nontraditional intercensal interval lengths. Thus we can obtain mortality estimates for less developed countries with defective data, despite departures from stability. Further, we assess the sensitivity of the method to violations in various assumptions underlying the procedure: error in estimated growth rates, existence of non-zero net intercensal migration, age dependence in the completeness of death registration, and misreporting of age at death and age in the population. We demonstrate the use of the method in an application to data referring to Argentine females during the period 1960 to 1970. {\textcopyright} 1984 Population Association of America.},
author = {Bennett, Neil G. and Horiuchi, Shiro},
doi = {10.2307/2061041},
isbn = {0070-3370},
issn = {00703370},
journal = {Demography},
number = {2},
pages = {217--233},
pmid = {6734860},
title = {{Mortality estimation from registered deaths in less developed countries}},
volume = {21},
year = {1984}
}
@article{Richman2019,
author = {Richman, Ronald and W{\"{u}}thrich, Mario V.},
doi = {10.2139/ssrn.3441030},
journal = {SSRN Electronic Journal},
month = {sep},
publisher = {Elsevier BV},
title = {{Lee and Carter go Machine Learning: Recurrent Neural Networks}},
year = {2019}
}
@book{Dorrington2019,
author = {Dorrington, Rob and Bradshaw, Debbie},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dorrington, Bradshaw - 2019 - RAPID MORTALITY SURVEILLANCE REPORT 2017.pdf:pdf},
isbn = {9781928340362},
title = {{RAPID MORTALITY SURVEILLANCE REPORT 2017}},
url = {www.mrc.ac.za/bod/reports.htm},
year = {2019}
}
@phdthesis{Loon2017,
author = {van Loon, Paul Rene Frank},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Loon - 2017 - EMPIRICAL STUDIES IN CORPORATE CREDIT MODELLING LIQUIDITY PREMIA, FACTOR PORTFOLIOS {\&} MODEL UNCERTAINTY.pdf:pdf},
number = {March},
school = {Heriot-Watt University},
title = {{EMPIRICAL STUDIES IN CORPORATE CREDIT MODELLING; LIQUIDITY PREMIA, FACTOR PORTFOLIOS {\&} MODEL UNCERTAINTY}},
year = {2017}
}
@article{Lu2012,
author = {Lu, J L C and Wong, W and Bajekal, M},
isbn = {2044-0456},
journal = {British Actuarial Journal},
pages = {1--35},
title = {{Mortality improvement by socio-economic circumstances in England (1982 to 2006)}},
year = {2012}
}
@book{Pearl2018,
abstract = {A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence "Correlation is not causation." This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality--the study of cause and effect--on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.},
address = {New York},
author = {Pearl, Judea and Mackenzie, D},
booktitle = {Basic Books},
doi = {10.1090/noti1912},
isbn = {0465097618},
issn = {0002-9920},
publisher = {Basic Books},
title = {{The Book of Why: the New Science of Cause and Effect}},
year = {2018}
}
@article{Makridakis2018,
abstract = {The M4 competition is the continuation of three previous competitions started more than 45 years ago whose purpose was to learn how to improve forecasting accuracy, and how such learning can be applied to advance the theory and practice of forecasting. The purpose of M4 was to replicate the results of the previous ones and extend them into three directions: First significantly increase the number of series, second include Machine Learning (ML) forecasting methods, and third evaluate both point forecasts and prediction intervals. The five major findings of the M4 Competitions are: 1. Out Of the 17 most accurate methods, 12 were “combinations” of mostly statistical approaches. 2. The biggest surprise was a “hybrid” approach that utilized both statistical and ML features. This method's average sMAPE was close to 10{\%} more accurate than the combination benchmark used to compare the submitted methods. 3. The second most accurate method was a combination of seven statistical methods and one ML one, with the weights for the averaging being calculated by a ML algorithm that was trained to minimize the forecasting. 4. The two most accurate methods also achieved an amazing success in specifying the 95{\%} prediction intervals correctly. 5. The six pure ML methods performed poorly, with none of them being more accurate than the combination benchmark and only one being more accurate than Na{\"{i}}ve2. This paper presents some initial results of M4, its major findings and a logical conclusion. Finally, it outlines what the authors consider to be the way forward for the field of forecasting.},
author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
doi = {10.1016/j.ijforecast.2018.06.001},
isbn = {0169-2070},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Benchmarking methods,Forecasting accuracy,Forecasting competitions,M Competitions,Machine Learning (ML) methods,Practice of forecasting,Prediction intervals (PIs),Time series methods},
month = {oct},
number = {4},
pages = {802--808},
publisher = {Elsevier B.V.},
title = {{The M4 Competition: Results, findings, conclusion and way forward}},
url = {https://www.sciencedirect.com/science/article/pii/S0169207018300785},
volume = {34},
year = {2018}
}
@article{DBLP:journals/corr/IoffeS15,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
pages = {448--456},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
url = {http://arxiv.org/abs/1502.03167},
volume = {1},
year = {2015}
}
@article{Keren2018,
abstract = {Ongoing developments in neural network models are continually advancing the state-of-the-art in terms of system accuracy. However, the predicted labels should not be regarded as the only core output; also important is a well-calibrated estimate of the prediction uncertainty. Such estimates and their calibration are critical in many practical applications. Despite their obvious aforementioned advantage in relation to accuracy, contemporary neural networks can, generally, be regarded as poorly calibrated and as such do not produce reliable output probability estimates. Furthermore, while post-processing calibration solutions can be found in the relevant literature, these tend to be for systems performing classification. In this regard, we herein present two novel methods for acquiring calibrated predictions intervals for neural network regressors: Empirical calibration and temperature scaling. In experiments using different regression tasks from the audio and computer vision domains, we find that both our proposed methods are indeed capable of producing calibrated prediction intervals for neural network regressors with any desired confidence level, a finding that is consistent across all datasets and neural network architectures we experimented with. In addition, we derive an additional practical recommendation for producing more accurate calibrated prediction intervals. We release the source code implementing our proposed methods for computing calibrated predicted intervals.},
archivePrefix = {arXiv},
arxivId = {1803.09546},
author = {Keren, Gil and Cummins, Nicholas and Schuller, Bjorn},
doi = {10.1109/ACCESS.2018.2871713},
eprint = {1803.09546},
issn = {21693536},
journal = {IEEE Access},
keywords = {Machine learning,artificial neural networks},
pages = {54033--54041},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Calibrated prediction intervals for neural network regressors}},
volume = {6},
year = {2018}
}
@misc{Dorrington2013a,
address = {Paris},
author = {Dorrington, R E},
booktitle = {Moultrie,T., Dorrington, R., Hill, A., Hill, K., Tim{\ae}us, I., Zaba, B.(Eds.), Tools for Demographic Estimation. Paris: International Union for the Scientific Study of Population},
number = {2016/01/31},
publisher = {International Union for the Scientific Study of Population},
title = {{The Generalized Growth Balance Method}},
url = {http://demographicestimation.iussp.org/content/generalized-growth-balance-method},
volume = {2016},
year = {2013}
}
@book{Buhlmann2006,
author = {B{\"{u}}hlmann, Hans and Gisler, Alois},
booktitle = {Scandinavian Actuarial Journal},
doi = {10.1080/03461230600889660},
isbn = {354029273X},
issn = {0346-1238},
publisher = {Springer Science {\&} Business Media},
title = {{A Course in Credibility Theory and its Applications}},
year = {2005}
}
@article{Wuthrich2019a,
abstract = {Generalized linear models have the important property of providing unbiased estimates on a portfolio level. This implies that generalized linear models manage to provide accurate prices on a portfolio level. On the other hand, neural networks may provide very accurate prices on an individual policy level, but state-of-the-art use of neural networks does not pay any attention to unbiasedness on the portfolio level. This is an implicit consequence of applying early stopping rules in gradient descent methods for model fitting. In the present paper we discuss this deficiency and we provide two different techniques to overcome this drawback of neural network model fitting.},
author = {W{\"{u}}thrich, Mario V.},
doi = {10.1007/s13385-019-00215-z},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Balance property,Exponential dispersion family,Generalized linear model,Gradient descent method,Neural network,Regression tree,Unbiasedness},
month = {oct},
pages = {1--24},
publisher = {Springer Berlin Heidelberg},
title = {{Bias regularization in neural network models for general insurance pricing}},
year = {2019}
}
@techreport{StatsSA2014,
author = {{Stats SA}},
title = {{Census 2011: Profile of older persons in South Africa}},
year = {2014}
}
@article{Cairns2006,
abstract = {In this article, we consider the evolution of the post-age-60 mortality curve in the United Kingdom and its impact on the pricing of the risk associated with aggregate mortality improvements over time: so-called longevity risk. We introduce a two-factor stochastic model for the development of this curve through time. The first factor affects mortality-rate dynamics at all ages in the same way, whereas the second factor affects mortality-rate dynamics at higher ages much more than at lower ages. The article then examines the pricing of longevity bonds with different terms to maturity referenced to different cohorts. We find that longevity risk over relatively short time horizons is very low, but at horizons in excess of ten years it begins to pick up very rapidly. A key component of the article is the proposal and development of a method for calculating the market risk-adjusted price of a longevity bond. The proposed adjustment includes not just an allowance for the underlying stochastic mortality, but also makes an allowance for parameter risk. We utilize the pricing information contained in the November 2004 European Investment Bank longevity bond to make inferences about the likely market prices of the risks in the model. Based on these, we investigate how future issues might be priced to ensure an absence of arbitrage between bonds with different characteristics. {\textcopyright} The Journal of Risk and Insurance, 2006,.},
author = {Cairns, Andrew J.G. and Blake, David and Dowd, Kevin},
doi = {10.1111/j.1539-6975.2006.00195.x},
isbn = {0022-4367
1539-6975},
issn = {00224367},
journal = {Journal of Risk and Insurance},
number = {4},
pages = {687--718},
title = {{A two-factor model for stochastic mortality with parameter uncertainty: Theory and calibration}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1539-6975.2006.00195.x/abstract},
volume = {73},
year = {2006}
}
@article{Jasilionis2015,
author = {Jasilionis, Domantas and Scholz, Rembrandt and Jdanov, Dmitri},
title = {{ABOUT MORTALITY DATA FOR NEW ZEALAND}},
year = {2015}
}
@misc{Chollet2015,
author = {Chollet, F},
shorttitle = {Keras},
title = {{Keras}},
url = {http://keras.io keras.io},
year = {2015}
}
@article{Bennett1981,
abstract = {Death registration statistics, even when incomplete, can provide valuable information about mortality. In particular, the age structure of deaths can be used to estimate the completeness of registration, provided that this completeness does not vary substantially with age. Two methods of estimating the completeness of death registration from the distribution of deaths by age are described. The first is derived from stable population theory and requires an estimate of the rate of natural increase of the population, as well as assuming stability. However, the technique can also be used to generate simultaneously estimates of the rate of natural increase and of death registration completeness. The second method which requires two census age distributions and intercensal deaths by age, estimates the relative enumeration completeness of the two censuses as well as the completeness of death registration and requires only that the population be closed. Results are sensitive to overstatement of age. The methods are illustrated by being applied to figures from Thailand for the period 1960-70 and are found to work satisfactorily. {\textcopyright} 1980 Taylor and Francis Group, LLC.},
author = {Preston, S. and Hill, K.},
doi = {10.1080/00324728.1980.10410395},
isbn = {0032-4701},
issn = {14774747},
journal = {Population Studies},
number = {2},
pages = {349--366},
title = {{Estimating the completeness of death registration}},
volume = {34},
year = {1980}
}
@article{Nievergelt1969,
abstract = {Perceptrons were invented in the fifties when “learning machine” was an exciting new concept. For a decade thereafter, there has been much describing, experimenting, and speculating about what perceptrons can and cannot do. Discussions of this topic were typically lively and vague, because the underlying model and the concepts used were rarely completely defined. Copyright {\textcopyright} 1969 by The Institute of Electrical and Electronics Engineers, Inc.},
author = {Nievergelt, J.},
doi = {10.1109/T-C.1969.222718},
isbn = {9780262534772},
issn = {00189340},
journal = {IEEE Transactions on Computers},
number = {6},
pages = {572},
title = {{R69-13 Perceptrons: An Introduction to Computational Geometry}},
volume = {C-18},
year = {1969}
}
@article{Zou2005,
abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso. {\textcopyright} 2005 Royal Statistical Society.},
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00503.x},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Zou-2005-Regularization and variable selection.pdf:pdf},
isbn = {1467-9868},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Grouping effect,LARS algorithm,Lasso,P ≫ n problem,Penalization,Variable selection},
number = {2},
pages = {301--320},
title = {{Regularization and variable selection via the elastic net}},
volume = {67},
year = {2005}
}
@article{Richman2020,
author = {Perla, Francesca and Richman, Ronald and Scognamiglio, Salvatore and W{\"{u}}thrich, Mario V.},
journal = {SSRN Electronic Journal},
title = {{Time-Series Forecasting of Mortality Rates using Deep Learning}},
year = {2020}
}
@techreport{ActuarialSocietyofSouthAfrica2014,
address = {Cape Town},
author = {{Actuarial Society of South Africa}},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Actuarial Society of South Africa - 2014 - SAP 901 - General Actuarial Practice.pdf:pdf},
institution = {Actuarial Society of South Africa},
title = {{SAP 901 - General Actuarial Practice}},
year = {2014}
}
@article{Li2009,
abstract = {Traditionally, actuaries have modeled mortality improvement using deterministic reduction factors, with little consideration of the associated uncertainty. As mortality improvement has become an increasingly significant source of financial risk, it has become important to measure the uncertainty in the forecasts. Probabilistic confidence intervals provided by the widely accepted Lee-Carter model are known to be excessively narrow, due primarily to the rigid structure of the model. In this paper, we relax the model structure by considering individual differences (heterogeneity) in each age-period cell. The proposed extension not only provides a better goodness-of-fit based on standard model selection criteria, but also ensures more conservative interval forecasts of central death rates and hence can better reflect the uncertainty entailed. We illustrate the results using US and Canadian mortality data.},
author = {Li, Johnny Siu-Hang and Hardy, Mary R. and Tan, Ken Seng},
doi = {10.2143/ast.39.1.2038060},
issn = {0515-0361},
journal = {ASTIN Bulletin},
month = {may},
number = {1},
pages = {137--164},
publisher = {Cambridge University Press (CUP)},
title = {{Uncertainty in Mortality Forecasting: An Extension to the Classical Lee-Carter Approach}},
volume = {39},
year = {2009}
}
@incollection{tsanakas2018beyond,
abstract = {We critically assess the dominant conceptualization of model risk, which focuses on the risk arising from the propagation of model flaws into poor decisions, mediated by a rigid decision process. We argue that this notion of model risk suffers from significant limitations: it reflects a partial view of rationality and fails to acknowledge the diversity of uses of (and interactions through) models taking place in insurance organizations. Our empirical research on internal models in the London insurance market shows that models are used by practitioners with a sophisticated understanding of their limitations and are enmeshed in a variety of discourses, going beyond questions of technical validity. Appreciating the ways in which quantitative models are used in practice is crucial to understanding what risks the use of such models may entail and how a governance response is to be structured.},
author = {Tsanakas, Andreas and Cabantous, Laure},
booktitle = {Risk Modeling for Hazards and Disasters},
doi = {10.1016/B978-0-12-804071-3.00015-X},
isbn = {9780128040935},
keywords = {Capital model,Cultural theory,Insurance,Insurance practice,Internal model,Model governance,Model risk,Model uncertainty,Plural rationalities,Solvency II},
pages = {299--305},
publisher = {Elsevier},
title = {{Beyond "Model Risk": A Practice Perspective on Modeling in Insurance}},
year = {2017}
}
@article{Li,
author = {Li, Nan and Gerland, Patrick},
title = {{Using census data to estimate old-age mortality for developing countries}}
}
@book{Oberguggenberger,
author = {Oberguggenberger, Michael and Ostermann, Alexander},
file = {:C$\backslash$:/Users/user-pc/Desktop/2018{\_}Book{\_}AnalysisForComputerScientists.pdf:pdf},
isbn = {9783319911540},
title = {{Undergraduate Topics in Computer Science Analysis for Computer Scientists Foundations, Methods, and Algorithms Second Edition}},
url = {http://www.springer.com/series/7592}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
isbn = {1476-4687},
issn = {00280836},
journal = {Nature},
number = {6088},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@article{Aseervatham2016,
abstract = {As of 21 December 2012, the use of gender as an insurance rating category is prohibited in the EU. Any remaining pricing disparities between men and women will now be traced back to the reasonable pricing of characteristics that happen to differ between the groups or to the pricing of characteristics that differ between sexes in a way that proxies for gender. Using data from an automobile insurer, we analyse how the standard industry approach of simply omitting gender from the pricing formula, which allows for proxy effects, differs from the benchmark for what prices would look like if direct gender effects were removed and other variables did not adjust as proxies. We find that the standard industry approach will likely be influenced by proxy effects for younger and older drivers. Our method can simply be applied to almost any setting where a regulator is considering a uniform pricing reform.},
author = {Aseervatham, Vijay and Lex, Christoph and Spindler, Martin},
doi = {10.1057/gpp.2015.22},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aseervatham, Lex, Spindler - 2016 - How Do Unisex Rating Regulations Affect Gender Differences in Insurance Premiums.pdf:pdf},
issn = {14680440},
journal = {Geneva Papers on Risk and Insurance: Issues and Practice},
keywords = {automobile insurance,risk classification,unisex premium},
number = {1},
pages = {128--160},
title = {{How do unisex rating regulations affect gender differences in insurance premiums?}},
url = {www.genevaassociation.org},
volume = {41},
year = {2016}
}
@techreport{CEIOPS2010,
author = {{Committee of European Insurance and Occupational Pensions Supervisors (CEIOPS)}},
institution = {Committee of Insurance and Occupational Pensions Regulators (CEIOPS)},
number = {Available at: https://eiopa.europa.eu},
title = {{CEIOPS Advice for Level 2 Implementing Measures on Solvency II: Article 111 and 304 -- Equity Risk Sub-Module}},
url = {https://eiopa.europa.eu},
year = {2010}
}
@inproceedings{Hwang2005,
abstract = {This paper deals with the estimation of the linear and the nonlinear quantile regressions using the idea of support vector machine. Accordingly, the optimization problem is transformed into the Lagrangian dual problem, which is easier to solve. In particular, for the nonlinear quantile regression the idea of kernel function is introduced, which allows us to perform operations in the input space rather than the high dimensional feature space. Experimental results are then presented which illustrate the performance of the proposed method. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
author = {Hwang, Changha and Shim, Jooyong},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/11539087_66},
issn = {03029743},
number = {PART I},
pages = {512--520},
publisher = {Springer, Berlin, Heidelberg},
title = {{A simple quantile regression via support vector machine}},
volume = {3610},
year = {2005}
}
@techreport{UN2015,
abstract = {The 193-Member United Nations General Assembly today formally adopted the 2030 Agenda for Sustainable Development, along with a set of bold new Global Goals, which Secretary-General Ban Ki-moon hailed as a universal, integrated and transformative vision for a better world. “The new agenda is a promise by leaders to all people everywhere. It is an agenda for people, to end poverty in all its forms – an agenda for the planet, our common home,” declared Mr. Ban as he opened the UN Sustainable Development Summit which kicked off today and wraps up Sunday. The UN chief's address came ahead of the Assembly's formal adoption of the new framework, Transforming Our World: the 2030 Agenda for Sustainable Development, which is composed of 17 goals and 169 targets to wipe out poverty, fight inequality and tackle climate over the next 15 years. The Goals aim to build on the work of the historic Millennium Development Goals (MDGs), which in September 2000, rallied the world around a common 15-year agenda to tackle the indignity of poverty. The Summit opened with a full programme of events, including a screening of the film The Earth From Space, performances by UN Goodwill Ambassadors Shakira Angelique Kidjo, as well as call to action by female education advocate and the youngest-ever Nobel Laureate, Malala Yousafzai along with youth representatives as torch bearers to a sustainable future. The adoption ceremony was presided over by Danish Prime Minister Lars L{\o}kke Rasmussen and Ugandan President Yoweri Kaguta Museveni, who stressed the successes of the MDGSs and the need for the full implementation of the new Agenda. Speaking to the press after the adoption of the Agenda, Mr. Ban said: “These Goals are a blueprint for a better future. Now we must use the goals to transform the world. We will do that through partnership and through commitment. We must leave no-one behind. In his opening address to the Assembly, which also marks the Organization's 70th anniversary, the UN chief hailed the new framework as an agenda for shared prosperity, peace and partnership. “It conveys the urgency of climate action. It is rooted in gender equality and respect for the rights of all.” Mr. Ban urged the world leaders and others convened at the event to successfully implement the Global Goals or Agenda 30 by launching ‘renewed global partnership.' “The 2030 Agenda compels us to look beyond national boundaries and short-term interests and act in solidarity for the long-term. We can no longer afford to think and work in silos. Institutions will have to become fit for a grand new purpose. The United Nations system is strongly committed to supporting Member States in this great new endeavour,” said Mr. Ban. “We must engage all actors, as we did in shaping the Agenda. We must include parliaments and local governments, and work with cities and rural areas. We must rally businesses and entrepreneurs. We must involve civil society in defining and implementing policies – and give it the space to hold us to account. We must listen to scientists and academia. We will need to embrace a data revolution. Most important, we must set to work – now,” added the Secretary-General. “Seventy years ago, the United Nations rose from the ashes of war. Governments agreed on a visionary Charter dedicated to ‘We the Peoples'. The Agenda you are adopting today advances the goals of the Charter. It embodies the aspirations of people everywhere for lives of peace, security and dignity on a healthy planet,” said Mr. Ban. General Assembly President Mogens Lykketoft called the 2030 Agenda on Sustainable Development “ambitious” in confronting the injustices of poverty, marginalization and discrimination. “We recognize the need to reduce inequalities and to protect our common home by changing unsustainable patterns of consumption and production. And, we identify the overwhelming need to address the politics of division, corruption and irresponsibility that fuel conflict and hold back development,” he said. On the adoption of the new agenda, UN Economic and Social Council President (ECOSOC) Oh Joon said action on Sustainable Development Goals must start immediately. “The Economic and Social Council stands ready to kick-start the work on the new agenda,” he added. Source: UN News Centre},
address = {New York},
author = {UN},
booktitle = {United Nations},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/UN - 2015 - Transforming Our World the 2030 Agenda for Sustainable Development United Nations United Nations Transforming Our World the.pdf:pdf},
institution = {United Nations},
keywords = {2030 Agenda for Sustainable Development},
number = {October},
pages = {1--35},
title = {{Transforming Our World: the 2030 Agenda for Sustainable Development United Nations United Nations Transforming Our World: the 2030 Agenda for Sustainable Development. A/RES/70/1}},
url = {sustainabledevelopment.un.org https://www.un.org/en/development/desa/population/migration/generalassembly/docs/globalcompact/A{\_}RES{\_}70{\_}1{\_}E.pdf},
volume = {16301},
year = {2015}
}
@article{Wuthrich2018b,
abstract = {Machine learning techniques make it feasible to calculate claims reserves on individual claims data. This paper illustrates how these techniques can be used by providing an explicit example in individual claims reserving.},
author = {W{\"{u}}thrich, Mario V.},
doi = {10.1080/03461238.2018.1428681},
isbn = {0346-1238},
issn = {16512030},
journal = {Scandinavian Actuarial Journal},
keywords = {Individual claims data,individual claims reserving,machine learning,micro-level stochastic reserving,regression tree},
number = {6},
pages = {465--480},
title = {{Machine learning in individual claims reserving}},
volume = {2018},
year = {2018}
}
@book{Berman,
abstract = {This book presents a basic introduction to quantum mechanics at the undergraduate level. Depending on the choice of topics, it can be used for a one-semester or two-semester course. An attempt has been made to anticipate the conceptual problems students encounter when they first study quantum mechanics. Wherever possible, examples are given to illustrate the underlying physics associated with the mathematical equations of quantum mechanics. To this end, connections are made with corresponding phenomena in classical mechanics and electromagnetism. The problems at the end of each chapter are intended to help students master the course material and to explore more advanced topics. Many calculations exploit the extraordinary capabilities of computer programs such as Mathematica, MatLab, and Maple. Students are urged to use these programs, just as they had been urged to use calculators in the past. The treatment of various topics is rather complete, in that most steps in derivations are included. Several of the chapters go beyond what is traditionally covered in an introductory course. The goal of the presentation is to provide the students with a solid background in quantum mechanics.{\#}xE000. Introduction -- Mathematical Introduction -- Free Particle Schroedinger Equation -- Free-Particle Wave Packets -- Schroedinger's Equation with Potential Energy: Introduction to Operators -- Postulates and Basic Elements of Quantum Mechanics: Properties of Operators -- Problems in 1-dimension: General Considerations, Infinite Well Potential, Piecewise Constant Potentials, and Delta Function Potentials -- Simple Harmonic Oscillator -- One Dimension -- Problems in 2 and 3-dimensions -- General Considerations -- Central Forces and Angular Momentum -- Spherically Symmetric Potentials -- Radial Equation -- Dirac Notation -- Spin -- Important Basics from Phys 453 -- Perturbation Theory -- Variational Approach -- WKB Approximation -- Scattering -- 1-D -- Scattering -- 3-D -- Symmetries and Transformations -- Rotations -- Examples -- Addition of Angular Momentum: Clebsch-Gordan Coefficients -- Vector and Tensor Operators: Wigner-Eckart Theorem -- Spin-Orbit Interactions -- Hydrogen Atom with Spin in External Fields -- Time-Dependent Problems -- Approximation Techniques in Time-Dependent Problems -- Fermi's Golden Rule.},
author = {Berman, Paul R.},
file = {:C$\backslash$:/Users/user-pc/Desktop/2018{\_}Book{\_}IntroductoryQuantumMechanics.pdf:pdf},
isbn = {9783319685960},
pages = {637},
title = {{Introductory quantum mechanics : a traditional approach emphasizing connections with classical physics}}
}
@article{cortes1995support,
author = {Cortes, Corinna and Vapnik, Vladimir},
journal = {Machine learning},
number = {3},
pages = {273--297},
publisher = {Springer},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@article{Darikwa2013,
author = {Darikwa, Timotheus B and Dorrington, Rob},
title = {{The level and trends of child mortality in South Africa, 1996-2006}},
year = {2013}
}
@article{General2014,
abstract = {The general-purpose drawing package Tik Z can be used to typeset commutative diagrams and other kinds of mathematical pictures, generating high-quality results. The present package facilitates the creation of such diagrams by providing a convenient set of macros and reasonable default settings. Familiarity with Tik Z is helpful but not necessary, since the examples contained here cover the most common situations. This software is distributed under the terms of the GNU General Public License, version 3 or later.},
author = {General, G N U and License, Public},
file = {:C$\backslash$:/R/refas/tikz-cd-doc.pdf:pdf},
number = {February},
pages = {0--32},
title = {{Commutative diagrams with Ti k Z}},
year = {2014}
}
@article{Bradshaw2004,
abstract = {This article discusses the adoption in 1998 by South Africa of a rapid surveillance system to monitor the changing age pattern of registered deaths. Data from the population register maintained by the Department of Home Affairs showed a steady increase in the number of adult deaths (aged {\textgreater}=15 years) between 1998 and 2003. These data also showed continuing shifts in the age distribution of deaths with a large increase in the number of young adult deaths, particularly among women. Data analysis, taking into account both population growth and the increasing registration of deaths, demonstrated rapidly changing mortality patterns following an age pattern consistent with the heterosexual human immunodeficiency virus (HIV)/acquired immune deficiency syndrome (AIDS) epidemic. It is concluded that the South African government should implement a comprehensive plan to prevent and treat HIV/AIDS as rapidly as possible.},
author = {Bradshaw, Debbie and Laubscher, Ria and Dorrington, Rob and Bourne, David E. and Tim{\ae}us, Ian M.},
doi = {10.7196/SAMJ.2552},
isbn = {0038-2469},
issn = {02569574},
journal = {South African Medical Journal},
number = {4},
pages = {278--279},
title = {{Unabated rise in number of adult deaths in South Africa [7]}},
volume = {94},
year = {2004}
}
@article{Gray1986,
abstract = {Methods for correcting for underenumeration in mortality estimates have been developing intensively during the last fifteen years. While existing methods can be shown to be overspecified, this is particularly evident for techniques in which age-specific growth rates are used. The paper surveys some existing analytical techniques which do not use age-specific growth rates, by examining results and precision when used with 24 selected data sets. The specification problem is then analysed, and a new less-specified technique is introduced. The technique assumes an age-invariant rate of underenumeration of deaths, but allows age-specific growth rates to vary in a minimally consistent manner from a common general level. The results obtained by using this technique on the earlier data sets are presented, and the precision obtained is compared with the results from existing methods. The results are very favourable to the new technique.},
author = {Gray, Alan},
doi = {10.2307/2174584},
isbn = {00324728},
journal = {Population Studies},
number = {3},
pages = {425--436},
publisher = {Taylor {\&} Francis, Ltd. on behalf of the Population Investigation Committee},
title = {{Sectional Growth Balance Analysis for Non-stable Closed Populations}},
url = {http://www.jstor.org/stable/2174584},
volume = {40},
year = {1986}
}
@article{Mullainathan2017,
abstract = {Machines are increasingly doing "intelligent" things. Face recognition algorithms use a large dataset of photos labeled as having a face or not to estimate a function that predicts the presence y of a face from pixels x. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble—and thus where they can be most usefully applied.},
author = {Mullainathan, Sendhil and Spiess, Jann},
doi = {10.1257/jep.31.2.87},
isbn = {0895-3309},
issn = {08953309},
journal = {Journal of Economic Perspectives},
number = {2},
pages = {87--106},
title = {{Machine learning: An applied econometric approach}},
volume = {31},
year = {2017}
}
@book{Moultrie2013,
author = {Moultrie, T and Dorrington, R and Hill, A and Hill, K and Timaeus, I and Zaba, B},
booktitle = {Paris: International Union for the Scientific Study of Population},
number = {2016/01/31},
title = {{Tools for Demographic Estimation}},
url = {http://demographicestimation.iussp.org/},
year = {2013}
}
@article{Deprez2017a,
abstract = {Various stochastic models have been proposed to estimate mortality rates. In this paper we illustrate how machine learning techniques allow us to analyze the quality of such mortality models. In addition, we present how these techniques can be used for differentiating the different causes of death in mortality modeling.},
archivePrefix = {arXiv},
arxivId = {1705.03396},
author = {Deprez, Philippe and Shevchenko, Pavel V. and W{\"{u}}thrich, Mario V.},
doi = {10.1007/s13385-017-0152-4},
eprint = {1705.03396},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Boosting,Cause-of-death mortality,Machine learning,Mortality modeling,Regression},
month = {dec},
number = {2},
pages = {337--352},
publisher = {Springer Berlin Heidelberg},
title = {{Machine learning techniques for mortality modeling}},
volume = {7},
year = {2017}
}
@article{Klinker2010,
abstract = {GLMs that include explanatory classification variables with sparsely populated levels assign large standard errors to these levels but do not otherwise shrink estimates toward the mean in response to low credibility. Accordingly, actuaries have attempted to superimpose credibility on a GLM setting, but the resulting methods do not appear to have caught on. The Generalized Linear Mixed Model (GLMM) is yet another way of introducing credibility-like shrinkage toward the mean in a GLM setting. Recently available statistical software, such as SAS PROC GLIMMIX, renders these models more readily accessible to actuaries. This paper offers background on GLMMs and presents a case study displaying shrinkage towards the mean very similar to Buhlmann-Straub credibility. Keywords:},
author = {Klinker, Fred},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klinker, {\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_} - 2011 - Generalized Linear Mixed Models for Rat.pdf:pdf},
journal = {Casualty Actuarial Society E-Forum, Winter 2011 Volume 2},
keywords = {{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_},credibility,generalized,generalized linear models,glmms,glms,linear mixed effects,linear mixed models,lme,models},
number = {1},
pages = {1--25},
title = {{Generalized Linear Mixed Models for Ratemaking: A Means of Introducing Credibility into a Generalized Linear Model Setting}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Generalized+Linear+Mixed+Models+for+Ratemaking+:+A+Means+of+Introducing+Credibility+into+a+Generalized+Linear+Model+Setting{\#}0},
volume = {2},
year = {2010}
}
@article{Hainaut2018a,
abstract = {This article explores the capacity of self-organizing maps (SOMs) for analysing non-life insurance data. Contrary to feed forward neural networks, also called perceptron, a SOM does not need any a priori information on the relevancy of variables. During the learning procedure, the SOM algorithm selects the most relevant combination of explanatory variables and reduces by this way the collinearity bias. However, the specific features of insurance data require adapting the classic SOM framework to manage categorical variables and the low frequency of claims. This work proposes several extensions of SOMs in order to study the claims frequency of a portfolio of motorcycle insurances.},
author = {Hainaut, Donatien},
doi = {10.1007/s13385-018-0189-z},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hainaut-2018-A self-organizing predictive map.pdf:pdf},
issn = {2190-9733},
journal = {European Actuarial Journal},
number = {1},
pages = {173--207},
shorttitle = {A self-organizing predictive map for non-life insu},
title = {{A self-organizing predictive map for non-life insurance}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=3099979},
volume = {9},
year = {2019}
}
@misc{NationalHealthService2019,
author = {{National Health Service}},
shorttitle = {Supporting Information},
title = {{Supporting Information: Lower Layer Super Output Area}},
url = {https://bit.ly/2pTW7nc},
urldate = {2019-11-21},
year = {2019}
}
@article{Li2004,
author = {Li, Nan and Lee, Ronald and Tuljapurkar, Shripad},
isbn = {1751-5823},
journal = {International Statistical Review},
number = {1},
pages = {19--36},
title = {{Using the Lee–Carter Method to Forecast Mortality for Populations with Limited Data*}},
volume = {72},
year = {2004}
}
@article{Coale1990,
author = {Coale, Ansley J and Caselli, Graziella},
isbn = {0016-6987},
journal = {Genus},
pages = {1--23},
title = {{Estimation of the number of persons at advanced ages from the number of deaths at each age in the given year and adjacent years}},
year = {1990}
}
@inproceedings{Krizhevsky2012,
abstract = {Nowadays, parallel computing plays a key role in high-performance computing and it is gradually replacing traditional supercomputers. The computing units (processors) in parallel computing systems are linked by different types of networks [1]. The allocation of the computing resources to tasks is a crucial issue in these complex systems. Therefore there is a need to design efficient resource allocation algorithms. Unfortunately, many traditional scheduling algorithms are not able to fulfill this requirement, due to the large amount of data that needs to be transmitted through the network, as well as synchronization and other scheduling overhead required by the processors allotted to the same task. Thus, many parallel task (PT) models have been proposed to address this problem. One of these models is the malleable task (MT) model (sometimes also called moldable task) [2]. This is a promising model that has been used in real application [3]. In the MT model, the processing time of a task depends on the number of processors allotted to it. The communication overhead and synchronization is implicitly included in the processing time. The idea behind theMTs is to provide an alternative strategy to model the communication delays. An MT includes elementary operations (e.g., a numerical routine or a nested loop) with sufficient parallelism to be amenable for multiprocessor processing. Thus, a sequential task can be regarded as a special case of the MT model. The MT model has been used in realistic applications [3,4].},
author = {Jansen, Klaus and Zhang, Hu},
booktitle = {Handbook of Approximation Algorithms and Metaheuristics},
doi = {10.1201/9781420010749},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Krizhevsky-2012-Imagenet classification with d.pdf:pdf},
isbn = {9781420010749},
pages = {45--1--45--16},
shorttitle = {Imagenet classification with deep convolutional ne},
title = {{Scheduling malleable tasks}},
year = {2007}
}
@misc{Parodi2016a,
address = {Dublin},
author = {Parodi, P},
booktitle = {GIRO 2016},
title = {{No Title}},
year = {2016}
}
@article{Pearl2009,
abstract = {This review presents empirical researcherswith recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interventions, (also called "causal effects" or "policy evaluation") (2) queries about probabilities of counterfactuals, (including assessment of "regret," "attribution" or "causes of effects") and (3) queries about direct and indirect effects (also known as "mediation"). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
author = {Pearl, Judea},
doi = {10.1214/09-SS057},
file = {:C$\backslash$:/R/refas/pearl2009causal.pdf:pdf},
issn = {19357516},
journal = {Statistics Surveys},
keywords = {Causal effects,Causes of effects,Confounding,Counterfactuals,Graphical methods,Mediation,Policy evaluation,Potential-outcome,Structural equationmodels},
number = {September},
pages = {96--146},
title = {{Causal inference in statistics: An overview}},
volume = {3},
year = {2009}
}
@article{Bailey1960,
author = {Bailey, Robert A. and Simon, LeRoy J.},
doi = {10.1017/S0515036100009569},
issn = {17831350},
journal = {ASTIN Bulletin},
number = {4},
pages = {192--217},
title = {{Two Studies in Automobile Insurance Ratemaking}},
volume = {1},
year = {1960}
}
@article{Robinson1993,
author = {Robinson, J Gregory and Ahmed, Bashir and Gupta, Prithwis Das and Woodrow, Karen A},
isbn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {423},
pages = {1061--1071},
title = {{Estimation of population coverage in the 1990 United States Census based on demographic analysis}},
volume = {88},
year = {1993}
}
@article{Meinshausen2006,
abstract = {Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.},
author = {Meinshausen, Nicolai},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meinshausen - 2006 - Quantile Regression Forests.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Adaptive neighborhood regression,Quantile regression,Random forests},
pages = {983--999},
title = {{Quantile regression forests}},
volume = {7},
year = {2006}
}
@article{Birnbaum2011,
abstract = {To quantify the deaths from human immunodeficiency virus (HIV) infection or acquired immunodeficiency syndrome (AIDS) that are misattributed to other causes in South Africa's death registration data and to adjust for this bias.},
author = {Birnbaum, Jeanette Kurian and Murray, Christopher JL and Lozano, Rafael},
doi = {10.2471/blt.11.086280},
isbn = {0042-9686},
issn = {0042-9686},
journal = {Bulletin of the World Health Organization},
number = {4},
pages = {278--285},
title = {{Exposing misclassified HIV/AIDS deaths in South Africa}},
volume = {89},
year = {2011}
}
@article{Learning2017,
author = {Learning, Machine and Introduction, An and Learning, Statistical and Statistics, Introduction-statistical-learning-applications- and Error, Training},
isbn = {1461471370},
keywords = {aforementioned topic really well,and my own future,and so,chapter,ever come across,for others,good understanding of the,i thought i should,introduction,learning,ml books i have,neither too abstract,nor too,one of the best,post pouring out my,right in the second,rigorous,self,statistical,this book provides a,to,understanding of the concept-,write a blog},
pages = {2015--2018},
title = {{Sachin Joglekar ' s blog Programming | Python | ML On the Bias / Variance tradeoff in Machine Learning}},
year = {2017}
}
@article{Aseervatham2016a,
abstract = {As of 21 December 2012, the use of gender as an insurance rating category is prohibited in the EU. Any remaining pricing disparities between men and women will now be traced back to the reasonable pricing of characteristics that happen to differ between the groups or to the pricing of characteristics that differ between sexes in a way that proxies for gender. Using data from an automobile insurer, we analyse how the standard industry approach of simply omitting gender from the pricing formula, which allows for proxy effects, differs from the benchmark for what prices would look like if direct gender effects were removed and other variables did not adjust as proxies. We find that the standard industry approach will likely be influenced by proxy effects for younger and older drivers. Our method can simply be applied to almost any setting where a regulator is considering a uniform pricing reform.},
author = {Aseervatham, Vijay and Lex, Christoph and Spindler, Martin},
doi = {10.1057/gpp.2015.22},
file = {:C$\backslash$:/Users/user-pc/Desktop/Aseervatham2016{\_}Article{\_}HowDoUnisexRatingRegulationsAf.pdf:pdf},
issn = {14680440},
journal = {Geneva Papers on Risk and Insurance: Issues and Practice},
keywords = {automobile insurance,risk classification,unisex premium},
number = {1},
pages = {128--160},
title = {{How do unisex rating regulations affect gender differences in insurance premiums?}},
volume = {41},
year = {2016}
}
@article{lipton2016mythos,
abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
archivePrefix = {arXiv},
arxivId = {1606.03490},
author = {Lipton, Zachary C.},
doi = {10.1145/3233231},
eprint = {1606.03490},
issn = {15577317},
journal = {Communications of the ACM},
number = {10},
pages = {35--43},
title = {{The mythos of model interpretability}},
volume = {61},
year = {2018}
}
@article{Richman2019d,
abstract = {The Lee-Carter (LC) model is a basic approach to forecasting mortality rates of a single population. Although extensions of the LC model to forecasting rates for multiple populations have recently been proposed, the structure of these extended models is hard to justify and the models are often difficult to calibrate, relying on customised optimisation schemes. Based on the paradigm of representation learning, we extend the LCmodel to multiple populations using neural networks, which automatically select an optimal model structure. We fit this model to mortality rates since 1950 for all countries in the Human Mortality Database and observe that the out-of-sample forecasting performance of the model is highly competitive.},
author = {Richman, R and W{\"{u}}thrich, Mario V.},
doi = {10.1017/S1748499519000071},
issn = {17485002},
journal = {Annals of Actuarial Science},
keywords = {Lee-Carter model,Mortality forecasting,multiple populations,neural networks},
title = {{A neural network extension of the Lee-Carter model to multiple populations}},
year = {2019}
}
@misc{KPMG2017,
author = {KPMG},
number = {17 June},
title = {{The Chaotic Middle}},
url = {https://assets.kpmg.com/content/dam/kpmg/us/pdf/2017/06/chaotic-middle-autonomous-vehicle-paper.pdf},
volume = {2018},
year = {2017}
}
@article{Krikler2004,
author = {Krikler, Samuel and Dolberger, Dan and Eckel, Jacob},
doi = {10.1057/palgrave.fsm.4770142},
file = {:C$\backslash$:/Users/user-pc/Downloads/krikler2004.pdf:pdf},
issn = {1363-0539},
journal = {Journal of Financial Services Marketing},
number = {1},
pages = {68--79},
title = {{Method and tools for insurance price and revenue optimisation}},
volume = {9},
year = {2004}
}
@techreport{Chen2019,
abstract = {We propose a novel approach to estimate asset pricing models for individual stock returns that takes advantage of the vast amount of conditioning information, while keeping a fully flexible form and accounting for time-variation. Our general non-linear asset pricing model is estimated with deep neural networks applied to all U.S. equity data combined with a substantial set of macroeconomic and firm-specific information. We estimate the stochastic discount factor that explains all asset returns from the conditional moment constraints implied by no-arbitrage. Our asset pricing model outperforms out-of-sample all other benchmark approaches in terms of Sharpe ratio, explained variation and pricing errors. We trace its superior performance to including the no-arbitrage constraint in the estimation and to accounting for macroeconomic conditions and non-linear interactions between firm-specific characteristics. Our generative adversarial network enforces no-arbitrage by identifying the portfolio strategies with the most pricing information. Our recurrent Long-Short-Term-Memory network finds a small set of hidden economic state processes. A feedforward network captures the non-linear effects of the conditioning variables. Our model allows us to identify the key factors that drive asset prices and generate profitable investment strategies.},
archivePrefix = {arXiv},
arxivId = {1904.00745},
author = {Chen, Luyang and Pelger, Markus and Zhu, Jason},
booktitle = {SSRN Electronic Journal},
doi = {10.2139/ssrn.3350138},
eprint = {1904.00745},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Pelger, Zhu - 2019 - Deep Learning in Asset Pricing.pdf:pdf},
keywords = {Alpha,Characteristics-Sorted Factor Models,Cross-Sectional Return,Deep Learn-ing,Firm Characteristics,Machine Learning,Pricing Errors},
title = {{Deep Learning in Asset Pricing}},
year = {2019}
}
@article{Dorrington2013,
author = {Dorrington, R E},
journal = {Moultrie,T., Dorrington, R., Hill, A., Hill, K., Tim{\ae}us, I., Zaba, B.(Eds.), Tools for Demographic Estimation. Paris: International Union for the Scientific Study of Population},
title = {{Synthetic extinct generations methods}},
year = {2013}
}
@inproceedings{ribeiro2016should,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2939672.2939778},
eprint = {1602.04938},
file = {:C$\backslash$:/R/refas/rfp0573-ribeiroA.pdf:pdf},
isbn = {9781450342322},
organization = {ACM},
pages = {1135--1144},
title = {{"Why should i trust you?" Explaining the predictions of any classifier}},
volume = {13-17-Augu},
year = {2016}
}
@article{Ramsay1988,
author = {Ramsay, James O},
isbn = {0883-4237},
journal = {Statistical science},
pages = {425--441},
title = {{Monotone regression splines in action}},
year = {1988}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. {\textcopyright} 2006 Massachusetts Institute of Technology.},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
doi = {10.1162/neco.2006.18.7.1527},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hinton-2006-A fast learning algorithm for deep.pdf:pdf},
isbn = {0899-7667},
issn = {08997667},
journal = {Neural Computation},
number = {7},
pages = {1527--1554},
pmid = {16764513},
shorttitle = {A fast learning algorithm for deep belief nets},
title = {{A fast learning algorithm for deep belief nets}},
volume = {18},
year = {2006}
}
@article{Lecun2015Nature,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
issn = {14764687},
journal = {Nature},
keywords = {Computer science,Mathematics and computing},
month = {may},
number = {7553},
pages = {436--444},
pmid = {26017442},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{glasserman2014robust,
abstract = {Financial risk measurement relies on models of prices and other market variables, but models inevitably rely on imperfect assumptions and estimates, creating model risk. Moreover, optimization decisions, such as portfolio selection, amplify the effect of model error. In this work, we develop a framework for quantifying the impact of model error and for measuring and minimizing risk in a way that is robust to model error. This robust approach starts from a baseline model and finds the worst-case error in risk measurement that would be incurred through a deviation from the baseline model, given a precise constraint on the plausibility of the deviation. Using relative entropy to constrain model distance leads to an explicit characterization of worst-case model errors; this characterization lends itself to Monte Carlo simulation, allowing straightforward calculation of bounds on model error with very little computational effort beyond that required to evaluate performance under the baseline nominal model. This approach goes well beyond the effect of errors in parameter estimates to consider errors in the underlying stochastic assumptions of the model and to characterize the greatest vulnerabilities to error in a model. We apply this approach to problems of portfolio risk measurement, credit risk, delta hedging and counterparty risk measured through credit valuation adjustment. {\textcopyright} 2013 Taylor {\&} Francis.},
author = {Glasserman, Paul and Xu, Xingbo},
doi = {10.1080/14697688.2013.822989},
issn = {14697688},
journal = {Quantitative Finance},
keywords = {Derivatives risk management,Risk management,Risk measures,Validation of pricing models},
number = {1},
pages = {29--58},
publisher = {Taylor {\&} Francis},
title = {{Robust risk measurement and model risk}},
volume = {14},
year = {2014}
}
@article{Hejazi2016,
abstract = {Managing and hedging the risks associated with Variable Annuity (VA) products require intraday valuation of key risk metrics for these products. The complex structure of VA products and computational complexity of their accurate evaluation have compelled insurance companies to adopt Monte Carlo (MC) simulations to value their large portfolios of VA products. Because the MC simulations are computationally demanding, especially for intraday valuations, insurance companies need more efficient valuation techniques. Recently, a framework based on traditional spatial interpolation techniques has been proposed that can significantly decrease the computational complexity of MC simulation (Gan and Lin, 2015). However, traditional interpolation techniques require the definition of a distance function that can significantly impact their accuracy. Moreover, none of the traditional spatial interpolation techniques provide all of the key properties of accuracy, efficiency, and granularity (Hejazi et al., 2015). In this paper, we present a neural network approach for the spatial interpolation framework that affords an efficient way to find an effective distance function. The proposed approach is accurate, efficient, and provides an accurate granular view of the input portfolio. Our numerical experiments illustrate the superiority of the performance of the proposed neural network approach compared to the traditional spatial interpolation schemes.},
archivePrefix = {arXiv},
arxivId = {1606.07831},
author = {Hejazi, Seyed Amir and Jackson, Kenneth R.},
doi = {10.1016/j.insmatheco.2016.06.013},
eprint = {1606.07831},
isbn = {0167-6687},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Monte Carlo simulation,Neural network,Portfolio valuation,Spatial interpolation,Variable annuity},
pages = {169--181},
title = {{A neural network approach to efficient valuation of large portfolios of variable annuities}},
volume = {70},
year = {2016}
}
@techreport{Jdanov2008,
author = {Jdanov, Dmitri A and Jasilionis, Domantas and Soroko, E L and Rau, Roland and Vaupel, James W},
publisher = {Max Planck Institute for Demographic Research, Rostock, Germany},
title = {{Beyond the Kannisto-Thatcher database on old age mortality: An assessment of data quality at advanced ages}},
year = {2008}
}
@inproceedings{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors. Copyright 2010 by the author(s)/owner(s).},
author = {Nair, Vinod and Hinton, Geoffrey E.},
booktitle = {ICML 2010 - Proceedings, 27th International Conference on Machine Learning},
isbn = {9781605589077},
pages = {807--814},
shorttitle = {Rectified linear units improve restricted boltzman},
title = {{Rectified linear units improve Restricted Boltzmann machines}},
year = {2010}
}
@incollection{Bengio2007,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), rea- soning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, withmin- imal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally lim- ited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very ineffi- cient in terms of required number of computational elements and examples. Sec- ond, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learn- ing) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more ab- stract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence. 1},
author = {Bengio, Y and LeCun, Y},
booktitle = {Large-Scale Kernel Machines},
doi = {10.7551/mitpress/7496.003.0016},
editor = {Bottou, L and Chapelle, O and DeCoste, D and Weston, J},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Bengio-Scaling learning algorithms towards AI.pdf:pdf},
publisher = {MIT Press.},
shorttitle = {Scaling learning algorithms towards AI},
title = {{Scaling Learning Algorithms toward AI}},
year = {2007}
}
@article{Li2013,
abstract = {We examine the application of a Poisson common factor model for the projection of mortality jointly for females and males. The model structure is an extension of the classical Lee-Carter method in which there is a common factor for the aggregate population, while a number of additional sex-specific factors can also be incorporated. The Poisson distribution is a natural choice for modelling the number of deaths, and its use provides a formal statistical framework for model selection, parameter estimation, and data analysis. Our results for Australian data show that this model leads to projected life expectancy values similar to those produced by the separate projection of mortality for females and males, but possesses the additional advantage of ensuring that the projected male-to-female ratio for death rates at each age converges to a constant. Moreover, the randomness of the corresponding residuals indicates that the model fit is satisfactory.},
annote = {Li, Jackie
eng
England
2012/07/14 06:00
Popul Stud (Camb). 2013;67(1):111-26. doi: 10.1080/00324728.2012.689316. Epub 2012 Jul 12.},
author = {Li, J},
doi = {10.1080/00324728.2012.689316},
isbn = {1477-4747 (Electronic)
0032-4728 (Linking)},
journal = {Popul Stud (Camb)},
keywords = {*Life Expectancy,*Mortality,*Poisson Distribution,Adolescent,Adult,Aged,Aged, 80 and over,Australia/epidemiology,Child,Child, Preschool,Female,Humans,Infant,Infant, Newborn,Male,Middle Aged,Models, Statistical,Young Adult},
number = {1},
pages = {111--126},
pmid = {22788919},
title = {{A Poisson common factor model for projecting mortality and life expectancy jointly for females and males}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22788919},
volume = {67},
year = {2013}
}
@misc{Kuo2018,
address = {GitHub},
author = {Kuo, K},
shorttitle = {DeepTriangle},
title = {{DeepTriangle}},
url = {https://github.com/kevinykuo/deeptriangle},
year = {2018}
}
@misc{Allaire2018,
author = {Allaire, J J and Chollet, F},
booktitle = {R Studio},
publisher = {RStudio, Google},
title = {{R Interface to 'Keras'}},
url = {https://keras.rstudio.com/},
year = {2017}
}
@article{Henckaerts2019,
abstract = {Pricing actuaries typically operate within the framework of generalized linear models (GLMs). With the upswing of data analytics, our study puts focus on machine learning methods to develop full tariff plans built from both the frequency and severity of claims. We adapt the loss functions used in the algorithms such that the specific characteristics of insurance data are carefully incorporated: highly unbalanced count data with excess zeros and varying exposure on the frequency side combined with scarce, but potentially long-tailed data on the severity side. A key requirement is the need for transparent and interpretable pricing models which are easily explainable to all stakeholders. We therefore focus on machine learning with decision trees: starting from simple regression trees, we work towards more advanced ensembles such as random forests and boosted trees. We show how to choose the optimal tuning parameters for these models in an elaborate cross-validation scheme, we present visualization tools to obtain insights from the resulting models and the economic value of these new modeling approaches is evaluated. Boosted trees outperform the classical GLMs, allowing the insurer to form profitable portfolios and to guard against potential adverse risk selection.},
archivePrefix = {arXiv},
arxivId = {1904.10890},
author = {Henckaerts, Roel and C{\^{o}}t{\'{e}}, Marie-Pier and Antonio, Katrien and Verbelen, Roel},
eprint = {1904.10890},
file = {:C$\backslash$:/R/refas/henckaerts2019boosting.pdf:pdf},
keywords = {cross-validation,deviance,frequency,gradient boosting,interpretable machine learning,machine,model lift,severity modeling},
number = {1972},
pages = {1--34},
title = {{Boosting insights in insurance tariff plans with tree-based machine learning methods}},
url = {http://arxiv.org/abs/1904.10890},
year = {2019}
}
@inproceedings{Barnett,
abstract = {In recent years a number of authors (Brosius, 1992; Mack, 1993, 1994; and Murphy, 1994) have shown that link ratio techniques for loss reserving can be regarded as weighted regressions of a certain kind. We extend these regression models to handle different exposure bases and modeling of trends in the incremental data, and develop a variety of diagnostic tools for testing the assumptions these techniques carry with them. The new 'extended link ratio family' (ELRF) of regression models is used to test the assumptions made by the standard link ratio techniques and compare their predictive power with modeling trends in the incremental data. Not only do the ELRF regression models indicate that for most, if not all, cumulative arrays, the assumptions made by the standard link ratio techniques are not satisfied by the data, but that modeling the trends in the (log) incremental data has more predictive power. The ELRF modeling structure creates a bridge to a statistical (probabilistic) modeling framework where the assumptions are more in keeping with what we see in actual data. There is a paradigm shift from the standard link ratio techniques to the statistical modeling framework; and the ELRF models can be regarded as the bridge from the 'old' paradigm to the 'new'. There are three critical stages involved in arriving at a reserve figure, namely, extraction of information from the data in terms of trends and stability thereof, and distributions about these trends; formulation of assumptions about the future leading to forecasting of distributions of paid losses; and correlation between lines and security level sought. Finally, other benefits of the new statistical paradigm are discussed, including segmentation, credibility and reserves or distributions for different layers.},
author = {Barnett, Glen and Zehnwirth, Ben},
booktitle = {Proceedings of the Casualty {\ldots}},
number = {167},
pages = {245--321},
title = {{Best estimates for reserves}},
volume = {LXXXVII},
year = {2000}
}
@misc{Schreiber2017a,
address = {DIA Munich 2017},
author = {Schreiber, D},
number = {17 June},
title = {{The Future of Insurance}},
url = {https://www.youtube.com/watch?time{\_}continue=1{\&}v=LDOhFHJqKqI},
volume = {2018},
year = {2017}
}
@article{Dacorogna2018,
abstract = {The development of risk model for managing portfolio of financial institutions and insurance companies require both from the regulatory and management points of view a strong validation of the quality of the results provided by internal risk models. In Solvency II for instance, regulators ask for independent validation reports from companies who apply for the approval of their internal models.  Unfortunately, the usual statistical techniques do not work for the validation of risk models as we lack enough data to significantly test the results of the models. We will certainly never have enough data to statistically estimate the significance of the VaR at a probability of 1 over 200 years, which is the risk measure required by Solvency II. Instead, we need to develop various strategies to test the reasonableness of the model.  In this paper, we review various ways, management and regulators can gain confidence in the quality of models. It all starts by ensuring a good calibration of the risk models and the dependencies between the various risk drivers. Then applying stress tests to the model and various empirical analysis, in particular the probability integral transform, we build a full and credible framework to validate risk models.},
author = {Dacorogna, Michel M.},
doi = {10.2139/ssrn.2983837},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dacorogna - 2018 - Approaches and Techniques to Validate Internal Model Results.pdf:pdf},
journal = {SSRN Electronic Journal},
keywords = {risk models,solvency,statistical tests,stress tests,validation},
pages = {1--19},
title = {{Approaches and Techniques to Validate Internal Model Results}},
year = {2018}
}
@phdthesis{Loon2017,
author = {van Loon, Paul Rene Frank},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Loon - 2017 - EMPIRICAL STUDIES IN CORPORATE CREDIT MODELLING LIQUIDITY PREMIA, FACTOR PORTFOLIOS {\&} MODEL UNCERTAINTY.pdf:pdf},
number = {March},
school = {Heriot-Watt University},
title = {{EMPIRICAL STUDIES IN CORPORATE CREDIT MODELLING; LIQUIDITY PREMIA, FACTOR PORTFOLIOS {\&} MODEL UNCERTAINTY}},
year = {2017}
}
@article{PeterTerBerge2008,
author = {{Peter Ter Berge}},
file = {:C$\backslash$:/Users/user-pc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peter Ter Berge - 2008 - Counterparty Portfolio modelling of default risk.pdf:pdf},
journal = {Life {\&} Pensions},
number = {April 2008},
pages = {29--33},
title = {{Counterparty Portfolio modelling of default risk}},
year = {2008}
}
@article{Ferguson1989,
abstract = {1.1 1992 has become a symbol. A symbol of change in a Europe which feels itself threatened by the economic might of America in the West and Japan in the East. 1992 really does mean that millions of people, despite wide gulfs of language and culture, and a history of often bloody conflict between them, are making one more effort of collective will to club together in the belief that the one thing they have in common, geography, is sufficient reason to disguise differences and create a powerful economic union for the greater benefit of European suppliers and consumers.},
author = {Ferguson, D. G. R. and Croucher, P. E. and Franklin, N. A. M. and Henty, J. M. and Parmee, D. S. and Saunders, A. and Shaw, G. J. M.},
doi = {10.1017/s0020268100036660},
issn = {0020-2681},
journal = {Journal of the Institute of Actuaries},
month = {dec},
number = {3},
pages = {453--507},
publisher = {Cambridge University Press (CUP)},
title = {{A single European market for actuaries}},
volume = {116},
year = {1989}
}
@article{DalMoro2013,
abstract = {{\textless}b{\textgreater}French Abstract:{\textless}/b{\textgreater} Que ce soit en Suisse ou dans l'Union Europ{\'{e}}enne, les cadres r{\'{e}}glementaires des march{\'{e}}s de l'assurance {\'{e}}voluent et imposent aux diff{\'{e}}rents acteurs de mettre en place une fonction actuarielle (article 48 de la Directive Solvabilit{\'{e}} 2) ou de d{\'{e}}signer un actuaire responsable (article 24 de la Loi sur la Surveillance des Assurances). Si les textes de la Directive europ{\'{e}}enne n'ont pas encore {\'{e}}t{\'{e}} transpos{\'{e}}s et comportent ainsi une large zone d'ombre concernant la responsabilit{\'{e}} r{\'{e}}elle des actuaires, il n'en va pas de m{\^{e}}me pour les textes suisses, le r{\'{e}}gime {\'{e}}tant d{\'{e}}j{\`{a}} en place depuis quelques ann{\'{e}}es d{\'{e}}j{\`{a}}. De son c{\^{o}}t{\'{e}}, l'Association Actuarielle Internationale (AAI) travaille actuellement sur l'{\'{e}}laboration de normes internationales encadrant les travaux des actuaires, tout en s'assurant de la compatibilit{\'{e}} de ces derni{\`{e}}res avec les normes Solvabilit{\'{e}} 2 et Suisses. Dans tous les cas, tout comme le commissaire aux comptes, l'actuaire devient l'une des pi{\`{e}}ces maitresses du dispositif visant {\`{a}} assurer la solvabilit{\'{e}} des compagnies d'assurance ainsi qu'{\`{a}} am{\'{e}}liorer la ma{\^{i}}trise des risques. Dans ce contexte, le cadre l{\'{e}}gal actuel est-il r{\'{e}}ellement adapt{\'{e}}?  {\textless}b{\textgreater}English Abstract:{\textless}/b{\textgreater} Both in Switzerland and in the European Union, the insurance market regulatory frameworks are evolving and impose to (re)insurance companies to put in place an actuarial function holder (article 48 of the Solvency 2 Directive) or to name a responsible actuary (article 24 of the Swiss Insurance Supervision Law). Contrary to the European Directive which has not yet been approved by the parlaments of each EU member, hence leaving room for interpretation as to the real responsbility of the actuarial function holder, the Swiss regulation related to the responsible actuary has been in place for some time already. At its level, the International Actuarial Association (IAA) works currently on the design of international norms regulating the works of actuaries making sure that there norms will comply with both Solvency 2 and Swiss requirements. In the end, like the external auditors, the actuary is becoming one of the masterpieces to ensure the solvency of insurance companies and the continuous improvement of risk management. In this context, this article reviews if the current legal framework is well adapted for the role of actuaries.},
author = {{Dal Moro}, Eric and Mieh{\'{e}}, Pierre},
doi = {10.2139/ssrn.2292028},
journal = {SSRN Electronic Journal},
month = {aug},
publisher = {Elsevier BV},
title = {{Fonction Actuarielle Solvabilit{\'{e}} Ii/Fonction D'Actuaire Responsable Swiss Solvency Test: Le Cadre L{\'{e}}gal Est-Il Adapt{\'{e}} {\`{A}} Un Exercice Ind{\'{e}}pendant Et Efficient? Quelques Exemples Concrets (Actuarial Function Holder Solvency 2/Responsible Actuary Swiss Solven}},
year = {2013}
}
@article{Gutterman2002,
abstract = {The demands that financial reporting of insurance companies present to actuaries are great and growing. With the prospects of change in the rules for financial reporting becoming more likely and insurance products becoming more complex, it is desirable to examine the evolving roles of the actuary and the actuarial profession. This paper describes these changes and the value that actuaries bring to financial reporting. The challenges presented are significant. As the methods of assessing and managing risk change are becoming more complex, the best efforts of the profession and individual actuaries will be needed to ensure that the actuary's role is enhanced and expanded. Not only will the techniques used evolve, but the audiences served by the actuary will become even more demanding. The actuarial profession is better situated than other professions to meet these demands. {\textcopyright} 2002 Taylor {\&} Francis Group, LLC.},
author = {Gutterman, Sam},
doi = {10.1080/10920277.2002.10596043},
issn = {10920277},
journal = {North American Actuarial Journal},
month = {apr},
number = {2},
pages = {47--59},
title = {{The Evolving Role of the Actuary in Financial Reporting of Insurance}},
volume = {6},
year = {2002}
}
@article{Dumbreck2007a,
abstract = {This Presidential Address considers what needs to be done to enable the United Kingdom actuarial profession to flourish in a more competitive environment. It addresses the need for continued modernisation of the actuarial skill-set, and the need to pay greater attention to the way we communicate our findings. The Address briefly reviews some of the recent changes to the regulation of life assurance in the U.K. before considering the role and structure of the professional body in the post-Morris era. It ends with some thoughts on developments in education and CPD.},
author = {Dumbreck, Nicholas John},
doi = {10.1017/s1357321700001409},
issn = {1357-3217},
journal = {British Actuarial Journal},
month = {mar},
number = {1},
pages = {1--21},
publisher = {Cambridge University Press (CUP)},
title = {{Address by the President of the Institute of Actuaries. Raising Our Game}},
volume = {13},
year = {2007}
}
@article{Kunreuther1989,
abstract = {Although there is a large body of literature on the unwillingness of the insurance industry to offer protection against risks such as environmental pollution, there has been only limited analysis regarding two key actors in the decision process—actuaries and underwriters. However, if actuaries and underwriters feel that a particular type of risk is uninsurable, then it is unlikely to be offered. This paper investigates how ambiguities associated with a risk affect actuary and underwriter decision processes, the paper contends that the decision of the insurance industry to not provide protection against environmental pollution liability is due to ambiguity of the risk. Empirical evidence from controlled laboratory experiments with actuaries and a model of choice in underwriter decision processes support this point. Copyright {\textcopyright} 1989, Wiley Blackwell. All rights reserved},
author = {Kunreuther, Howard},
doi = {10.1111/j.1539-6924.1989.tb00997.x},
issn = {15396924},
journal = {Risk Analysis},
keywords = {Insurance,ambiguity,decision processes,environmental pollution},
number = {3},
pages = {319--328},
title = {{The Role of Actuaries and Underwriters in Insuring Ambiguous Risks}},
volume = {9},
year = {1989}
}
@article{Zeff1998,
abstract = {Recent tensions between private-sector interests and the SEC are reviewed. Much of the pressure on auditor independence may be due to weaknesses in the structure of corporate governance. Two reforms that might contribute to shoring up the independence of auditors are proposed. {\textcopyright} 1998 Academic Press.},
author = {Zeff, Stephen A.},
doi = {10.1006/cpac.1998.0259},
issn = {10452354},
journal = {Critical Perspectives on Accounting},
number = {5},
pages = {535--543},
publisher = {Academic Press},
title = {{Independence and standard setting}},
volume = {9},
year = {1998}
}
@incollection{Tess2004,
address = {Chichester, UK},
author = {Tess, Daniel},
booktitle = {Encyclopedia of Actuarial Science},
doi = {10.1002/9780470012505.tai026},
month = {sep},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Insurance Regulation and Supervision}},
url = {http://doi.wiley.com/10.1002/9780470012505.tai026},
year = {2004}
}
@article{Dewing2010,
abstract = {One feature of the UK Financial Services Authority's (FSA) approach to regulation is the involvement of private actors in the regulatory process. This paper explores the introduction of 'Principles and Practices of Financial Management' (PPFM) for the management of with-profits funds of UK life insurers, and reports the experience of actuaries as private actors having a key role in implementing the FSA's PPFM framework. Actuaries reported experiencing considerable challenges in implementing the PPFM framework and, while accountability had increased, actuaries expressed scepticism about the ability of consumers and policyholders to make use of the increased amount of publicly available information. {\textcopyright} 2010 Taylor {\&} Francis.},
author = {Dewing, Ian and Russell, Peter},
doi = {10.1080/02642060903191116},
issn = {02642069},
journal = {Service Industries Journal},
keywords = {Actuaries,Financial services authority,Life insurance,Private actors,Regulation},
month = {oct},
number = {12},
pages = {1955--1966},
title = {{Implementing new financial regulation: Actuaries and UK with-profits funds}},
volume = {30},
year = {2010}
}
@techreport{Mba2012,
author = {Mba, Michael Siyanbola and Fiin, Fimc},
file = {::},
title = {{Corporate Governance in the Insurance Industry}},
year = {2012}
}
@misc{JansenvanVuuren2017,
author = {{Jansen van Vuuren}, Liesel and Reyers, Michelle and {Van Schalkwyk}, Henco},
booktitle = {Southern African Business Review (1997)},
issn = {1561-896X},
keywords = {actuarial science,business,enterprise risk management,financial risk management,it risk management,operational risk,risk analysis,risk management,risk management plan,risk pool},
number = {1},
pages = {129--149},
title = {{Assessing the impact of Solvency Assessment and Management on risk management in South African insurance companies}},
url = {https://www.ajol.info/index.php/sabr/article/view/155434},
urldate = {2020-01-30},
volume = {21},
year = {2017}
}
@article{Daykin1992,
abstract = {Following a brief outline of the origins of the actuarial profession in the United Kingdom, the paper traces the involvement of actuaries in the supervision of insurance in the U.K., and recalls the origins and early development of actuaries in government. In 1919, the Government Actuary's Department (GAD) was formed, and the paper explores the developing role of GAD in the supervision of insurance and how insurance supervision has evolved into a close partnership between the actuarial profession and the supervisors.},
author = {Daykin, C. D.},
doi = {10.1017/s0020268100019892},
issn = {0020-2681},
journal = {Journal of the Institute of Actuaries},
number = {2},
pages = {313--343},
publisher = {Cambridge University Press (CUP)},
title = {{The developing role of the Government Actuary's Department in the supervision of insurance}},
volume = {119},
year = {1992}
}
@article{Deighton2009,
abstract = {For some while there has been a growing awareness from both internal and external stakeholders that the governance and risk management in United Kingdom (U.K.) insurance companies needed to be enhanced. The proposed European Union Solvency II Directive makes this very explicit and the current economic turmoil has put a much stronger emphasis on the whole process: it is being seen as the right thing to do, rather than simply a regulatory requirement. In this paper, we set out the background to and recent history of governance for U.K. insurance companies, and consider how enterprise risk management can bring together the various control frameworks needed to support that governance. Whilst no two companies are the same, and hence the solutions to these issues will vary, there are several common themes linked to successful implementation. Similarly, various barriers to success are identified, together with solutions to resolve them.},
author = {Deighton, S. P. and Dix, R. C. and Graham, J. R. and Skinner, J. M. E.},
doi = {10.1017/s1357321700005729},
issn = {1357-3217},
journal = {British Actuarial Journal},
month = {sep},
number = {3},
pages = {503--556},
publisher = {Cambridge University Press (CUP)},
title = {{Governance and Risk Management in United Kingdom Insurance Companies}},
volume = {15},
year = {2009}
}
@article{Thornton1979,
abstract = {The guide to the profession on this subject first appeared in the Faculty Year Book for 1975-76 and it was discussed at our sessional meeting in February 1976 (and at the Institute's sessional meeting in January 1976). In the time since these meetings there have been changes in the guide and additions to it. No doubt future changes in business conditions in the years ahead will bring forth further amendments. The reasons for devoting another sessional meeting to it at this time are, firstly, to review the manner of its production and to consider what weight it carries, and, secondly, to give members the opportunity of considering the detailed wording of the guide, as distinct from its general intention. The analogy that comes to mind is with an Act of Parliament. The Bill is prepared in a department of government, is drawn up with care and professional skill by parliamentary draftsmen, and receives two readings before the House of Commons when its general aims and intentions are approved: it then moves into the committee stage where, if time permits, every clause is gone over individually to see that it expresses clearly and unambiguously the intention of parliament. It then has its third reading and, if approved, may be expected to go on to become law. The sessional meetings here and in London in 1976 correspond in this analogy to the first two readings of the Bill, and they showed that the intention of the guide and its general terms had the approval of the profession.},
author = {Thornton, M. D.},
doi = {10.1017/s0071368600008533},
issn = {0071-3686},
journal = {Transactions of the Faculty of Actuaries},
pages = {24--62},
title = {{Actuaries and Long-Term Insurance Business}},
volume = {37},
year = {1979}
}
@article{Barrow1976a,
author = {Barrow, G. E.},
doi = {10.1017/s0020268100017881},
issn = {0020-2681},
journal = {Journal of the Institute of Actuaries},
month = {sep},
number = {2},
pages = {137--166},
publisher = {Cambridge University Press (CUP)},
title = {{Actuaries and long-term insurance business: introductory notes}},
volume = {103},
year = {1976}
}
@article{Dewing2006b,
abstract = {The paper investigates the Financial Services Authority's (FSA) reforms of the unique corporate governance arrangements of UK life insurers. In particular, the paper explores the need for special arrangements for the governance of with-profits funds, the role of actuaries in corporate governance and the proposals for reform which resulted from the FSA's With Profits Review and Lord Penrose's Report into the problems experienced at the Equitable Life Assurance Company. The paper concludes that post-Cadbury trends in corporate governance, in combination with the FSA's reforms, have placed boards at the centre stage of actuarial governance with directors no longer able to abrogate responsibility for actuarial decisions. There is also much to be learnt from studying changes to the actuarial and corporate governance of life insurers as it reveals the complex nature of corporate reality, and demonstrates the inadequacy of theoretical approaches that dichotomise corporate governance between Anglo-Saxon shareholder and continental European stakeholder models. {\textcopyright} 2006 Blackwell Publishing Ltd.},
author = {Dewing, Ian P. and Russell, Peter O.},
doi = {10.1111/j.1467-8683.2006.00497.x},
issn = {14678683},
journal = {Corporate Governance: An International Review},
keywords = {Actuaries,Auditors,Financial Services Authority,Financial services,Insurers,Regulation},
month = {may},
number = {3},
pages = {172--180},
title = {{Corporate governance - Regulation and reform: The actuarial governance of UK life insurers}},
url = {http://doi.wiley.com/10.1111/j.1467-8683.2006.00497.x},
volume = {14},
year = {2006}
}
@article{Foroughi2004,
abstract = {This paper explores the risks faced by South African life insurance companies arising from the provision of investment guarantees in products sold. The current thinking and practice of the larger South African life insurance companies regarding investment guarantees is set out following their responses to a survey. The paper examines the forms of investment guarantee available and the business issues created by the writing of these guarantees. These include issues around the design and pricing of new business, as well as the risk management of in force business. The paper also compares existing methods used internationally to value life insurance business with investment guarantees, focusing on the use of stochastic models. The different allowances for risk within each valuation method and the appropriateness of these allowances when valuing investment guarantees are considered. The stochastic models compared include both statistically based real world models and market consistent state price deflator or risk neutral models. Practical issues around the building of such asset liability stochastic models are briefly discussed. Finally, the authors put forward their own views of possible developments in the future within South Africa that may impact on life insurance business with investment guarantees, and the possible implications.},
author = {Foroughi, K and Jones, IA and Dardis, A},
doi = {10.4314/saaj.v3i1.24495},
issn = {1680-2179},
journal = {South African Actuarial Journal},
keywords = {Asset,Equity volatility,Financial options,Hedging,Investment guarantee,Market,Market risk premium,Maturity guarantee,Non,Smoothed,South Africa,Stochastic modelling,bonus business,consistent valuation,liability matching,profit guarantee},
number = {1},
publisher = {Sabinet},
title = {{Investment Guarantees in the South African Life Insurance Industry}},
volume = {3},
year = {2004}
}
@article{Johnston1989a,
abstract = {1.1 A paper about the Appointed Actuary is essentially a paper about prudential supervision of life insurance companies. The system which has operated in the UK since the mid-1970's is only partly one of Government supervision. Through the professional role of the Appointed Actuary, it also contains elements of a system of self-regulation with the Institute and Faculty of Actuaries standing in place of SRO's. Unlike the self-regulatory arrangements of the Financial Services Act, though, this second part of the system has grown up by custom and practice and in certain respects it is not codified. However it enables the Insurance Companies Act to be operated successfully.},
author = {Johnston, E. A.},
doi = {10.1017/s0020268100036453},
file = {:C$\backslash$:/Users/user-pc/OneDrive - QED Actuaries {\&} Consultants/41213568.pdf:pdf},
issn = {0020-2681},
journal = {Journal of the Institute of Actuaries},
number = {1},
pages = {27--100},
publisher = {Cambridge University PressInstitute and Faculty of Actuaries},
title = {{The appointed actuary}},
url = {https://www.jstor.org/stable/41213568},
volume = {116},
year = {1989}
}
@article{Abbott1986,
abstract = {1.1 At the 1984 seminar of the Institute's General Insurance Study Group the author presented a draft of some guidance notes which were intended to be the starting point for establishing a framework of recognized good practice for United Kingdom actuaries working in general insurance. The draft. amended to take into account comments made at and subsequent to that seminar. appears as Appendix A, with the title “Notes on recommended practice (abbreviated to NORP) for actuarial reporting in general insurance”. It was felt that in presenting the notes to the profession at large. an introductory paper would be helpful, not only in explaining some of the issues covered in the proposed NORP but in discussing more generally the role of the actuary in general insurance and what an actuary should be prepared to advise on.},
author = {Abbott, W. M.},
doi = {10.1017/s0020268100042530},
issn = {0020-2681},
journal = {Journal of the Institute of Actuaries},
month = {dec},
number = {3},
pages = {299--339},
publisher = {Cambridge University Press (CUP)},
title = {{Actuaries and general insurance}},
volume = {113},
year = {1986}
}
@techreport{Colenutt1979a,
abstract = {This paper outlines the main principles of supervision of general insurance companies in the United Kingdom, and of Lloyd's of London, with particular emphasis on prudential regulation and the role of actuaries. Some international trends in insurance supervision are identified and indications given of how supervision may develop in the future, in the light of the role of the International Accounting Standards Committee in setting accounting standards and the role of the International Association of Insurance Supervisors in developing globally accepted standards for insurance regulation.},
author = {Colenutt, Dennis},
booktitle = {The Journal of Risk and Insurance},
doi = {10.2307/252070},
file = {::},
issn = {00224367},
keywords = {Lloyd's of London,Regulation,actuary,dynamic financial analysis,solvency},
number = {2},
pages = {77},
title = {{The Regulation of Insurance Intermediaries in the United Kingdom}},
volume = {46},
year = {1979}
}
@article{OBrien2016a,
abstract = {We examine the roles of actuaries in UK life offices, along with trends, challenges to and opportunities for actuaries. We carry out an analysis of senior roles in life offices, a questionnaire survey and interviews with relevant senior personnel. We find that actuaries occupy many important roles in life offices and are regarded as having good industry knowledge and technical skills, especially in financial modelling. There are fewer executive directors and more non-executive directors of life offices who are actuaries compared with the position in 1990. A higher proportion of reserved roles is outsourced to consultants than was the case in 1990. Only a small number of Actuarial Function Holders are directors. Actuaries are more siloed than was the case in the past, although actuaries are well represented in the finance and risk functions of many offices. Although actuarial work in connection with the preparation for Solvency II will decline, there will be important ongoing requirements for actuaries following Solvency II implementation. We also see opportunities for actuaries in four areas: in risk management, in financial analysis and management based on Solvency II and international financial reporting standards, in connection with “big data”, and in product development and the customer proposition. There are implications for the examination syllabus, continuing professional development and research.},
author = {O'Brien, C. D. and Gallagher, G. A. and Green, R. J. and Hughes, D. W. and Liang, F. and Robinson, S. A. and Simmons, P. and Tay, A. J. W. H.},
doi = {10.1017/s1357321715000070},
issn = {1357-3217},
journal = {British Actuarial Journal},
month = {mar},
number = {01},
pages = {134--164},
publisher = {Cambridge University Press (CUP)},
title = {{The roles of actuaries in UK life offices: changes and challenges}},
volume = {21},
year = {2016}
}
@article{Gunz2009,
abstract = {The actuarial profession has a long history of providing critical expertise to society. The services delivered are some of the most complex and mysterious to outsiders of all professions but little has been written about the professional responsibilities of actuaries in the academic literature beyond that of the profession itself. This paper makes the case that the issues surrounding professional independence of actuaries are, in principle, similar to those that faced the audit profession before the scandals and resultant regulatory changes early this century. It is argued that, despite the position taken by the actuarial profession and management, the status quo raises genuine concerns about conflicts of interest and independence and that the risks that arise are of sufficient magnitude that they should at least be the subject of a full debate. {\textcopyright} 2008 Springer Science+Business Media B.V.},
author = {Gunz, Sally and McCutcheon, John and Reynolds, Frank},
doi = {10.1007/s10551-008-9985-8},
issn = {01674544},
journal = {Journal of Business Ethics},
keywords = {Actuaries,Auditors,Conflict of interest},
month = {sep},
number = {1},
pages = {77--89},
title = {{Independence, conflict of interest and the actuarial profession}},
volume = {89},
year = {2009}
}
@article{Gunz2011,
author = {Gunz, Sally and Jennings, Marianne M.},
doi = {10.1111/j.1744-1714.2011.01122.x},
issn = {17441714},
journal = {American Business Law Journal},
number = {4},
pages = {641--711},
title = {{A Proactive Proposal for Self-Regulation of the Actuarial Profession: A Means of Avoiding the Audit Profession's Post-Enron Regulatory Fate}},
url = {https://heinonline.org/HOL/Page?handle=hein.journals/ambuslj48{\&}id=659{\&}div={\&}collection=},
volume = {48},
year = {2011}
}
@article{Daykin1999,
abstract = {The actuary has played a role in regulation in the United Kingdom since 1819. More recently, in 1974, the Appointed Actuary system was introduced for life insurance companies, backed by strong professional guidance. Derivatives of the Appointed Actuary concept have been implemented in a number of other countries. Meanwhile, in the UK, defined benefit occupational pension schemes are now required to appoint a Scheme Actuary, who has a statutory whistle-blowing role under the Pensions Act 1995. A number of statutory roles for pension actuaries were in place prior to this. In general insurance, Lloyd's syndicates are now required to obtain an actuarial opinion on the end of year provisions, as part of the Lloyd's market regulatory structure, and friendly societies must obtain an actuarial opinion on their technical provisions once every three years. This paper reviews some of the different regulatory roles of the actuary in the UK, draws some comparisons with the situation in other countries, considers the strengths and weaknesses of the present situation and invites debate and discussion on the way forward in order to optimise the contribution which the profession can make to the public interest in the field of regulation.},
author = {Daykin, C.D.},
doi = {10.1017/s1357321700000568},
issn = {1357-3217},
journal = {British Actuarial Journal},
month = {aug},
number = {3},
pages = {529--574},
publisher = {Cambridge University Press (CUP)},
title = {{The Regulatory Role of the Actuary}},
volume = {5},
year = {1999}
}
@article{Williams2016,
abstract = {The Solvency II Directive introduces the idea of a formal Actuarial Function to have responsibility over delivering the requirements of Article 48 of the Directive. Article 48 describes the responsibilities as being concerned with technical provisions, an opinion on reinsurance adequacy, an opinion on underwriting policy and contributing to the risk management system. Considerable documentation has been produced by the Prudential Regulation Authority (PRA), the Institute and Faculty of Actuaries (IFoA) and the European Insurance and Occupational Pensions Authority on the subject, much of it very recent to the publication of this paper. The purpose of this paper is to provide the reader with some practical insights and suggestions around addressing the requirements of Article 48 of the Solvency II Directive in general insurance firms, taking into consideration the publications of the aforementioned regulatory authorities. It is not our intention to give advice, nor to be seen to give advice, but rather to make suggestions and observations that we hope the reader will find useful. The Regulations lay down the tasks of the Actuarial Function, so insurers should consider the need for formal terms of reference, backed up by proportionate governance procedures. The Regulations also require the production of an Actuarial Function Report to document the tasks undertaken by the Actuarial Function and its results. Such a report can be an aggregate report, made up of individual component reports completed at suitable points in the Actuarial Function's work cycle, so long as it reports on all the required tasks. The technical provisions section should cover at least all the areas laid down in the Delegated Acts. The opinions required covering reinsurance adequacy and underwriting policy are not formal “sign offs”, but contributions to the effective running of the insurer by applying the skills and knowledge of actuaries to areas for which they are not normally responsible. Again, the Delegated Acts mandate the minimum contribution the Actuarial Function should make. The responsibility for delivering the work of the Actuarial Function does not have to be given to a member of the IFoA; however, the PRA is going to require (at least) one person to be designated the “Chief Actuary”, defined as the person responsible for delivering the requirements of Article 48 of the Directive. In response, the IFoA has stated its intention to require its members holding the role of Chief Actuary, as defined by the PRA, to hold a practicing certificate. Any Actuarial Function will need to consider issues of governance, independence and conflicts of interests. The PRA intends to require the Actuarial Function to be independent of an insurer's revenue-generating functions. In addition, normal good governance requires a degree of separation between those who perform Actuarial Function work and those who review and supervise it. There are numerous stakeholders in the Actuarial Function's work. Some of these will rely on the output of the Actuarial Function, others will provide inputs to its work. Setting out stakeholder responsibilities clearly and in advance will be of vital importance. Good communication and coordination between these groups will be important to the efficient running of the insurer. Bringing together issues of governance, independence and meeting the Directive and regulators' requirements will require a suitable organisational structure which will also need to consider practical issues, such as the availability of suitable staff. Many such arrangements may be possible, but all will require trading off advantages and disadvantages. The Actuarial Function is primarily about good practice and getting the most out of the actuarial skills available. For many insurers, meeting the requirements should not be unduly burdensome.},
author = {Williams, Richard L. and Anzsar, J. and Bulmer, R. and Buntine, J. and Byrne, M. and Gedalla, B. and Goswamy, P. and Grant, J. and Heah, W. and Keshani, S. and Shah, J.},
doi = {10.1017/s1357321716000052},
issn = {1357-3217},
journal = {British Actuarial Journal},
month = {sep},
number = {3},
pages = {476--530},
publisher = {Cambridge University Press (CUP)},
title = {{Application of the Solvency II Actuarial Function to general insurance firms}},
volume = {21},
year = {2016}
}
@article{Livsey1964,
abstract = { This paper is intended to give a fairly complete picture of the actuary in Canada when read in conjunction with T. R. Suttie's paper, ‘The Actuary in Canada' ( J.S.S. 13 , 199). Suttie concentrated on the life insurance business in his paper, and with this in mind, I shall refer mainly to consulting actuarial work. Consulting practice in Canada is very similar to that in Great Britain, where the major part of the actuary's time is devoted to pension funds. The main differences are that Canadian consulting firms may advertise, and may also include insurance brokerage as part of their business provided that they hold the necessary licences. The small amount of friendly society work in Canada is handled by actuaries who specialize in such work, and there is no market in reversions or life interests.},
author = {Livsey, F.},
doi = {10.1017/s0020269x00007799},
issn = {0020-269X},
journal = {Journal of the Staple Inn Actuarial Society},
month = {jul},
number = {04},
pages = {388--400},
publisher = {Cambridge University Press (CUP)},
title = {{The Consulting Actuary in Canada.}},
volume = {17},
year = {1964}
}
@article{Collins2009,
abstract = {This article investigates reform of the actuarial profession following the establishment of the UK Financial Services Authority and as a result of the problems emerging at the Equitable Life Assurance Society. Perceptions on changes to the role of life actuaries are explored using interviews with senior actuaries and accountants. The study complements the few existing academic analyses of actuaries and yet challenges these analyses inasmuch as it locates actuarial work within a broader sociological frame. Thus, the article views the actuarial profession not as a simple collection of traits but as a dynamic socio-historical project that reflects and projects professional knowledge claims.The article concludes that the imposed reforms have rescued the actuarial profession from its failure to reform itself, at least in the short term. The main price to be paid is that regulation of the actuarial profession is firmly locked into the regulatory structures of the accountancy profession. {\textcopyright} 2009 BSA Publications Ltd{\textregistered}.},
author = {Collins, David and Dewing, Ian and Russell, Peter},
doi = {10.1177/0950017009102857},
issn = {09500170},
journal = {Work, Employment and Society},
keywords = {Accountancy profession,Actuarial profession,Economic change,Insurance,Regulation},
month = {jun},
number = {2},
pages = {249--266},
title = {{The actuary as fallen hero: On the reform of a profession}},
url = {http://journals.sagepub.com/doi/10.1177/0950017009102857},
volume = {23},
year = {2009}
}
@article{Kelly2012,
abstract = {Property/casualty (P/C) insurers are required to establish loss reserves for unpaid losses at the time that the loss has occurred or is reasonably expected to have occurred. We examine factors that may impact the accurate setting of loss reserves. These include the level of rate regulation faced by the insurer and the incentives to underestimate or overestimate reserves to improve financial ratios or improve solvency scores, to reduce earnings, to defer taxes, or to smooth earnings volatility in order to meet shareholder expectations. The employment status of the Appointed Actuary, that is, whether the Appointed Actuary is an employee of the firm or a consultant, may also impact reserve accuracy. Using a variety of regression models with data from 1995 to 2010, we examine the impact of these factors on the accuracy of reserves posted by Canadian P/C insurers. Our results provide no evidence of systematic differences in the magnitude or direction of loss reserve errors between insurers that use company actuaries versus those that use consultant actuaries. However, we find that for both consultant and company actuaries positive reserve errors are associated with increases in global stock market returns and decreases in unanticipated inflation. The insurance market cycle impacts reserve errors for company actuaries and not consultant actuaries. As well, our results indicate that as the proportion of short-tailed business increases in a company, consultant actuaries are more likely to over-reserve. Similar to many previous studies using U.S. data, we do not find strong evidence regarding insurers' incentives to deliberately overstate or understate reserves: Loss reserves are relatively unbiased estimates of the true losses paid. Thus these findings should be welcome news to the actuarial profession in Canada and to the prudential regulator: The Appointed Actuary, regardless of employment status, provides objective and unbiased estimates of insurers' largest liability. {\textcopyright} 2012 Taylor {\&} Francis Group, LLC.},
author = {Kelly, Mary and Kleffner, Anne and Li, Si},
doi = {10.1080/10920277.2012.10590643},
issn = {10920277},
journal = {North American Actuarial Journal},
month = {jul},
number = {3},
pages = {285--305},
title = {{Loss Reserves and the Employment Status of the Appointed Actuary}},
volume = {16},
year = {2012}
}
