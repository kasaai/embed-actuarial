---
title: Categorical Predictor Embeddings in Actuarial Modeling
authors:
  - name: Kevin Kuo 
    affiliation: RStudio and Kasa AI
    email: kevin@kasa.ai
  - name: Ronald Richman
    affiliation: QED Actuaries and Consultants
    email: ronaldrichman@gmail.com
abstract: |
  Enter the text of your abstract here.
keywords:
  - blah
  - blee
  - bloo
  - these are optional and can be removed
bibliography: references.bib
output: rticles::arxiv_article
---

# Introduction

Categorical data are modelled by actuaries in many different contexts, from pricing of insurance products to reserving for the liabilities generated by these products. Sometimes, these data are modelled in an explicit manner, for example, when building models that apply across multiple categories, a form of dummy coding is usually used. For example, when building models to price the frequency of motor insurance claims, claim experience relating to different types of motor vehicle will often be modelled by including a (single) factor within models that modifies the relative frequency predicted for each type of vehicle. In other cases, modelling is performed for each category separately, thus the categorical data is used within the modelling in an implicit manner, for example, common practice is to estimate reserves for different lines of business separately, meaning to say, with no parameters being shared across each of the reserving models.

Recently, several studies of insurance problems have applied an alternative approach, originally from the natural language processing literature [@Bengio2003] and now applied to diverse types of machine learning problem [@Guo2016], known as **categorical embeddings**. Instead of trying to capture the differences between categories using a single factor, the categorical embedding approach rather maps each category to a low-dimensional numeric vector, which is then used within the model as a new predictor variable.

This approach to modelling categorical data has several advantages over more traditional treatments of categorical data. Using categorical embeddings instead of traditional techniques has been shown to increase predictive accuracy of models, for example, see [@Richman2018] in the context of pricing. Models incorporating categorical embeddings can be pre-calibrated to traditional actuarial models, increasing the speed with which these models can be calibrated and leading to models with better explainability [@wuthrich2019yes]. Finally, the similarity between the vectors learned for different categories can be inspected, sometimes leading to insights into the workings of models, see, for example, [@Kuo2019].

On the other hand, several open questions about the use of embeddings within actuarial work remain, which we aim to address in this study. First, hyperparameter settings for embeddings, such as the dimensions of the embedding layer and the use of regularization techniques such as dropout or normalization, that achieve optimal predictive performance has not yet been studied in detail in the actuarial literature. In this work, we aim to study how embeddings using different settings perform in the context of a large-scale predictive modelling problem, and give guidance on the process that can be followed to determine this in other problems. Although neural network have been shown to achieve excellent predictive accuracy on actuarial tasks, many actuaries still prefer to use GLM models for pricing tasks, thus, the issue of whether transferring embeddings to GLM models can achieve better performance is considered in this paper. Traditional actuarial techniques such as credibility theory have been used to work with some types of categorical data, but no study has been performed whether this can be applied within embeddings; here, we investigate whether performance is enhanced by applying credibility theory on embeddings relating to categorical variables with many labels. Whereas embedding layers are usually considered in the context of categorical data,the option exists to quantize numerical data and model it using embeddings, however, the results of doing so have not been investigated. Finally, in the past several years, a new type of neural network architecture based on attention [@Vaswani2017] has been successfully used on embeddings in the field of natural language processing and we incorporate attention based models into our predictive modelling example.

In this work, we utilize the recently released National Flood Insurance Program (NFIP) dataset [@FederalEmergencyManagementAgency2019] which provides exposure information for policies written under the NFIP since XXXXDateXXXX, as well as the claims data relating to these exposures. We refer the reader to Appendix \ref{NFIP_eda} for an exploratory analysis of this dataset.

The rest of this manuscript is organized as follows. Section \ref{lit_review} reviews recent applications of embeddings in the actuarial literature. Section \ref{defns} provides the notation used in the paper and defines GLMs, neural networks and related modelling concepts. In Section \ref{modelling}, we provide initial models for the NFIP dataset and consider the influence of hyperparameter choices on the results of the neural network model. Section \ref{tl} considers how successfully the embeddings used in the neural network model can be transferred to a GLM model of the same data. Extensions to the modelling approach are considered in Section \ref{extend} and we focus on attention based models in Section \ref{attention}. The interpretability of embedding layers is addressed in Section \ref{interpretation}. Finally, Section \ref{Conclusions} provides a discussion of the results of this paper and considers avenues for future research.

(TODO: link to github repo)

# Literature Review {#lit_review}

Categorical data are usually modelled within GLMs and other predictive models using indicator variables which capture the effect of each level of the category, see, for example, Section 2 in [@Goldburd], using one of two main encoding schemes: dummy-coding and one-hot encoding. Dummy-coding, used in the popular \texttt{R} statistical software, assigns one level of the category as a baseline, for which an indicator variable is not calibrated, and the rest of the levels are assigned indicator variables, thus, producing estimates within the model of how the effects of each level differ from the baselines. One-hot encoding, often used in machine learning, is similar to dummy-coding, but assigns indicator variables to each level, in other words, calibrates an extra indicator variable compared with dummy-coding.

A different approach to modelling categorical data is credibility theory (see [@Buhlmann2006] for an overview), which, in the context of rating, can be applied to derive premiums that reflect the experience of a particular policyholder, by estimating premiums as a weighted average between the premium produced using the collective experience (i.e. of all policyholders) and the premium produced using the experience of the particular policyholder. The weight used in this average is called a credibility factor and is calculated with reference to te variability of the policyholder experience relative to the variability of the group experience. In this context, the implicit categorical variable is the policyholder under consideration.

Generalized Linear Mixed Models (GLMMs) are an extension of GLMs that are designed for modelling categorical data using a principle very similar to that of credibility theory [@Klinker2010]. Instead of calibrating indicator variables for each level of the category, GLMMs estimate effects for each of these levels as a combination of the overall group mean and the experience in each level of the category.

Embedding layers represent a different approach to the problem of modelling categorical data that was recently introduced in an actuarial context. Note that in the next section, we reflect on similarities between the conventional approaches discussed above and embedding layers. [@Richman2018] reviewed the concept of embedding layers and connected the sharing of information across categories to the familiar concept of credibility theory. In that work, two applications of embedding layers were demonstrated. The first of these was in a Property and Casualty (P&C) pricing context, it was shown that the out-of-sample accuracy of a neural network trained to predict claims frequencies on motor third party liability was enhanced by modelling the categorical variables within this dataset using embedding layers. Second, a neural network with embedding layers was used to model all of the mortality rates in the Human Mortality Database, where the differences in population mortality across countries and the differences in mortality at different ages were modelled with embedding layers, again producing more accurate out of sample performance than the other models tested.

Contemporaneous with that work is the DeepTriangle model of [@Kuo2019], which applied recurrent neural networks to the problem of Incurred but not Reported (IBNR) loss reserving, to model jointly the paid and incurred losses in the Schedule P dataset. Embedding layers were used to capture the effect of differences in reserving delays and loss ratios for each company in the Schedule P dataset. Evaluating the results of the DeepTriangle method showed that the out of sample performance of the model (tested against the lower triangles in the Schedule P dataset) exceeded that of traditional IBNR reserving techniques.

Many other applications of embeddings have subsequently appeared in the actuarial literature. Within mortality forecasting, [@Richman2019d] and [@Richman2020] both apply embeddings layers to model and forecast mortality rates on a large scale. [@wuthrich2019yes] discussed how embeddings can be calibrated using GLM techniques and then incorporated into a combined actuarial neural network, with subsequent contributions in P&C pricing by [@schelldorfer2019nesting] and in IBNR reserving by [@Gabrielli2019c] and [@Gabrielli2019]. Other applications in IBNR reserving are in [@Kuo2020] and [@Delong2020a] who use embedding layers to model individual claims development.

# Definitions {#defns}

In this study, we are concerned with regression modelling, which is the task of predicting an unknown outcome $y$ on the basis of information about that outcome contained in predictor variables, or features, stored in a matrix $X$. For simplicity, we only consider the case of univariate outcomes, i.e., $y \in \mathbb{R}^1$. The outcomes and the rows of the predictor variable matrix are indexed by $i \in \{1 \dots I\}$, where $i$ represents a particular observation of $(y_i, \mathbf{x}_i)$, where bold indicates that we are now dealing with a vector. The columns of the predictor variables are indexed by $j \in \{1 \dots J\}$, where $j$ represents a particular predictor variable, of which $J$ have been observed, thus, we use the notation $X_j$ to represent the $j$th predictor variable and $X \in \mathbb{R}^J$. Formally, we look to build regression models that map from the predictor variables $\mathbf{x}_i$ to the outcome $y$ using a function $f$ of the form:

$$
\mathbf{f} : \mathbb{R}^J \mapsto \mathbb{R}^1, \quad \quad \mathbf{x_i} \mapsto  \mathbf f{(\mathbf{x_i})} = y. 
$$

We will use mainly use GLMs and neural networks to approximate the function $f(.)$.

The predictor variables that we consider here are comprised of two types: continuous variables, taking on numerical values and represented by the matrix $X_{num}$ with $J^{num}$ columns, and categorical variables, which take on discrete values indicating one of several possible categories, represented by the matrix $X_{cat}$ with $J^{cat}$ columns, such that $J^{num} + J^{cat} = J$.

## Categorical data modeling

A categorical variable $X_j, j \in J_{cat}$ takes as its value only one of a finite number of labels. Let the set of labels be $\mathcal{P}^j = \{p^j_1, p^j_2,\dots, p^j_{n_{\mathcal{P}^j}} \}$, where $n_\mathcal{P}^j = |\mathcal{P}^j|$ is the cardinality or number of levels, in $\mathcal{P}^j$. One-hot encoding maps each value $x_{i,j}$ of $X_j$ to $n_\mathcal{P}^j$ indicator variables, which take a value of $1$ if the label of $x_{i,j}$ corresponds to the level of the indicator variable, and $0$ otherwise. An example of one-hot encoding is shown in Table \ref{tab:onehot}.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
require(dplyr)
require(cellar)
require(recipes)
require(data.table)

# nfip = cellar_pull("nfip_claims", vintage = NULL)
# 
# states = nfip %>% select(state) %>% unique()
# 
# states %>% fwrite("manuscript/states.csv")

states= fread("../manuscript/states.csv")

sample_dat = sample_n(states,5) %>% arrange(state)

rec = recipe(~state, data = sample_dat)

dummies <- rec %>% step_dummy(state)
dummies <- prep(dummies, training = sample_dat)

dummy_data <- bake(dummies, new_data =sample_dat)

cbind(sample_dat, dummy_data) %>% knitr::kable(label = "tab:onehot", caption = "Example one-hot encoding of the state variable")
```

One-hot encoding is often used in the machine learning community while the statistical community often favors dummy coding, which, instead of assigning $n_p^j$ indicator variables, assigns one of the levels of the categories as a baseline, and maps all of the other $n_p^j -1$ variables to indicator variables.

The approach that we focus on in this study is to use embeddings, which map each level of the categorical data to a low dimensional vector of parameters that is learned together with the rest of the GLM or neural network that is used for the modeling problem. Formally, an embedding is

$$
z_\mathcal{P^j} : \mathcal{P^j} \to \mathbb{R}^{q_\mathcal{P^j} }, \quad \quad p^j \mapsto  z_\mathcal{P^j}(p),
$$

where $q_\mathcal{P^j}$ is the dimension of the embedding for the $j$th categorical variable and $z_\mathcal{P^j}(.)$ is an function that maps from the particular element of the labels $p$ to the embedding space. This function is left implicit, meaning to say, we allow the embeddings to be derived during the process of fitting the model and do not attempt to specify exactly how the embeddings can be derived from the input data.

## Predictive Modeling with GLM and a Minimalist Neural Network

In this section, we describe, fit, and evaluate both a minimalist neural network architecture utilizing embeddings and a traditional GLM. The working example for our experiments is as follows: Given a set of claims characteristics, we predict the losses paid on the property coverage of the policy. In the rest of this section, we describe the dataset we use, describe formally the models being considered, and discuss results.

## NFIP data

The data we use comes from the National Flood Insurance Program (NFIP) and is made available by the OpenFEMA initiative of the Federal Emergency Management Agency (FEMA) (TODO: cite). Two datasets are made available by OpenFEMA: A policies dataset with exposure information, and a claims dataset with claims transactions, including paid amounts. Because there is no way to associate records of the two datasets, we are limited to fitting severity models on the claims dataset. While the complete dataset contains over two million transactions, for the purposes of our experiments we limit ourselves to data from 2000 to August 2019, which amounts to approximately 1.4 million claims. The dataset can be downloaded from Cellar (TODO: link). The dataset includes a rich variety of variables, from occupancy type to coarse coordinates. For our models, we work with a few selected variables that represent continuous and discrete variables of low and high cardinalities, which we list in Table \ref{tab:variables}.

| Variable                                 | Type                                     |
|------------------------------------------|------------------------------------------|
| Building insurance coverage              | Numeric                                  |
| Basement enclosure type                  | Categorical                              |
| Number of floors in the insured building | Categorical (binned in original dataset) |
| Flood zone                               | Categorical (high cardinality)           |
| Primary residence                        | Categorical (binary indicator)           |

: \label{tab:variables} Variables used in modeling.

## Severity GLM

We fit a GLM with the following specifications:

-   Target variable: Amount paid on building claim

-   Predictors: *Log* of **building insurance coverage**, **basement enclosure type**, **number of floors in the insured building**, *prefix* of **flood zone**, and ***primary residence***.

-   Link function: log

-   Distribution: gamma

We take the log of the continuous predictor **building insurance coverage** following (cite cas glm monograph), which allows the scale of the predictor to match that of the target variable. Because the **flood zone** variable in the original data contains 60 levels, we take the prefix of the zone code, which corresponds to the level of risk as determined by FEMA (source?). For example, *A01*, *A02*, and so on are recoded as simply *A*.

A log link together with the gamma distribution is a standard choice for severity modeling, which provides a multiplicative structure where the response is positive.

## A Simple Neural Network

(Note: this section is WIP doesn't match up w/ code, which is also WIP) For the neural net, we utilize the same responsible variable, amount paid on building claim. We normalize the numeric variable ***building insurance coverage***, one-hot encode the **primary residence** variable, and apply embedding layers of dimension one to the rest of the categorical variables. We then connect the embeddings and the normalized variable to a single output unit with softplus activation, where the softplus function is defined to be

\begin{equation}
x \mapsto \log(1 + e^x).
\end{equation}

Figure (todo: add figure) exhibits the architecture of our simple neural network.

## Performance evaluation framework and results

To evaluate these baseline models and other models in this paper, we perform 10-fold cross validation and compare the root mean square error (RMSE) on the predicted and actual paid amounts. 

# Interpretations {#interpretation}

Extracting and interpreting trained embeddings, including visualization techniques Can we explain the learned embeddings using a GLM? distance to water etc Rainfall Density Distance to coast

# Transfer Learning {#tl}

How can embedding layers be used in GLM models? (Potential uses of embedding layers as a feature engineering technique for GLM) Transfer learned embeddings to GLM - backprop on NN and use as preprocessing Train GLM on embedding layers - backprop on 1 layer NN

# Extending the model {#extend}

Investigate approaches for using embedding layers for numeric variables, or numeric variables that were captured as categorical variables (Can the modelling of numerical variables benefit from the application of embedding layers?) How should this best be done? CatBoost - cut into 256 groups smoothness Map numeric to a dense layer and use that as embedding - basis expansion Add a distribution to the embeddings Can insights from credibility theory lead to enhanced embeddings? Take credibility mixture over embeddings Cluster embeddings PCA of embeddings

# Attention based modelling {#attention}

Can we get some lift by adding attention layers? TabNet model Transformer model

# Conclusions {#Conclusions}

Conclusions
